{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b79fd593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder,MinMaxScaler,StandardScaler\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression,LogisticRegressionCV,SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D,Input\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "# Importing necssary modules\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "from collections import Counter\n",
    "# Custom script \n",
    "#%matplotlibe inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e873dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['decile1b', 'decile3', 'lsat', 'ugpa', 'zfygpa', 'zgpa', 'fulltime', 'fam_inc', 'sex', 'race', 'tier', 'y'])\n",
      "['decile1b_1', 'decile1b_2', 'decile1b_3', 'decile1b_4', 'decile1b_5', 'decile1b_6', 'decile1b_7', 'decile1b_8', 'decile1b_9', 'decile1b_10', 'decile3_1', 'decile3_2', 'decile3_3', 'decile3_4', 'decile3_5', 'decile3_6', 'decile3_7', 'decile3_8', 'decile3_9', 'decile3_10', 'lsat', 'ugpa', 'zfygpa', 'zgpa', 'fulltime', 'fam_inc_1', 'fam_inc_2', 'fam_inc_3', 'fam_inc_4', 'fam_inc_5', 'sex', 'race', 'tier_1', 'tier_2', 'tier_3', 'tier_4', 'tier_5', 'tier_6', 'target']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(18692, 38)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import urllib2\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import preprocessing\n",
    "from random import seed, shuffle\n",
    "\n",
    "# import utils as ut\n",
    "\n",
    "SEED = 1234\n",
    "seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "def load_law():\n",
    "    FEATURES_CLASSIFICATION = [\"decile1b\", \"decile3\", \"lsat\", \"ugpa\", \"zfygpa\",\"zgpa\", \"fulltime\", \"fam_inc\", \"sex\", \"race\", \"tier\"]  # features to be used for classification\n",
    "    CONT_VARIABLES = [\"lsat\", \"ugpa\", \"zfygpa\", \"zgpa\"]  # continuous features, will need to be handled separately from categorical features, categorical features will be encoded using one-hot\n",
    "    CLASS_FEATURE = \"y\"  # the decision variable\n",
    "    SENSITIVE_ATTRS = [\"sex\"]\n",
    "    CAT_VARIABLES = [\"decile1b\", \"decile3\", \"fulltime\", \"fam_inc\", \"sex\", \"race\", \"tier\"]\n",
    "    CAT_VARIABLES_INDICES = [1,2,7,8,9,10,11]\n",
    "    # COMPAS_INPUT_FILE = \"bank-full.csv\"\n",
    "    INPUT_FILE = \"./datasets/law.csv\"\n",
    "\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    \n",
    "    # convert to np array\n",
    "    data = df.to_dict('list')\n",
    "    print(data.keys())\n",
    "    for k in data.keys():\n",
    "        data[k] = np.array(data[k])\n",
    "\n",
    "    \"\"\" Feature normalization and one hot encoding \"\"\"\n",
    "\n",
    "    # convert class label 0 to -1\n",
    "    y = data[CLASS_FEATURE]\n",
    "    #print(np.unique(y))\n",
    "    #y[y == 0] = 1\n",
    "    #y[y==0] = -1\n",
    "    #y[y == 1] = -1\n",
    "    y = np.array([int(k) for k in y])\n",
    "\n",
    "    X = np.array([]).reshape(len(y), 0)  # empty array with num rows same as num examples, will hstack the features to it\n",
    "    \n",
    "    x_control = defaultdict(list)\n",
    "    i=0\n",
    "    feature_names = []\n",
    "    \n",
    "    for attr in FEATURES_CLASSIFICATION:\n",
    "        vals = data[attr]\n",
    "        \n",
    "        if attr in CONT_VARIABLES:\n",
    "            vals = [float(v) for v in vals]\n",
    "            vals = preprocessing.scale(vals)  # 0 mean and 1 variance\n",
    "            vals = np.reshape(vals, (len(y), -1))  # convert from 1-d arr to a 2-d arr with one col\n",
    "            \n",
    "\n",
    "        else:  # for binary categorical variables, the label binarizer uses just one var instead of two\n",
    "            lb = preprocessing.LabelBinarizer()\n",
    "            lb.fit(vals)\n",
    "            vals = lb.transform(vals)\n",
    "           \n",
    "            #if attr == 'job':\n",
    "            #    print(lb.classes_)\n",
    "            #    print(lb.transform(lb.classes_))\n",
    "            \n",
    "           \n",
    "            \n",
    "            \n",
    "        # add to sensitive features dict\n",
    "        if attr in SENSITIVE_ATTRS:\n",
    "            x_control[attr] = vals\n",
    "            \n",
    "\n",
    "        # add to learnable features\n",
    "        X = np.hstack((X, vals))\n",
    "        \n",
    "        if attr in CONT_VARIABLES:  # continuous feature, just append the name\n",
    "            feature_names.append(attr)\n",
    "        else:  # categorical features\n",
    "            if vals.shape[1] == 1:  # binary features that passed through lib binarizer\n",
    "                feature_names.append(attr)\n",
    "            else:\n",
    "                for k in lb.classes_:  # non-binary categorical features, need to add the names for each cat\n",
    "                    feature_names.append(attr + \"_\" + str(k))\n",
    "\n",
    "    # convert the sensitive feature to 1-d array\n",
    "    \n",
    "    x_control = dict(x_control)\n",
    "    \n",
    "    for k in x_control.keys():\n",
    "        assert (x_control[k].shape[1] == 1)  # make sure that the sensitive feature is binary after one hot encoding\n",
    "        x_control[k] = np.array(x_control[k]).flatten()\n",
    "    \n",
    "    feature_names.append('target')\n",
    "    \n",
    "   \n",
    "    print(feature_names)\n",
    "    return X, y, feature_names.index(SENSITIVE_ATTRS[0]), 1, x_control\n",
    "\n",
    "X,y, sa_index, p_Group, x_control= load_law()\n",
    "\n",
    "print(X)\n",
    "#print(X[0])\n",
    "#print(X[0][1])\n",
    "#print(sa_index)\n",
    "np_Group = 0 #non-protected group's sa_value\n",
    "Y = []\n",
    "for i in y:\n",
    "    if (i == 0):\n",
    "        Y.append(1)\n",
    "    else:\n",
    "        Y.append(0)\n",
    "        \n",
    "Y = np.array(Y)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.2)\n",
    "Xtr = x_train\n",
    "Xte = x_test\n",
    "Ytr = y_train\n",
    "Yte = y_test\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb7ffedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_clients(image_list, label_list, num_clients, initial='clients'):\n",
    "    ''' return: a dictionary with keys clients' names and value as \n",
    "                data shards - tuple of images and label lists.\n",
    "        args: \n",
    "            image_list: a list of numpy arrays of training images\n",
    "            label_list:a list of binarized labels for each image\n",
    "            num_client: number of fedrated members (clients)\n",
    "            initials: the clients'name prefix, e.g, clients_1 \n",
    "            \n",
    "    '''\n",
    "\n",
    "    #create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
    "\n",
    "    #randomize the data\n",
    "    data = list(zip(image_list, label_list))\n",
    "    random.shuffle(data)\n",
    "\n",
    "    #shard data and place at each client\n",
    "    size = len(data)//num_clients\n",
    "    shards = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
    "\n",
    "    #number of clients must equal number of shards\n",
    "    assert(len(shards) == len(client_names))\n",
    "\n",
    "    return {client_names[i] : shards[i] for i in range(len(client_names))} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbc221ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clients = create_clients(Xtr, Ytr, num_clients=10, initial='client')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d8b5fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#maryam: create test data for each client\n",
    "clients_test_data = create_clients(Xte, Yte, num_clients=3, initial='client')\n",
    "clients = create_clients(Xtr, Ytr, num_clients=3, initial='client')\n",
    "def batch_data(data_shard, bs=30):\n",
    "    '''Takes in a clients data shard and create a tfds object off it\n",
    "    args:\n",
    "        shard: a data, label constituting a client's data shard\n",
    "        bs:batch size\n",
    "    return:\n",
    "        tfds object'''\n",
    "    #seperate shard into data and labels lists\n",
    "    data, label = zip(*data_shard)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
    "    return dataset.shuffle(len(label)).batch(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e237e7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#maryam: batch clients test data\n",
    "clients_test_data_batched = dict()\n",
    "for (client_name, data) in clients_test_data.items():\n",
    "    clients_test_data_batched[client_name] = batch_data(data)\n",
    "    \n",
    "clients_batched = dict()\n",
    "for (client_name, data) in clients.items():\n",
    "    clients_batched[client_name] = batch_data(data)\n",
    "#process and batch the test set  \n",
    "test_batched = tf.data.Dataset.from_tensor_slices((Xte, Yte)).batch(len(Yte))\n",
    "#print('Number of client datasets: {l}'.format(l=len(test_batched)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e94cf90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15030\n",
      "5010\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client_names = list(clients_batched.keys())\n",
    "bs = list(clients_batched[client_name])[0][0].shape[0] #?? what is this bs?\n",
    "#first calculate the total training data points across clinets\n",
    "global_count = sum([tf.data.experimental.cardinality(clients_batched[client_name]).numpy() for client_name in client_names])*bs\n",
    "local_count = tf.data.experimental.cardinality(clients_batched['client_1']).numpy()*bs\n",
    "print(global_count) ##??where do we use these counts\n",
    "print(local_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d56ee0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of client datasets: 3\n",
      "First dataset: <BatchDataset shapes: ((None, 38), (None,)), types: (tf.float64, tf.int32)>\n",
      "Number of client test datasets: 3\n"
     ]
    }
   ],
   "source": [
    "print('Number of client datasets: {l}'.format(l=len(clients_batched)))\n",
    "print('First dataset: {d}'.format(d=clients_batched['client_1']))\n",
    "#maryam: check length of number of test datasets created for clients\n",
    "print('Number of client test datasets: {l}'.format(l=len(clients_test_data_batched)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9fd2115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,BatchNormalization,InputLayer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "class SimpleMLP:\n",
    "    @staticmethod\n",
    "    def build(x_train,n):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(InputLayer(input_shape=(x_train.shape[1],)))\n",
    "        model.add(Dense(x_train.shape[1],activation='relu'))#,input_shape=(x_train.shape[1],)))\n",
    "        #model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(n*x_train.shape[1],activation='relu'))\n",
    "        #model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(n*x_train.shape[1],activation='relu'))\n",
    "\n",
    "        model.add(Dense(1,activation='sigmoid'))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c1a8baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\noptimizer = SGD(lr=lr, \\n                decay=lr / comms_round, \\n                momentum=0.9\\n               )  \\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.0001 \n",
    "comms_round = 50\n",
    "loss='binary_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "optimizer = Adam(learning_rate=lr)\n",
    "'''\n",
    "optimizer = SGD(lr=lr, \n",
    "                decay=lr / comms_round, \n",
    "                momentum=0.9\n",
    "               )  \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e34e6f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def weight_scalling_factor(clients_trn_data, client_name):\n",
    "    client_names = list(clients_trn_data.keys())\n",
    "    #get the bs\n",
    "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
    "    #first calculate the total training data points across clinets\n",
    "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n",
    "    # get the total number of data points held by a client\n",
    "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n",
    "    return local_count/global_count\n",
    "\n",
    "\n",
    "def scale_model_weights(weight, scalar):\n",
    "    '''function for scaling a models weights'''\n",
    "    weight_final = []\n",
    "    steps = len(weight)\n",
    "    for i in range(steps):\n",
    "        weight_final.append(scalar * weight[i])\n",
    "    return weight_final\n",
    "\n",
    "\n",
    "\n",
    "def sum_scaled_weights(scaled_weight_list):\n",
    "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
    "    avg_grad = list()\n",
    "    #get the average grad accross all client gradients\n",
    "    for grad_list_tuple in zip(*scaled_weight_list):\n",
    "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "        \n",
    "    return avg_grad\n",
    "\n",
    "def find_statistical_parity_score(data,labels,predictions):\n",
    "    protected_pos = 0.\n",
    "    protected_neg = 0.\n",
    "    non_protected_pos = 0.\n",
    "    non_protected_neg = 0.\n",
    "    '''\n",
    "    tp_protected = 0.\n",
    "    tn_protected = 0.\n",
    "    fp_protected = 0.\n",
    "    fn_protected = 0.\n",
    "\n",
    "    tp_non_protected = 0.\n",
    "    tn_non_protected = 0.\n",
    "    fp_non_protected = 0.\n",
    "    fn_non_protected = 0.\n",
    "    '''\n",
    "    saIndex = sa_index\n",
    "    saValue = p_Group\n",
    "    \n",
    "    for idx, val in enumerate(data):\n",
    "        # protected population\n",
    "        if val[saIndex] == saValue:\n",
    "            if predictions[idx] == 1:\n",
    "                protected_pos += 1.\n",
    "            else:\n",
    "                protected_neg += 1.\n",
    "            # correctly classified\n",
    "            #if labels[idx] == predictions[idx]:\n",
    "            #    if labels[idx] == 1:\n",
    "            #        tp_protected += 1.\n",
    "            #    else:\n",
    "            #        tn_protected += 1.\n",
    "            # misclassified\n",
    "            #else:\n",
    "            #    if labels[idx] == 1:\n",
    "            #        fn_protected += 1.\n",
    "            #    else:\n",
    "            #        fp_protected += 1.\n",
    "\n",
    "        else:\n",
    "            if predictions[idx] == 1:\n",
    "                non_protected_pos += 1.\n",
    "            else:\n",
    "                non_protected_neg += 1.\n",
    "\n",
    "            # correctly classified\n",
    "            #if labels[idx] == predictions[idx]:\n",
    "            #    if labels[idx] == 1:\n",
    "            #        tp_non_protected += 1.\n",
    "            #    else:\n",
    "            #        tn_non_protected += 1.\n",
    "            # misclassified\n",
    "            #else:\n",
    "            #    if labels[idx] == 1:\n",
    "            #        fn_non_protected += 1.\n",
    "            #    else:\n",
    "            #        fp_non_protected += 1.\n",
    "\n",
    "    #tpr_protected = tp_protected / (tp_protected + fn_protected)\n",
    "    #tnr_protected = tn_protected / (tn_protected + fp_protected)\n",
    "\n",
    "    #tpr_non_protected = tp_non_protected / (tp_non_protected + fn_non_protected)\n",
    "    #tnr_non_protected = tn_non_protected / (tn_non_protected + fp_non_protected)\n",
    "\n",
    "    C_prot = (protected_pos) / (protected_pos + protected_neg)\n",
    "    C_non_prot = (non_protected_pos) / (non_protected_pos + non_protected_neg)\n",
    "\n",
    "    stat_par = C_non_prot - C_prot\n",
    "    '''\n",
    "    print(\"protected_pos: %s\" %protected_pos)\n",
    "    print(\"protected_neg: %s\" %protected_neg)\n",
    "    print(\"non-protected_pos: %s\" %non_protected_pos)\n",
    "    print(\"non-protected_neg: %s\" %non_protected_neg)\n",
    "    '''\n",
    "    return stat_par\n",
    "    \n",
    "def find_eqop_score(data,labels,predictions):\n",
    "    '''\n",
    "    protected_pos = 0.\n",
    "    protected_neg = 0.\n",
    "    non_protected_pos = 0.\n",
    "    non_protected_neg = 0.\n",
    "    '''\n",
    "    tp_protected = 0.\n",
    "    tn_protected = 0.\n",
    "    fp_protected = 0.\n",
    "    fn_protected = 0.\n",
    "\n",
    "    tp_non_protected = 0.\n",
    "    tn_non_protected = 0.\n",
    "    fp_non_protected = 0.\n",
    "    fn_non_protected = 0.\n",
    "    saIndex = sa_index\n",
    "    saValue = p_Group\n",
    "    for idx, val in enumerate(data):\n",
    "        # protrcted population\n",
    "        if val[saIndex] == saValue:\n",
    "            #if predictions[idx] == 1:\n",
    "            #    protected_pos += 1.\n",
    "            #else:\n",
    "            #    protected_neg += 1.\n",
    "\n",
    "            # correctly classified\n",
    "            if labels[idx] == predictions[idx]:\n",
    "                if labels[idx] == 1:\n",
    "                    tp_protected += 1.\n",
    "                else:\n",
    "                    tn_protected += 1.\n",
    "            # misclassified\n",
    "            else:\n",
    "                if labels[idx] == 1:\n",
    "                    fn_protected += 1.\n",
    "                else:\n",
    "                    fp_protected += 1.\n",
    "\n",
    "        else:\n",
    "            #if predictions[idx] == 1:\n",
    "            #    non_protected_pos += 1.\n",
    "            #else:\n",
    "            #    non_protected_neg += 1.\n",
    "\n",
    "            # correctly classified\n",
    "            if labels[idx] == predictions[idx]:\n",
    "                if labels[idx] == 1:\n",
    "                    tp_non_protected += 1.\n",
    "                else:\n",
    "                    tn_non_protected += 1.\n",
    "            # misclassified\n",
    "            else:\n",
    "                if labels[idx] == 1:\n",
    "                    fn_non_protected += 1.\n",
    "                else:\n",
    "                    fp_non_protected += 1.\n",
    "\n",
    "    tpr_protected = tp_protected / (tp_protected + fn_protected)\n",
    "    #tnr_protected = tn_protected / (tn_protected + fp_protected)\n",
    "\n",
    "    tpr_non_protected = tp_non_protected / (tp_non_protected + fn_non_protected)\n",
    "    #tnr_non_protected = tn_non_protected / (tn_non_protected + fp_non_protected)\n",
    "\n",
    "    \n",
    "    eqop = tpr_non_protected - tpr_protected\n",
    "    return eqop\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def test_client_model(X_test, Y_test,  model):\n",
    "    \n",
    "    cce = tf.keras.losses.BinaryCrossentropy()\n",
    "    logits = model.predict(X_test)\n",
    "    preidect = np.around(logits)\n",
    "    preidect = np.nan_to_num(preidect)\n",
    "    Y_test = np.nan_to_num(Y_test)\n",
    "    conf = (confusion_matrix(Y_test,preidect)) \n",
    "    TN = conf[0][0]\n",
    "    FP = conf[0][1]\n",
    "    FN = conf[1][0]\n",
    "    TP = conf[1][1]\n",
    "    sensitivity = TP/(TP+FN) \n",
    "    specificity = TN/(FP+TN)\n",
    "        \n",
    "    BalanceACC = (sensitivity+specificity)/2\n",
    "    stat_parity = find_statistical_parity_score(X_test,Y_test,preidect)\n",
    "    assigned_positive_labels = 0\n",
    "    total_positive_labels = 0\n",
    "    \n",
    "    \n",
    "    unique, counts = np.unique(preidect, return_counts=True)\n",
    "    count_ap_dict = dict(zip(unique, counts))\n",
    "    assigned_positive_labels = count_ap_dict.get(1,0)\n",
    "    \n",
    "    unique, counts = np.unique(Y_test, return_counts=True)\n",
    "    count_tp_dict = dict(zip(unique, counts))\n",
    "    total_positive_labels = count_tp_dict.get(1,0)\n",
    "    \n",
    "    '''\n",
    "    for i in range(len(preidect)):\n",
    "        if preidect[i]==1:\n",
    "            assigned_positive_labels+=1\n",
    "        if Y_test[i]==1:\n",
    "            total_positive_labels+=1\n",
    "    '''    \n",
    "    eqop = find_eqop_score(X_test,Y_test,preidect) \n",
    "    print('eqop: {}'.format(eqop))\n",
    "    return eqop, assigned_positive_labels, total_positive_labels, BalanceACC\n",
    "    #print('stat_parity: {}'.format(stat_parity))\n",
    "    \n",
    "    #return stat_parity, assigned_positive_labels, total_positive_labels, BalanceACC\n",
    "    \n",
    "def test_model(X_test, Y_test,  model, comm_round):\n",
    "    cce = tf.keras.losses.BinaryCrossentropy()\n",
    "    logits = model.predict(X_test)\n",
    "    preidect = np.around(logits)\n",
    "    preidect = np.nan_to_num(preidect)\n",
    "    Y_test = np.nan_to_num(Y_test)\n",
    "    stat_parity = find_statistical_parity_score(X_test,Y_test,preidect)\n",
    "    eqop = find_eqop_score(X_test,Y_test,preidect)    \n",
    "        \n",
    "    conf = (confusion_matrix(Y_test,preidect))   \n",
    "    loss = cce(Y_test, preidect)\n",
    "    acc = accuracy_score(preidect,Y_test)\n",
    "    print('comm_round: {} | global_acc: {} | global_loss: {}'.format(comm_round, acc, loss))\n",
    "    return acc, loss,conf,stat_parity, eqop\n",
    "\n",
    "def find_class_Weight(labels,majority_label,minority_label):\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    count_ap_dict = dict(zip(unique, counts))\n",
    "    \n",
    "    majority_class_weight = 1\n",
    "    minority_class_weight = count_ap_dict.get(majority_label,0)/count_ap_dict.get(minority_label,1)\n",
    "    class_weights={majority_label:1,minority_label:minority_class_weight}\n",
    "    return class_weights\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d648153",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# bulid K-SMOTE algorithem\n",
    "# input (dmin, dmaj, K=number of data sample from dmin to upsampling, r=ratio of samples to reate in compare to dmaj)\n",
    "# output(synthatic data for a client)\n",
    "import random\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "\n",
    "def k_nearest_neighbors(data, predict, k):\n",
    "    #k=8\n",
    "    #if len(data) >= k:\n",
    "    #    warnings.warn('K is set to a value less than total voting groups!')\n",
    "\n",
    "    distances = []\n",
    "    count = 0\n",
    "    for sample in data:\n",
    "        euclidean_distance = np.linalg.norm(np.array(sample)-np.array(predict))\n",
    "        distances.append([euclidean_distance,count])\n",
    "        count+=1\n",
    "    \n",
    "    votes = [i[1] for i in sorted(distances)[:k]] ##votes is returning indexes of k random samples\n",
    "\n",
    "    #vote_result = Counter(votes).most_common(9)[0][0]\n",
    "    return votes\n",
    "\n",
    "\n",
    "def fair_k_nearest_neighbors(data, predict, k):\n",
    "    #k=8\n",
    "    #if len(data) >= k:\n",
    "    #    warnings.warn('K is set to a value less than total voting groups!')\n",
    "\n",
    "    distances = []\n",
    "    count = 0\n",
    "    for sample in data:\n",
    "        euclidean_distance = np.linalg.norm(np.array(sample)-np.array(predict))\n",
    "        distances.append([euclidean_distance,count])\n",
    "        count+=1\n",
    "    \n",
    "    votes = [i[1] for i in sorted(distances)[:k]] ##votes is returning indexes of k random samples\n",
    "\n",
    "    #vote_result = Counter(votes).most_common(9)[0][0]\n",
    "    return votes\n",
    "\n",
    "##algo 2:\n",
    "\n",
    "def fair_kSMOTE_algo_2(dmajor,dminor,k,r):\n",
    "    S = []\n",
    "    Ns =  int(r*(len(dmajor)))\n",
    "    \n",
    "    Nks = int(Ns / (k-1))\n",
    "    difference = Ns-Nks*(k-1)\n",
    "    \n",
    "    rb = []\n",
    "    #pick a random k sample from dmin and save them in rb\n",
    "    dmin_rand = random.sample(dminor, k-1)   \n",
    "    #for debugging\n",
    "    sens_attr_vals = []\n",
    "    i = 0\n",
    "    \n",
    "    #do algorithem (choose the nearest neighbor and linear interpolation)\n",
    "    for xb in dmin_rand:\n",
    "        N= k_nearest_neighbors(dminor,xb,k) #from minority-p\n",
    "        \n",
    "        #do linear interpolation\n",
    "        Sxb = []\n",
    "        \n",
    "        if i==0:\n",
    "            Nkss = Nks+difference\n",
    "        else:\n",
    "            Nkss = Nks\n",
    "        \n",
    "        i+=1\n",
    "        for s in range(Nkss):\n",
    "            \n",
    "            \n",
    "            j = 1  ##?? j?\n",
    "            #randome k sample\n",
    "            #j = random.randint(0, len(N))\n",
    "            \n",
    "            ##here nearst neghber insted of dminor\n",
    "            #linear interpolation\n",
    "            x_new = abs(((dminor[N[j]]-xb) * random.sample(range(0, 1), 1)))\n",
    "            j+=1\n",
    "            \n",
    "            while(j < len(N)):\n",
    "                #here on random xb\n",
    "                ind = N[j]\n",
    "                x_new = abs(x_new + ((dminor[ind]-xb) * random.sample(range(0, 1), 1)))\n",
    "                j += 1\n",
    "                \n",
    "            x_new = x_new / (len(N)-1) ##??? why scaling with k here\n",
    "            Synthesized_instance = xb + x_new ##?> why we need to sum xb and x_new\n",
    "            \n",
    "            \n",
    "            #for algo 3 when finding nearest neighbors from min_np and assigning the \n",
    "            #'p' sensitive value to all synthesized instances\n",
    "            #Synthesized_instance[sa_index] = xb[sa_index] \n",
    "            Sxb.append(Synthesized_instance)\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        S.append(Sxb)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return S\n",
    "\n",
    "\n",
    "\n",
    "###algo 3, algo 3 x_synthesized as a mixture of nearest neighbors from dmin-p and dmin-np\n",
    "\n",
    "def fair_kSMOTE(dmajor,dminor_wg,dminor,k,r):\n",
    "    S = []\n",
    "    #Ns =  int(r*(len(dmajor) - len(dminor)))\n",
    "    Ns =  int(r*(len(dmajor)))\n",
    "    \n",
    "    \n",
    "    Nks = int(Ns / (k-1))\n",
    "    difference = Ns-Nks*(k-1)\n",
    "    #if r==-1:\n",
    "    #    Nks = 1\n",
    "    rb = []\n",
    "    #pick a random k sample from dmin and save them in rb\n",
    "    dmin_rand = random.sample(dminor, k-1)   \n",
    "    #for debugging\n",
    "    sens_attr_vals = []\n",
    "    \n",
    "    \n",
    "    #do algorithem (choose the nearest neighbor and linear interpolation)\n",
    "    i = 0\n",
    "    \n",
    "    for xb in dmin_rand:\n",
    "        N= k_nearest_neighbors(dminor,xb,int(k/2)+1) #from minority-p\n",
    "        N2= k_nearest_neighbors(dminor_wg,xb,int(k/2)) #from minority-np\n",
    "    \n",
    "        N3 = np.hstack((N, N2))\n",
    "        if i==0:\n",
    "            Nkss = Nks+difference\n",
    "        else:\n",
    "            Nkss = Nks\n",
    "        \n",
    "        i+=1\n",
    "        #do linear interpolation\n",
    "        Sxb = []\n",
    "        \n",
    "        for s in range(Nkss):\n",
    "            \n",
    "            j = 1  ##?? j?\n",
    "            #randome k sample\n",
    "            #j = random.randint(0, len(N))\n",
    "            \n",
    "            ##here nearst neghber insted of dminor\n",
    "            #linear interpolation\n",
    "            x_new = abs(((dminor[N[j]]-xb) * random.sample(range(0, 1), 1)))\n",
    "            j+=1\n",
    "            \n",
    "            while(j < len(N)):\n",
    "                #here on random xb\n",
    "                ind = N[j]\n",
    "                \n",
    "                x_new = abs(x_new + ((dminor[ind]-xb) * random.sample(range(0, 1), 1)))\n",
    "                j += 1\n",
    "            j = 0\n",
    "            while(j < len(N2)):\n",
    "                #here on random xb\n",
    "                ind = N2[j]\n",
    "                \n",
    "                x_new = abs(x_new + ((dminor_wg[ind]-xb) * random.sample(range(0, 1), 1)))\n",
    "                j += 1    \n",
    "            x_new = x_new / (len(N3)-1) ##??? why scaling with k here\n",
    "            Synthesized_instance = xb + x_new ##?> why we need to sum xb and x_new\n",
    "            \n",
    "            \n",
    "            #for algo 3 when finding nearest neighbors from min_np and assigning the \n",
    "            #'p' sensitive value to all synthesized instances\n",
    "            Synthesized_instance[sa_index] = xb[sa_index] \n",
    "            Sxb.append(Synthesized_instance)\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        S.append(Sxb)\n",
    "    \n",
    "    \n",
    "   \n",
    "    return S\n",
    "\n",
    "\n",
    "##algo 3 x_synthesized separately for nearest neighbors from dmin-p and dmin-np\n",
    "'''\n",
    "def fair_kSMOTE(dmajor,dminor_wg,dminor,k,r):\n",
    "    S = []\n",
    "    Ns =  int(r*(len(dmajor) - len(dminor)))\n",
    "    #print(\"Ns %s\" % Ns)\n",
    "    Nks = int(Ns / (k-1))\n",
    "    #print(\"Nks%s\"%Nks)\n",
    "    rb = []\n",
    "    #pick a random k sample from dmin and save them in rb\n",
    "    dmin_rand = random.sample(dminor, k)   \n",
    "    #for debugging\n",
    "    sens_attr_vals = []\n",
    "    \n",
    "    \n",
    "    #do algorithem (choose the nearest neighbor and linear interpolation)\n",
    "    for xb in dmin_rand:\n",
    "        N= k_nearest_neighbors(dminor,xb,int(k/2)+1) #from minority-p\n",
    "        N2= k_nearest_neighbors(dminor_wg,xb,int(k/2)) #from minority-np\n",
    "        \n",
    "        #do linear interpolation\n",
    "        Sxb = []\n",
    "        for s in range(int(Nks/2)):\n",
    "            j = 1  ##?? j?\n",
    "            #randome k sample\n",
    "            #j = random.randint(0, len(N))\n",
    "            \n",
    "            ##here nearst neghber insted of dminor\n",
    "            #linear interpolation\n",
    "            x_new = ((dminor[N[j]]-xb) * random.sample(range(0, 1), 1))\n",
    "            j+=1\n",
    "            \n",
    "            while(j < len(N)):\n",
    "                #here on random xb\n",
    "                ind = N[j]\n",
    "                x_new = x_new + ((dminor[ind]-xb) * random.sample(range(0, 1), 1))\n",
    "                j += 1\n",
    "                \n",
    "            x_new = x_new / (len(N)-1) ##??? why scaling with k here\n",
    "            Synthesized_instance = xb + x_new ##?> why we need to sum xb and x_new\n",
    "            \n",
    "            #for debugging\n",
    "            sens_attr_vals.append(Synthesized_instance[13])\n",
    "            #for algo 3 when finding nearest neighbors from min_np and assigning the \n",
    "            #'p' sensitive value to all synthesized instances\n",
    "            Synthesized_instance[sa_index] = xb[sa_index] \n",
    "            Sxb.append(Synthesized_instance)\n",
    "            \n",
    "            \n",
    "            j = 1  ##?? j?\n",
    "            #randome k sample\n",
    "            #j = random.randint(0, len(N))\n",
    "            \n",
    "            ##here nearst neghber insted of dminor\n",
    "            #linear interpolation\n",
    "            x_new = ((dminor[N2[j]]-xb) * random.sample(range(0, 1), 1))\n",
    "            j+=1\n",
    "            \n",
    "            while(j < len(N2)):\n",
    "                #here on random xb\n",
    "                ind = N2[j]\n",
    "                x_new = x_new + ((dminor[ind]-xb) * random.sample(range(0, 1), 1))\n",
    "                j += 1\n",
    "                \n",
    "            x_new = x_new / len(N2) ##??? why scaling with k here\n",
    "            Synthesized_instance = xb + x_new ##?> why we need to sum xb and x_new\n",
    "            Synthesized_instance[sa_index] = xb[sa_index] \n",
    "            Sxb.append(Synthesized_instance)         \n",
    "            \n",
    "        S.append(Sxb)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return S\n",
    "'''   \n",
    "##algo 4 \n",
    "'''\n",
    "def fair_kSMOTE(dmajor,dminor_wg,dminor,dmin_client,k,r):\n",
    "    S = []\n",
    "    Ns =  int(r*(len(dmajor) - len(dminor)))\n",
    "    #print(\"Ns %s\" % Ns)\n",
    "    Nks = int(Ns / (k-1))\n",
    "    #print(\"Nks%s\"%Nks)\n",
    "    rb = []\n",
    "    #pick a random k sample from dmin and save them in rb\n",
    "    dmin_rand = random.sample(dminor, k)   \n",
    "    #for debugging\n",
    "    sens_attr_vals = []\n",
    "    \n",
    "    \n",
    "    #do algorithem (choose the nearest neighbor and linear interpolation)\n",
    "    for xb in dmin_rand:\n",
    "        N= k_nearest_neighbors(dmin_client,xb,k) #from dmin-p and dmin-np\n",
    "        \n",
    "        \n",
    "        #do linear interpolation\n",
    "        Sxb = []\n",
    "        for s in range(Nks):\n",
    "            j = 1  ##?? j?\n",
    "            #randome k sample\n",
    "            #j = random.randint(0, len(N))\n",
    "            \n",
    "            ##here nearst neghber insted of dminor\n",
    "            #linear interpolation\n",
    "            x_new = ((dmin_client[N[j]]-xb) * random.sample(range(0, 1), 1))\n",
    "            j+=1\n",
    "            \n",
    "            while(j < len(N)):\n",
    "                #here on random xb\n",
    "                ind = N[j]\n",
    "                x_new = x_new + ((dmin_client[ind]-xb) * random.sample(range(0, 1), 1))\n",
    "                j += 1\n",
    "                \n",
    "            x_new = x_new / (len(N)-1) ##??? why scaling with k here\n",
    "            Synthesized_instance = xb + x_new ##?> why we need to sum xb and x_new\n",
    "            \n",
    "            \n",
    "            #for algo 4 when finding nearest neighbors from min_np and assigning the \n",
    "            #'p' sensitive value to all synthesized instances\n",
    "            Synthesized_instance[sa_index] = xb[sa_index] \n",
    "            Sxb.append(Synthesized_instance)\n",
    "                    \n",
    "            \n",
    "        S.append(Sxb)    \n",
    "    \n",
    "    return S\n",
    "\n",
    "'''\n",
    "def kSMOTE(dmajor,dminor,k,r):\n",
    "    S = []\n",
    "    Ns = int(r * (len(dmajor) - len(dminor)))\n",
    "    Nks = int(Ns / k)\n",
    "    rb = []\n",
    "    #pick a random k sample from dmin and save them in rb\n",
    "    dmin_rand = random.sample(dminor, k)   \n",
    "    #for debugging\n",
    "    sens_attr_vals = []\n",
    "    \n",
    "    \n",
    "    #do algorithem (choose the nearest neighbor and linear interpolation)\n",
    "    for xb in dmin_rand:\n",
    "        N= k_nearest_neighbors(dminor,xb,k)\n",
    "        \n",
    "        \n",
    "        #do linear interpolation\n",
    "        Sxb = []\n",
    "        for s in range(Nks):\n",
    "            j = 1  ##?? j?\n",
    "            #randome k sample\n",
    "            #j = random.randint(0, len(N))\n",
    "            \n",
    "            ##here nearst neghber insted of dminor\n",
    "            #linear interpolation\n",
    "            x_new = ((dminor[N[j]]-xb) * random.sample(range(0, 1), 1))\n",
    "            j+=1\n",
    "            \n",
    "            while(j < len(N)):\n",
    "                #here on random xb\n",
    "                ind = N[j]\n",
    "                x_new = abs(x_new + ((dminor[ind]-xb) * random.sample(range(0, 1), 1)))\n",
    "                j += 1\n",
    "                \n",
    "            x_new = x_new / len(N) ##??? why scaling with k here\n",
    "            Synthesized_instance = xb + x_new ##?> why we need to sum xb and x_new\n",
    "            \n",
    "            #for debugging\n",
    "            sens_attr_vals.append(Synthesized_instance[10])\n",
    "            \n",
    "            #Synthesized_instance[sa_index] = xb[sa_index] ##Smote is synthesizing the values as if they are numbers but sensitive attribute \n",
    "                                                          ## is binary so we need to keep it binary\n",
    "            Sxb.append(Synthesized_instance)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        S.append(Sxb)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return S\n",
    "\n",
    "def splitYtrain_smotenc(Xtr,Ytr,minority_lable):\n",
    "  \n",
    "    dmaj_x = []\n",
    "    dmin_x = []\n",
    "    label_maj_x = []\n",
    "    label_min_x = []\n",
    "    \n",
    "    for i in range(len(Ytr)):\n",
    "        if((Ytr[i]) == minority_lable):\n",
    "            dmin_x.append(Xtr[i])\n",
    "            label_min_x.append(Ytr[i])\n",
    "        else:\n",
    "            dmaj_x.append(Xtr[i])\n",
    "            label_maj_x.append(Ytr[i])\n",
    "    \n",
    "    return dmaj_x,dmin_x, label_min_x, label_maj_x\n",
    "def splitYtrain(Xtr,Ytr,minority_lable):\n",
    "    #print(Ytr)\n",
    "    dmaj_x = []\n",
    "    dmin_x = []\n",
    "    \n",
    "    \n",
    "    for i in range(len(Ytr)):\n",
    "        if((Ytr[i]) == minority_lable):\n",
    "            dmin_x.append(Xtr[i])\n",
    "            \n",
    "        else:\n",
    "            dmaj_x.append(Xtr[i])\n",
    "            \n",
    "    \n",
    "    return dmaj_x,dmin_x\n",
    "def splitYtrain_sa_value(Xtr,Ytr,minority_lable,majority_label): #splite Ytrain based on sensitive attribute value\n",
    "    #print(Ytr)\n",
    "    dmaj_p_x = []\n",
    "    dmaj_np_x = []\n",
    "    dmin_p_x = []\n",
    "    dmin_np_x = []\n",
    "    \n",
    "    \n",
    "    for i in range(len(Ytr)):\n",
    "        if((Ytr[i]) == minority_lable and (Xtr[i][sa_index])==p_Group): #select minority instances with \"protected\" value \n",
    "            dmin_p_x.append(Xtr[i])\n",
    "        elif((Ytr[i]) == minority_lable and (Xtr[i][sa_index])==np_Group): #select minority instances with \"protected\" value \n",
    "            dmin_np_x.append(Xtr[i])\n",
    "        elif ((Ytr[i]) == majority_label and (Xtr[i][sa_index])==p_Group): #select minority(positive class) instances with \"non-protected\" value\n",
    "            dmaj_p_x.append(Xtr[i])\n",
    "        elif ((Ytr[i]) == majority_label and (Xtr[i][sa_index])==np_Group): #select minority(positive class) instances with \"non-protected\" value\n",
    "            dmaj_np_x.append(Xtr[i])\n",
    "    \n",
    "    return dmin_p_x, dmin_np_x, dmaj_p_x, dmaj_np_x\n",
    "def get_statistics(Xtr,Ytr,minority_lable,majority_label): #splite Ytrain based on sensitive attribute value\n",
    "    #print(Ytr)\n",
    "    dmaj_p_x =0\n",
    "    dmaj_np_x = 0\n",
    "    dmin_p_x = 0\n",
    "    dmin_np_x = 0\n",
    "    \n",
    "    \n",
    "    for i in range(len(Ytr)):\n",
    "        if((Ytr[i]) == minority_lable and (Xtr[i][sa_index])==p_Group): #select minority instances with \"protected\" value \n",
    "            dmin_p_x+=1\n",
    "        elif((Ytr[i]) == minority_lable and (Xtr[i][sa_index])==np_Group): #select minority instances with \"protected\" value \n",
    "            dmin_np_x+=1\n",
    "        elif ((Ytr[i]) == majority_label and (Xtr[i][sa_index])==p_Group): #select minority(positive class) instances with \"non-protected\" value\n",
    "            dmaj_p_x+=1\n",
    "        elif ((Ytr[i]) == majority_label and (Xtr[i][sa_index])==np_Group): #select minority(positive class) instances with \"non-protected\" value\n",
    "            dmaj_np_x+=1\n",
    "    \n",
    "    return dmin_p_x, dmin_np_x, dmaj_p_x, dmaj_np_x\n",
    "\n",
    "def create_synth_data(clinet_traning_x, clinet_traning_y, minority_lable,majority_label,k,r,group):\n",
    "    \n",
    "    \n",
    "    \n",
    "    #create two data set from traning data (one for maj (ex.0 class) and one for min(ex.1 class)) \n",
    "    #for simple federated learning\n",
    "    dmaj_client,dmin_client = splitYtrain(clinet_traning_x,clinet_traning_y,minority_lable)\n",
    "    \n",
    "    #for fair federated learning\n",
    "    \n",
    "    \n",
    "    dmin_p_x, dmin_np_x, dmaj_p_x, dmaj_np_x = splitYtrain_sa_value(clinet_traning_x,clinet_traning_y,minority_lable,majority_label)\n",
    "    \n",
    "    group_names = ['dmin_p_x', 'dmin_np_x', 'dmaj_p_x', 'dmaj_np_x']\n",
    "    group_label_dict = {'dmin_p_x':minority_label, 'dmin_np_x': minority_label, 'dmaj_p_x':  majority_label, 'dmaj_np_x': majority_label}\n",
    "    group_dict = {'dmin_p_x':dmin_p_x, 'dmin_np_x': dmin_np_x, 'dmaj_p_x':  dmaj_p_x, 'dmaj_np_x': dmaj_np_x}\n",
    "    \n",
    "    group_lengths = [len(dmin_p_x),len(dmin_np_x), len(dmaj_p_x), len(dmaj_np_x)] \n",
    "    \n",
    "    #group with maximum length\n",
    "    max_length_group = group_lengths.index(max(group_lengths))  \n",
    "    \n",
    "    #group name which has maximum length\n",
    "    max_group_name = group_names[max_length_group]\n",
    "    \n",
    "    #find the insances in the group with maximum length and store them in dmaj_x\n",
    "    for key, value in group_dict.items():\n",
    "        if key== max_group_name:\n",
    "            dmaj_x = value\n",
    "            break\n",
    "    Xtr_new = []\n",
    "    Ytr_new = []  \n",
    "    \n",
    "    '''\n",
    "    ##Algo 4:\n",
    "    dmaj_x = dmaj_p_x\n",
    "    dmin_x = dmin_p_x\n",
    "    #dmin_np = dmin_np_x\n",
    "    x_syn = fair_kSMOTE(dmaj_x,dmin_np_x,dmin_x,dmin_client,k,r)\n",
    "    # add the created synthatic data to the traning data\n",
    "    # here merrge old traning data with the new synthatic data\n",
    "    new_label = minority_label\n",
    "    for j in x_syn:\n",
    "        for s in j:\n",
    "            Xtr_new.append(s)\n",
    "            Ytr_new.append(new_label)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    ##Algo 3:\n",
    "    \n",
    "     ##Algo 3:\n",
    "    if group =='min_p':\n",
    "        dmaj_x = dmaj_p_x\n",
    "        dmin_x = dmin_p_x\n",
    "        \n",
    "        #dmin_np = dmin_np_x\n",
    "        x_syn = fair_kSMOTE(dmaj_x,dmin_np_x,dmin_x,k,r)\n",
    "        #x_syn = fair_kSMOTE_algo_2(dmaj_x,dmin_x,k,r)\n",
    "        # add the created synthatic data to the traning data\n",
    "        # here merrge old traning data with the new synthatic data\n",
    "        new_label = minority_label\n",
    "        for j in x_syn:\n",
    "            for s in j:\n",
    "                Xtr_new.append(s)\n",
    "                Ytr_new.append(new_label)\n",
    "        \n",
    "    elif group =='maj_np':\n",
    "        dmaj_x = dmin_np_x\n",
    "        dmin_x = dmaj_np_x\n",
    "        #dmin_np = dmin_np_x\n",
    "        x_syn = fair_kSMOTE_algo_2(dmaj_x,dmin_x,k,r)\n",
    "        \n",
    "        \n",
    "        # add the created synthatic data to the traning data\n",
    "        # here merrge old traning data with the new synthatic data\n",
    "        new_label = majority_label\n",
    "        for j in x_syn:\n",
    "            for s in j:\n",
    "                Xtr_new.append(s)\n",
    "                Ytr_new.append(new_label)\n",
    "    \n",
    "    ##Algo 2:\n",
    "    '''\n",
    "    dmaj_x = dmaj_p_x\n",
    "    dmin_x = dmin_p_x\n",
    "    x_syn = fair_kSMOTE(dmaj_x,dmin_client,dmin_x,k,r)\n",
    "    # add the created synthatic data to the traning data\n",
    "    # here merrge old traning data with the new synthatic data\n",
    "    new_label = minority_label\n",
    "    \n",
    "    for j in x_syn:\n",
    "        for s in j:\n",
    "            Xtr_new.append(s)\n",
    "            Ytr_new.append(new_label)\n",
    "    \n",
    "    '''\n",
    "    ##Algo1:\n",
    "    '''\n",
    "    for key, value in group_dict.items():\n",
    "        if key!= max_group_name:  \n",
    "            dmin_x = value\n",
    "            x_syn = fair_kSMOTE(dmaj_x,dmin_x,k,r)\n",
    "            # add the created synthatic data to the traning data\n",
    "            \n",
    "            # here merrge old traning data with the new synthatic data\n",
    "            new_label = group_label_dict[key]\n",
    "            i = 0\n",
    "            for j in x_syn:\n",
    "                for s in j:\n",
    "                    Xtr_new.append(s)\n",
    "                    Ytr_new.append(new_label)\n",
    "                    i+=1\n",
    "            #print(\"i%s\"%i)\n",
    "    '''        \n",
    "    '''    \n",
    "    for k in clinet_traning_x:\n",
    "        Xtr_new.append(k)\n",
    "        \n",
    "    for k in clinet_traning_y:\n",
    "        Ytr_new.append(k)\n",
    "    Xtr_new = np.array(Xtr_new)\n",
    "    Ytr_new = np.array(Ytr_new) \n",
    "    '''\n",
    "    \n",
    "    return Xtr_new,Ytr_new\n",
    "def k_nearest_neighbors_modified(data, predict, k,data_rand_index):\n",
    "    distances = []\n",
    "    count=0\n",
    "    for sample in data:\n",
    "        if count in data_rand_index:\n",
    "            count+=1\n",
    "        else:\n",
    "            euclidean_distance = np.linalg.norm(np.array(sample)-np.array(predict))\n",
    "            distances.append([euclidean_distance,count])\n",
    "            count+=1\n",
    "    '''        \n",
    "        euclidean_distance = np.linalg.norm(np.array(sample)-np.array(predict))\n",
    "        distances.append([euclidean_distance,count])\n",
    "        count+=1\n",
    "    sorted_distances = sorted(distances)\n",
    "    if sorted_distances[0][1] in data_rand_index:\n",
    "        sorted_distances.pop(0)\n",
    "    if sorted_distances[0][1] in data_rand_index:\n",
    "        sorted_distances.pop(0)\n",
    "    votes = [i[1] for i in sorted_distances [:k]]\n",
    "    '''\n",
    "    votes = [i[1] for i in sorted(distances)[:k]] ##votes is returning indexes of k random samples\n",
    "    #vote_result = Counter(votes).most_common(9)[0][0]\n",
    "    return votes\n",
    "def downsample_utility_function(clinet_traning_x, clinet_traning_y,data,data_index, k,r,label,reduction_amount):\n",
    "    S = []\n",
    "    #pick a random k sample from data which we want to downsample\n",
    "    d_rand = []\n",
    "    d_rand_index = []\n",
    "    \n",
    "    #reduction_amount = int(r*len(data))  \n",
    "    #print(\"reduction amount: %s\" % reduction_amount)\n",
    "    \n",
    "    for i in range(reduction_amount):\n",
    "        index = random.randint(0, (len(data)-1))\n",
    "        #random.sample(range(0,len(data)),1)\n",
    "        \n",
    "        d_rand.append(data[index])\n",
    "        d_rand_index.append(index)\n",
    "    data_rand_index = list(d_rand_index)\n",
    "    Sxb = []   \n",
    "    sens_attr_vals = []\n",
    "    k = 3 ##number of nearest neighbours\n",
    "    indices_neighbours_removed = list(d_rand_index)\n",
    "    #print(len(d_rand_index))\n",
    "    #do algorithm (choose the nearest neighbor and linear interpolation)\n",
    "    \n",
    "    Nks=1\n",
    "    S=[]\n",
    "    #do algorithem (choose the nearest neighbor and linear interpolation)\n",
    "    for xb in d_rand:\n",
    "        #N= k_nearest_neighbors(data,xb,k)\n",
    "        N= k_nearest_neighbors_modified(data,xb,k,data_rand_index)\n",
    "        \n",
    "        #do linear interpolation\n",
    "        Sxb = []\n",
    "        for s in range(Nks):\n",
    "            j = 0 #j = 1\n",
    "            #linear interpolation\n",
    "            x_new = ((data[N[j]]-xb) * random.sample(range(0, 1), 1))\n",
    "            indices_neighbours_removed.append(N[1])\n",
    "            j+=1\n",
    "            \n",
    "            while(j < len(N)-1):\n",
    "                #here on random xb\n",
    "                ind = N[j]\n",
    "                x_new = x_new + ((data[ind]-xb) * random.sample(range(0, 1), 1))\n",
    "                j += 1\n",
    "                \n",
    "            x_new = x_new / (len(N)-1) \n",
    "            Synthesized_instance = xb + x_new \n",
    "            Synthesized_instance[sa_index] = xb[sa_index] \n",
    "            \n",
    "            Sxb.append(Synthesized_instance)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        S.append(Sxb)\n",
    "    #print(indices_neighbours_removed)\n",
    "    \n",
    "    \n",
    "    for i in range(len(indices_neighbours_removed)):\n",
    "        indx = indices_neighbours_removed[i]\n",
    "        indices_neighbours_removed[i] = data_index[indx]\n",
    "    indices = np.unique(indices_neighbours_removed)\n",
    "    indices = list(indices)\n",
    "    difference = (reduction_amount*2)-len(indices)\n",
    "    \n",
    "        \n",
    "    #for i in range(0,difference-1):\n",
    "    #    indices.pop(0)\n",
    "    #print(\"reduced amount: %s\" % len(indices))\n",
    "    \n",
    "    indices.sort()\n",
    "    \n",
    "    Xtr = []\n",
    "    Ytr = []\n",
    "    for k in clinet_traning_x:\n",
    "        Xtr.append(k)\n",
    "        \n",
    "    for k in clinet_traning_y:\n",
    "        Ytr.append(k)\n",
    "    \n",
    "    for i in range(len(indices)):\n",
    "        indx = indices[i]-i\n",
    "        Xtr.pop(indx)\n",
    "        Ytr.pop(indx)\n",
    "    \n",
    "    for j in S:\n",
    "            for s in j:\n",
    "                Xtr.append(s)\n",
    "                Ytr.append(label)\n",
    "    '''\n",
    "    indices=[]\n",
    "    for i in range(difference):\n",
    "        index = random.randint(0, (len(Xtr)-1))\n",
    "        indices.append(index)\n",
    "    indices.sort()\n",
    "    \n",
    "    for i in range(len(indices)):\n",
    "        indx = indices[i]-i\n",
    "        Xtr.pop(indx)\n",
    "        Ytr.pop(indx)\n",
    "    '''    \n",
    "    Xtr = np.array(Xtr)\n",
    "    Ytr = np.array(Ytr)\n",
    "    return Xtr, Ytr\n",
    "def splitYtrain_sa_value_index(Xtr,Ytr,minority_lable,majority_label): #split data based on sensitive attribute value\n",
    "    #print(Ytr)\n",
    "    dmaj_p_x = []\n",
    "    dmaj_p_index = [] ##index of maj_p instance in the main dataset\n",
    "    \n",
    "    dmaj_np_x = []\n",
    "    dmaj_np_index = [] ##index of maj_np instance in the main dataset\n",
    "    \n",
    "    dmin_p_x = []\n",
    "    dmin_p_index = [] ##index of min_p instance in the main dataset\n",
    "    \n",
    "    dmin_np_x = []\n",
    "    dmin_np_index = [] ##index of min_np instance in the main dataset\n",
    "    \n",
    "    \n",
    "    for i in range(len(Ytr)):\n",
    "        if((Ytr[i]) == minority_lable and (Xtr[i][sa_index])==p_Group): #select minority instances with \"protected\" value \n",
    "            dmin_p_x.append(Xtr[i])\n",
    "            dmin_p_index.append(i)\n",
    "        elif((Ytr[i]) == minority_lable and (Xtr[i][sa_index])==np_Group): #select minority instances with \"protected\" value \n",
    "            dmin_np_x.append(Xtr[i])\n",
    "            dmin_np_index.append(i)\n",
    "        elif ((Ytr[i]) == majority_label and (Xtr[i][sa_index])==p_Group): #select minority(positive class) instances with \"non-protected\" value\n",
    "            dmaj_p_x.append(Xtr[i])\n",
    "            dmaj_p_index.append(i)\n",
    "        elif ((Ytr[i]) == majority_label and (Xtr[i][sa_index])==np_Group): #select minority(positive class) instances with \"non-protected\" value\n",
    "            dmaj_np_x.append(Xtr[i])\n",
    "            dmaj_np_index.append(i)\n",
    "    \n",
    "    return dmin_p_x, dmin_np_x, dmaj_p_x, dmaj_np_x, dmin_p_index, dmin_np_index, dmaj_p_index, dmaj_np_index\n",
    "\n",
    "def downsample(clinet_traning_x, clinet_traning_y, minority_lable,majority_label,k,r,group):\n",
    "    \n",
    "    #group: maj_p, maj_np, min_p, min_np\n",
    "    \n",
    "    dmaj_client,dmin_client = splitYtrain(clinet_traning_x,clinet_traning_y,minority_lable)\n",
    "   \n",
    "    #for fair federated learning\n",
    "    \n",
    "    dmin_p_x, dmin_np_x, dmaj_p_x, dmaj_np_x, dmin_p_index, dmin_np_index, dmaj_p_index, dmaj_np_index = splitYtrain_sa_value_index(clinet_traning_x,clinet_traning_y,minority_lable,majority_label)\n",
    "    \n",
    "    ##Algo 5:\n",
    "    if group =='min_np':\n",
    "        \n",
    "        label = minority_label\n",
    "        data = dmin_np_x\n",
    "        reduction_amount = int(r*len(data))\n",
    "        #print(\"reduction amount: %s\" % reduction_amount)\n",
    "        x_small,y_small = downsample_utility_function(clinet_traning_x, clinet_traning_y,data,dmin_np_index, k,r,label,reduction_amount)\n",
    "        \n",
    "        #difference = len(clinet_traning_x) - len(x_small)\n",
    "        #if difference+2<reduction_amount:\n",
    "        #    reduction_amount = reduction_amount-difference\n",
    "        #    x_small,y_small = downsample_utility_function(x_small,y_small,data,dmin_np_index, k,r,label,reduction_amount)\n",
    "        \n",
    "        \n",
    "        # add the created synthatic data to the traning data\n",
    "        # here merrge old traning data with the new synthatic data\n",
    "        #new_label = minority_label\n",
    "        #for j in x_small:\n",
    "        #    for s in j:\n",
    "        #        Xtr_new.append(s)\n",
    "        #        Ytr_new.append(new_label)\n",
    "        \n",
    "    elif group =='maj_p':\n",
    "        label = majority_label\n",
    "        data = dmaj_p_x\n",
    "        reduction_amount = int(r*len(data))\n",
    "        #print(\"reduction amount: %s\" % reduction_amount)\n",
    "        x_small,y_small = downsample_utility_function(clinet_traning_x, clinet_traning_y,data,dmaj_p_index, k,r,label,reduction_amount)\n",
    "        # add the created synthatic data to the traning data\n",
    "        # here merrge old traning data with the new synthatic data\n",
    "        #difference = len(clinet_traning_x) - len(x_small)\n",
    "        #if difference+2<reduction_amount:\n",
    "        #    reduction_amount = reduction_amount-difference\n",
    "        #    x_small,y_small = downsample_utility_function(x_small,y_small,data,dmin_np_index, k,r,label,reduction_amount)\n",
    "    return x_small,y_small\n",
    "def create_synth_data_from_smote_nc(clinet_traning_x, clinet_traning_y, minority_lable,majority_label,k,r):\n",
    "    #create two data set from traning data (one for maj (ex.0 class) and one for min(ex.1 class)) \n",
    "    \n",
    "    #for simple federated learning\n",
    "    #dmaj_x,dmin_x = splitYtrain(clinet_traning_x,clinet_traning_y,minority_lable)\n",
    "    \n",
    "    #for fair federated learning\n",
    "    CAT_VARIABLES_INDICES = [1,2,3,4,6,7,8,10,14,15]\n",
    "    dmaj_x,dmin_x,label_min_x,label_maj_x = splitYtrain_smotenc(clinet_traning_x,clinet_traning_y,minority_lable)\n",
    "    smote_nc = SMOTENC(categorical_features=CAT_VARIABLES_INDICES, sampling_strategy = r, random_state=0)\n",
    "    X_resampled, y_resampled = smote_nc.fit_resample(clinet_traning_x, clinet_traning_y)\n",
    "    \n",
    "    return X_resampled,y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b06b366",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.0015686274509804088\n",
      "balanced_accuracy_client 0.7627480158730159\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.05713319810682893\n",
      "balanced_accuracy_client 0.7814262545084845\n",
      "eqop: 0.03989181879648418\n",
      "eqop: 0.03989181879648418\n",
      "eqop: 0.03989181879648418\n",
      "eqop: 0.05713319810682893\n",
      "1\n",
      "eqop: 0.05713319810682893\n",
      "eqop: 0.03989181879648418\n",
      "eqop: 0.05713319810682893\n",
      "2\n",
      "eqop: 0.05713319810682893\n",
      "eqop: 0.05713319810682893\n",
      "3\n",
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: -0.047929367248265686\n",
      "balanced_accuracy_client 0.7681878825929995\n",
      "comm_round: 0 | global_acc: 0.7263974324685745 | global_loss: 5.744041442871094\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: 0.04235294117647059\n",
      "eqop: 0.029019607843137174\n",
      "eqop: 0.09568627450980394\n",
      "1\n",
      "eqop: 0.10901960784313725\n",
      "2\n",
      "eqop: 0.09568627450980394\n",
      "eqop: 0.12235294117647055\n",
      "3\n",
      "client_1\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.07437457741717379\n",
      "eqop: 0.09161595672751865\n",
      "eqop: 0.05713319810682893\n",
      "eqop: 0.05713319810682893\n",
      "1\n",
      "eqop: 0.05713319810682893\n",
      "eqop: 0.03989181879648418\n",
      "eqop: 0.05713319810682893\n",
      "2\n",
      "eqop: 0.05713319810682893\n",
      "eqop: 0.03989181879648418\n",
      "eqop: 0.05713319810682893\n",
      "3\n",
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: -0.018919487071683783\n",
      "comm_round: 1 | global_acc: 0.7360256753142551 | global_loss: 5.632513999938965\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: 0.04235294117647059\n",
      "eqop: 0.04941176470588238\n",
      "eqop: 0.06901960784313721\n",
      "1\n",
      "eqop: 0.06901960784313721\n",
      "eqop: 0.06901960784313721\n",
      "eqop: 0.08235294117647052\n",
      "2\n",
      "eqop: 0.10901960784313725\n",
      "3\n",
      "client_1\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.05713319810682893\n",
      "eqop: 0.05713319810682893\n",
      "eqop: 0.07437457741717379\n",
      "1\n",
      "eqop: 0.07437457741717379\n",
      "eqop: 0.03989181879648418\n",
      "eqop: 0.05713319810682893\n",
      "2\n",
      "eqop: 0.03989181879648418\n",
      "eqop: 0.07437457741717379\n",
      "3\n",
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: -0.018919487071683783\n",
      "comm_round: 2 | global_acc: 0.7550147098154587 | global_loss: 5.386497974395752\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: 0.0031372549019608176\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.05713319810682893\n",
      "eqop: 0.03989181879648418\n",
      "eqop: 0.03989181879648418\n",
      "1\n",
      "eqop: 0.03989181879648418\n",
      "eqop: 0.03989181879648418\n",
      "eqop: 0.03989181879648418\n",
      "2\n",
      "eqop: 0.03989181879648418\n",
      "eqop: 0.05713319810682893\n",
      "3\n",
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: -0.018919487071683783\n",
      "comm_round: 3 | global_acc: 0.765177855041455 | global_loss: 5.255289077758789\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: 0.016470588235294126\n",
      "eqop: 0.0760784313725491\n",
      "eqop: 0.06901960784313721\n",
      "eqop: 0.06980392156862747\n",
      "1\n",
      "eqop: 0.10901960784313725\n",
      "2\n",
      "eqop: 0.10901960784313725\n",
      "3\n",
      "client_1\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.05713319810682893\n",
      "eqop: 0.03989181879648418\n",
      "eqop: 0.03989181879648418\n",
      "eqop: 0.05713319810682893\n",
      "1\n",
      "eqop: 0.03989181879648418\n",
      "eqop: 0.03989181879648418\n",
      "2\n",
      "eqop: 0.03989181879648418\n",
      "eqop: 0.03989181879648418\n",
      "3\n",
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: 0.01009039310489801\n",
      "eqop: 0.03994113937355481\n",
      "eqop: 0.03994113937355481\n",
      "eqop: 0.03994113937355481\n",
      "eqop: 0.055707378599958\n",
      "1\n",
      "eqop: 0.02501576623922641\n",
      "eqop: 0.0407820054656296\n",
      "2\n",
      "eqop: 0.055707378599958\n",
      "3\n",
      "client_3\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "comm_round: 4 | global_acc: 0.7683872693233484 | global_loss: 5.1962456703186035\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.0031372549019608176\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.05713319810682893\n",
      "eqop: 0.07437457741717379\n",
      "eqop: 0.03989181879648418\n",
      "eqop: 0.03989181879648418\n",
      "eqop: 0.03989181879648418\n",
      "1\n",
      "eqop: 0.03989181879648418\n",
      "eqop: 0.03989181879648418\n",
      "2\n",
      "eqop: 0.03989181879648418\n",
      "3\n",
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: 0.010931259196972909\n",
      "eqop: 0.02585663233130131\n",
      "eqop: 0.02585663233130131\n",
      "eqop: 0.03994113937355481\n",
      "1\n",
      "eqop: 0.011772125289047808\n",
      "eqop: 0.03994113937355481\n",
      "2\n",
      "eqop: 0.0407820054656296\n",
      "3\n",
      "client_3\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "comm_round: 5 | global_acc: 0.7737362931265044 | global_loss: 5.117520332336426\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: 0.0235294117647058\n",
      "eqop: 0.06274509803921569\n",
      "eqop: 0.06274509803921569\n",
      "eqop: 0.06274509803921569\n",
      "1\n",
      "eqop: 0.06274509803921569\n",
      "eqop: 0.06274509803921569\n",
      "eqop: 0.06274509803921569\n",
      "2\n",
      "eqop: 0.04941176470588238\n",
      "eqop: 0.06274509803921569\n",
      "3\n",
      "client_1\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.05713319810682893\n",
      "eqop: 0.05713319810682893\n",
      "eqop: 0.03989181879648418\n",
      "eqop: 0.03989181879648418\n",
      "eqop: 0.03989181879648418\n",
      "eqop: 0.03989181879648418\n",
      "eqop: 0.03989181879648418\n",
      "eqop: 0.03989181879648418\n",
      "eqop: 0.03989181879648418\n",
      "1\n",
      "eqop: 0.03989181879648418\n",
      "eqop: 0.03989181879648418\n",
      "eqop: 0.03989181879648418\n",
      "eqop: 0.03989181879648418\n",
      "2\n",
      "eqop: 0.03989181879648418\n",
      "eqop: 0.03989181879648418\n",
      "eqop: 0.020283975659229236\n",
      "eqop: 0.03989181879648418\n",
      "3\n",
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: 0.03994113937355481\n",
      "eqop: -0.0023123817532058055\n",
      "client_3\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "comm_round: 6 | global_acc: 0.7812249264509227 | global_loss: 5.025673866271973\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: 0.0039215686274509665\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.05713319810682893\n",
      "eqop: 0.05713319810682893\n",
      "eqop: 0.05713319810682893\n",
      "1\n",
      "eqop: 0.03989181879648418\n",
      "eqop: 0.03989181879648418\n",
      "eqop: 0.03989181879648418\n",
      "2\n",
      "eqop: 0.03989181879648418\n",
      "eqop: 0.03989181879648418\n",
      "3\n",
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: 0.054025646415808315\n",
      "eqop: 0.054025646415808315\n",
      "eqop: 0.0689510195501366\n",
      "1\n",
      "eqop: 0.0697918856422115\n",
      "2\n",
      "eqop: 0.0697918856422115\n",
      "eqop: 0.0697918856422115\n",
      "eqop: 0.0697918856422115\n",
      "3\n",
      "client_3\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "comm_round: 7 | global_acc: 0.7788178657395025 | global_loss: 5.061756610870361\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.009411764705882342\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.05713319810682893\n",
      "eqop: 0.07437457741717379\n",
      "eqop: 0.07437457741717379\n",
      "1\n",
      "eqop: 0.03989181879648418\n",
      "eqop: 0.03989181879648418\n",
      "2\n",
      "eqop: 0.03989181879648418\n",
      "eqop: 0.05713319810682893\n",
      "3\n",
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: 0.054025646415808315\n",
      "eqop: 0.0548665125078831\n",
      "eqop: 0.0689510195501366\n",
      "1\n",
      "eqop: 0.0689510195501366\n",
      "2\n",
      "eqop: 0.0697918856422115\n",
      "3\n",
      "client_3\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "comm_round: 8 | global_acc: 0.7788178657395025 | global_loss: 5.0551958084106445\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.009411764705882342\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.05713319810682893\n",
      "eqop: 0.03989181879648418\n",
      "eqop: 0.03989181879648418\n",
      "1\n",
      "eqop: 0.03989181879648418\n",
      "2\n",
      "eqop: 0.03989181879648418\n",
      "3\n",
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: 0.010931259196972909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eqop: 0.0416228715577045\n",
      "eqop: 0.0697918856422115\n",
      "1\n",
      "eqop: 0.0697918856422115\n",
      "eqop: 0.0697918856422115\n",
      "eqop: 0.0697918856422115\n",
      "eqop: 0.0697918856422115\n",
      "eqop: 0.0847172587765398\n",
      "2\n",
      "eqop: 0.0847172587765398\n",
      "eqop: 0.0847172587765398\n",
      "3\n",
      "client_3\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "comm_round: 9 | global_acc: 0.7766782562182402 | global_loss: 5.048635482788086\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.015686274509803977\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.07437457741717379\n",
      "eqop: 0.07437457741717379\n",
      "eqop: 0.07437457741717379\n",
      "1\n",
      "eqop: 0.07437457741717379\n",
      "eqop: 0.07437457741717379\n",
      "2\n",
      "eqop: 0.05476673427991885\n",
      "eqop: 0.03752535496957399\n",
      "eqop: 0.03752535496957399\n",
      "3\n",
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: 0.011772125289047808\n",
      "eqop: 0.026697498423376098\n",
      "eqop: 0.0416228715577045\n",
      "1\n",
      "eqop: 0.0416228715577045\n",
      "2\n",
      "eqop: 0.055707378599958\n",
      "3\n",
      "client_3\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "comm_round: 10 | global_acc: 0.7790853169296603 | global_loss: 5.025673866271973\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.029019607843137285\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.07437457741717379\n",
      "eqop: 0.05713319810682893\n",
      "eqop: 0.05713319810682893\n",
      "eqop: 0.03752535496957399\n",
      "eqop: 0.03752535496957399\n",
      "eqop: 0.03752535496957399\n",
      "1\n",
      "eqop: 0.03752535496957399\n",
      "eqop: 0.03752535496957399\n",
      "2\n",
      "eqop: 0.017917511832319155\n",
      "eqop: -0.0040567951318458695\n",
      "3\n",
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: -0.017237754887534207\n",
      "comm_round: 11 | global_acc: 0.7849692431131319 | global_loss: 4.940388202667236\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.03529411764705892\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.07437457741717379\n",
      "eqop: 0.05476673427991885\n",
      "eqop: 0.07437457741717379\n",
      "1\n",
      "eqop: 0.017917511832319155\n",
      "eqop: 0.017917511832319155\n",
      "eqop: -0.02129817444219073\n",
      "2\n",
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: -0.017237754887534207\n",
      "comm_round: 12 | global_acc: 0.786039047873763 | global_loss: 4.933827877044678\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.04862745098039223\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.07437457741717379\n",
      "eqop: 0.07437457741717379\n",
      "eqop: 0.05713319810682893\n",
      "eqop: 0.05713319810682893\n",
      "1\n",
      "eqop: 0.07674104124408376\n",
      "2\n",
      "eqop: 0.05713319810682893\n",
      "eqop: -0.0016903313049357882\n",
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: -0.017237754887534207\n",
      "comm_round: 13 | global_acc: 0.7855041454934475 | global_loss: 4.946948528289795\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: 0.0039215686274509665\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.07437457741717379\n",
      "eqop: 0.07674104124408376\n",
      "eqop: 0.05713319810682893\n",
      "eqop: -0.0016903313049357882\n",
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: 0.011772125289047808\n",
      "eqop: 0.055707378599958\n",
      "eqop: 0.07147361782636119\n",
      "1\n",
      "eqop: 0.0855581248686147\n",
      "2\n",
      "eqop: 0.1004834980029431\n",
      "3\n",
      "client_3\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "comm_round: 14 | global_acc: 0.7788178657395025 | global_loss: 5.048635482788086\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.029019607843137285\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.07674104124408376\n",
      "eqop: -0.0016903313049357882\n",
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: -0.03216312802186261\n",
      "comm_round: 15 | global_acc: 0.7822947312115539 | global_loss: 5.012552738189697\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.009411764705882342\n",
      "WARNING:tensorflow:5 out of the last 22 calls to <function Model.make_test_function.<locals>.test_function at 0x000001BCAC2D2C80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.09398242055442863\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x000001BCAACF0950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "eqop: -0.0040567951318458695\n",
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: -0.0470885011561909\n",
      "comm_round: 16 | global_acc: 0.7940625835784969 | global_loss: 4.822300910949707\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.015686274509803977\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.05476673427991885\n",
      "eqop: 0.03752535496957399\n",
      "eqop: 0.05713319810682893\n",
      "1\n",
      "eqop: 0.017917511832319155\n",
      "eqop: -0.0016903313049357882\n",
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: 0.055707378599958\n",
      "eqop: 0.0407820054656296\n",
      "eqop: 0.0855581248686147\n",
      "1\n",
      "eqop: 0.0855581248686147\n",
      "eqop: 0.11540887113727138\n",
      "2\n",
      "eqop: 0.1294933781795249\n",
      "3\n",
      "client_3\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "comm_round: 17 | global_acc: 0.788713559775341 | global_loss: 4.914146423339844\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.009411764705882342\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.07437457741717379\n",
      "eqop: 0.07674104124408376\n",
      "eqop: 0.017917511832319155\n",
      "eqop: -0.02129817444219073\n",
      "1\n",
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: 0.0407820054656296\n",
      "eqop: 0.0855581248686147\n",
      "eqop: 0.11540887113727138\n",
      "1\n",
      "eqop: 0.11540887113727138\n",
      "eqop: 0.11540887113727138\n",
      "2\n",
      "eqop: 0.11540887113727138\n",
      "3\n",
      "client_3\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "comm_round: 18 | global_acc: 0.7876437550147098 | global_loss: 4.901025295257568\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.04235294117647059\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.03515889114266402\n",
      "eqop: 0.0006761325219744041\n",
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: 0.012612991381122485\n",
      "eqop: 0.0855581248686147\n",
      "eqop: 0.0855581248686147\n",
      "eqop: 0.0855581248686147\n",
      "eqop: 0.0996426319108682\n",
      "1\n",
      "eqop: 0.1294933781795249\n",
      "2\n",
      "eqop: 0.1444187513138533\n",
      "3\n",
      "client_3\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "comm_round: 19 | global_acc: 0.7793527681198181 | global_loss: 5.028954029083252\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.0556862745098039\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.05713319810682893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eqop: 0.017917511832319155\n",
      "eqop: -0.0016903313049357882\n",
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: 0.012612991381122485\n",
      "eqop: 0.05654824469203279\n",
      "eqop: 0.0855581248686147\n",
      "1\n",
      "eqop: 0.1004834980029431\n",
      "2\n",
      "eqop: 0.1004834980029431\n",
      "3\n",
      "client_3\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "comm_round: 20 | global_acc: 0.7817598288312383 | global_loss: 4.979751110076904\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.04235294117647059\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.017917511832319155\n",
      "eqop: -0.0016903313049357882\n",
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: 0.012612991381122485\n",
      "eqop: 0.0407820054656296\n",
      "eqop: 0.0855581248686147\n",
      "1\n",
      "eqop: 0.0855581248686147\n",
      "eqop: 0.11540887113727138\n",
      "2\n",
      "eqop: 0.11540887113727138\n",
      "eqop: 0.1294933781795249\n",
      "3\n",
      "client_3\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "comm_round: 21 | global_acc: 0.7817598288312383 | global_loss: 4.999432563781738\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.06901960784313732\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: -0.0016903313049357882\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: 0.011772125289047808\n",
      "eqop: 0.05654824469203279\n",
      "eqop: 0.07147361782636119\n",
      "1\n",
      "eqop: 0.07147361782636119\n",
      "eqop: 0.08639899096068959\n",
      "2\n",
      "eqop: 0.11540887113727138\n",
      "3\n",
      "client_3\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "comm_round: 22 | global_acc: 0.7764108050280824 | global_loss: 5.084717750549316\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.06901960784313732\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: -0.0016903313049357882\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: -0.003994113937355492\n",
      "comm_round: 23 | global_acc: 0.783364535972185 | global_loss: 5.005992889404297\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.03607843137254896\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.020283975659229236\n",
      "eqop: -0.0016903313049357882\n",
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: 0.02585663233130131\n",
      "eqop: 0.042463737649779176\n",
      "eqop: 0.0855581248686147\n",
      "1\n",
      "eqop: 0.1004834980029431\n",
      "2\n",
      "eqop: 0.1004834980029431\n",
      "3\n",
      "client_3\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "comm_round: 24 | global_acc: 0.7865739502540786 | global_loss: 4.927267551422119\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.04235294117647059\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.017917511832319155\n",
      "eqop: -0.0040567951318458695\n",
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: -0.0023123817532058055\n",
      "comm_round: 25 | global_acc: 0.7884461085851832 | global_loss: 4.897745132446289\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.029019607843137285\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: -0.0016903313049357882\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: 0.011772125289047808\n",
      "eqop: 0.08639899096068959\n",
      "eqop: 0.08639899096068959\n",
      "1\n",
      "eqop: 0.1004834980029431\n",
      "2\n",
      "eqop: 0.1145680050451966\n",
      "3\n",
      "client_3\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "comm_round: 26 | global_acc: 0.7830970847820273 | global_loss: 4.976470470428467\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.04235294117647059\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.017917511832319155\n",
      "eqop: -0.0016903313049357882\n",
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: 0.011772125289047808\n",
      "eqop: 0.0416228715577045\n",
      "eqop: 0.07147361782636119\n",
      "1\n",
      "eqop: 0.07147361782636119\n",
      "eqop: 0.1145680050451966\n",
      "2\n",
      "eqop: 0.1145680050451966\n",
      "3\n",
      "client_3\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "comm_round: 27 | global_acc: 0.7817598288312383 | global_loss: 4.99943208694458\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.0556862745098039\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.017917511832319155\n",
      "eqop: 0.015551048005409074\n",
      "eqop: 0.015551048005409074\n",
      "1\n",
      "eqop: -0.0040567951318458695\n",
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: 0.011772125289047808\n",
      "eqop: 0.0416228715577045\n",
      "eqop: 0.05654824469203279\n",
      "1\n",
      "eqop: 0.07147361782636119\n",
      "2\n",
      "eqop: 0.07147361782636119\n",
      "eqop: 0.0855581248686147\n",
      "3\n",
      "client_3\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "comm_round: 28 | global_acc: 0.7879112062048677 | global_loss: 4.8911848068237305\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.04235294117647059\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: -0.02129817444219073\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: -0.016396888795459308\n",
      "comm_round: 29 | global_acc: 0.7814923776410805 | global_loss: 5.015833377838135\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.03607843137254896\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: -0.0016903313049357882\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: 0.03994113937355481\n",
      "eqop: 0.07147361782636119\n",
      "eqop: 0.07147361782636119\n",
      "eqop: 0.0855581248686147\n",
      "1\n",
      "eqop: 0.0855581248686147\n",
      "eqop: 0.1004834980029431\n",
      "2\n",
      "eqop: 0.1004834980029431\n",
      "eqop: 0.1004834980029431\n",
      "3\n",
      "client_3\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "comm_round: 30 | global_acc: 0.7855041454934475 | global_loss: 4.946949005126953\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.04235294117647059\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.017917511832319155\n",
      "eqop: -0.0016903313049357882\n",
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: 0.011772125289047808\n",
      "eqop: 0.05654824469203279\n",
      "eqop: 0.07147361782636119\n",
      "1\n",
      "eqop: 0.0855581248686147\n",
      "2\n",
      "eqop: 0.0855581248686147\n",
      "eqop: 0.0855581248686147\n",
      "eqop: 0.0855581248686147\n",
      "3\n",
      "client_3\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "comm_round: 31 | global_acc: 0.7822947312115539 | global_loss: 5.012552738189697\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.06901960784313732\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.017917511832319155\n",
      "eqop: -0.0040567951318458695\n",
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: -0.018078620979608995\n",
      "comm_round: 32 | global_acc: 0.7895159133458144 | global_loss: 4.897745132446289\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.02274509803921565\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: -0.0016903313049357882\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: 0.011772125289047808\n",
      "eqop: 0.07147361782636119\n",
      "eqop: 0.07147361782636119\n",
      "1\n",
      "eqop: 0.08639899096068959\n",
      "2\n",
      "eqop: 0.10132436409501788\n",
      "3\n",
      "client_3\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "comm_round: 33 | global_acc: 0.7863064990639208 | global_loss: 4.923986911773682\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.04235294117647059\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.03515889114266402\n",
      "eqop: 0.032792427315753936\n",
      "eqop: -0.0016903313049357882\n",
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: -0.0023123817532058055\n",
      "comm_round: 34 | global_acc: 0.7852366943032897 | global_loss: 4.956789493560791\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.02274509803921565\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.017917511832319155\n",
      "eqop: -0.0016903313049357882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: 0.011772125289047808\n",
      "eqop: 0.07147361782636119\n",
      "eqop: 0.07147361782636119\n",
      "eqop: 0.08639899096068959\n",
      "1\n",
      "eqop: 0.10132436409501788\n",
      "2\n",
      "eqop: 0.0855581248686147\n",
      "eqop: 0.1444187513138533\n",
      "3\n",
      "client_3\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "comm_round: 35 | global_acc: 0.7798876705001337 | global_loss: 5.042075157165527\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.06274509803921569\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.017917511832319155\n",
      "eqop: 0.015551048005409074\n",
      "eqop: -0.0040567951318458695\n",
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: 0.011772125289047808\n",
      "eqop: 0.026697498423376098\n",
      "eqop: 0.05654824469203279\n",
      "1\n",
      "eqop: 0.0855581248686147\n",
      "2\n",
      "eqop: 0.07147361782636119\n",
      "eqop: 0.07147361782636119\n",
      "3\n",
      "client_3\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "comm_round: 36 | global_acc: 0.7857715966836052 | global_loss: 4.923986911773682\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.06901960784313732\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: -0.02129817444219073\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: -0.03048139583771281\n",
      "comm_round: 37 | global_acc: 0.7796202193099759 | global_loss: 5.032234191894531\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.0556862745098039\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.017917511832319155\n",
      "eqop: -0.040906017579445564\n",
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: 0.02585663233130131\n",
      "eqop: 0.07147361782636119\n",
      "eqop: 0.05654824469203279\n",
      "eqop: 0.07147361782636119\n",
      "1\n",
      "eqop: 0.08639899096068959\n",
      "2\n",
      "eqop: 0.10132436409501788\n",
      "3\n",
      "client_3\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "comm_round: 38 | global_acc: 0.7849692431131319 | global_loss: 4.933828353881836\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.0556862745098039\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: -0.0016903313049357882\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: -0.03048139583771281\n",
      "comm_round: 39 | global_acc: 0.7852366943032897 | global_loss: 4.950229167938232\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.04235294117647059\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.017917511832319155\n",
      "eqop: -0.0016903313049357882\n",
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: 0.011772125289047808\n",
      "eqop: 0.10132436409501788\n",
      "eqop: 0.07147361782636119\n",
      "eqop: 0.07147361782636119\n",
      "eqop: 0.0855581248686147\n",
      "1\n",
      "eqop: 0.08639899096068959\n",
      "2\n",
      "eqop: 0.08639899096068959\n",
      "eqop: 0.07147361782636119\n",
      "eqop: 0.1004834980029431\n",
      "3\n",
      "client_3\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "comm_round: 40 | global_acc: 0.7828296335918695 | global_loss: 4.986311435699463\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.04235294117647059\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.017917511832319155\n",
      "eqop: -0.0016903313049357882\n",
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: 0.02585663233130131\n",
      "eqop: 0.07147361782636119\n",
      "eqop: 0.08639899096068959\n",
      "1\n",
      "eqop: 0.08639899096068959\n",
      "2\n",
      "eqop: 0.08639899096068959\n",
      "eqop: 0.08639899096068959\n",
      "eqop: 0.1145680050451966\n",
      "3\n",
      "client_3\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "comm_round: 41 | global_acc: 0.7836319871623428 | global_loss: 4.969910144805908\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.04235294117647059\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.017917511832319155\n",
      "eqop: 0.015551048005409074\n",
      "eqop: 0.015551048005409074\n",
      "eqop: 0.032792427315753936\n",
      "1\n",
      "eqop: 0.013184584178498993\n",
      "eqop: 0.013184584178498993\n",
      "eqop: -0.0040567951318458695\n",
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: 0.011772125289047808\n",
      "eqop: 0.08639899096068959\n",
      "eqop: 0.07147361782636119\n",
      "eqop: 0.08639899096068959\n",
      "1\n",
      "eqop: 0.0855581248686147\n",
      "eqop: 0.0855581248686147\n",
      "2\n",
      "eqop: 0.0855581248686147\n",
      "eqop: 0.0855581248686147\n",
      "eqop: 0.07147361782636119\n",
      "eqop: 0.08639899096068959\n",
      "3\n",
      "client_3\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "comm_round: 42 | global_acc: 0.79005081572613 | global_loss: 4.851822853088379\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.061960784313725537\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: -0.0040567951318458695\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: -0.04540676897204121\n",
      "comm_round: 43 | global_acc: 0.7814923776410805 | global_loss: 5.00927209854126\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.02274509803921565\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.017917511832319155\n",
      "eqop: 0.013184584178498993\n",
      "eqop: 0.013184584178498993\n",
      "eqop: -0.0040567951318458695\n",
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: 0.02585663233130131\n",
      "eqop: 0.07147361782636119\n",
      "eqop: 0.08639899096068959\n",
      "1\n",
      "eqop: 0.08639899096068959\n",
      "eqop: 0.10132436409501788\n",
      "2\n",
      "eqop: 0.10132436409501788\n",
      "eqop: 0.08639899096068959\n",
      "eqop: 0.08639899096068959\n",
      "3\n",
      "client_3\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "comm_round: 44 | global_acc: 0.7863064990639208 | global_loss: 4.92398738861084\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.0556862745098039\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: -0.02129817444219073\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: -0.03132226192978771\n",
      "comm_round: 45 | global_acc: 0.7937951323883391 | global_loss: 4.825580596923828\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.04235294117647059\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.013184584178498993\n",
      "eqop: -0.0016903313049357882\n",
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: 0.02585663233130131\n",
      "eqop: 0.0416228715577045\n",
      "eqop: 0.07147361782636119\n",
      "1\n",
      "eqop: 0.07147361782636119\n",
      "2\n",
      "eqop: 0.07147361782636119\n",
      "eqop: 0.07147361782636119\n",
      "eqop: 0.0996426319108682\n",
      "3\n",
      "client_3\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "comm_round: 46 | global_acc: 0.7903182669162878 | global_loss: 4.861663341522217\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.04235294117647059\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: -0.0016903313049357882\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: -0.03048139583771281\n",
      "comm_round: 47 | global_acc: 0.7847017919229741 | global_loss: 4.969910144805908\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.02274509803921565\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: 0.05476673427991885\n",
      "eqop: 0.015551048005409074\n",
      "eqop: 0.013184584178498993\n",
      "eqop: 0.013184584178498993\n",
      "1\n",
      "eqop: 0.013184584178498993\n",
      "2\n",
      "eqop: -0.0040567951318458695\n",
      "client_2\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: 0.02585663233130131\n",
      "eqop: 0.08639899096068959\n",
      "eqop: 0.07147361782636119\n",
      "eqop: 0.07147361782636119\n",
      "1\n",
      "eqop: 0.0996426319108682\n",
      "2\n",
      "eqop: 0.07147361782636119\n",
      "eqop: 0.0855581248686147\n",
      "3\n",
      "client_3\n",
      "length of dataset after: 4984\n",
      "length of dataset after prev_xtr: 4984\n",
      "comm_round: 48 | global_acc: 0.788713559775341 | global_loss: 4.887905120849609\n",
      "x_test 3739 \n",
      "client_1\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_1\n",
      "eqop: -0.0556862745098039\n",
      "client_2\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_2\n",
      "eqop: -0.02129817444219073\n",
      "client_3\n",
      "length of dataset: 4984\n",
      "bismillah\n",
      "client_3\n",
      "eqop: -0.04540676897204121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comm_round: 49 | global_acc: 0.7889810109654988 | global_loss: 4.884624481201172\n",
      "x_test 3739 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "smlp_global = SimpleMLP()\n",
    "num_layers_mult=1\n",
    "n=num_layers_mult\n",
    "comms_round = 50\n",
    "global_model = smlp_global.build(Xtr,n)\n",
    "sensitivity_= []\n",
    "specificity_= []\n",
    "BalanceACC_= []\n",
    "G_mean_= []\n",
    "FP_rate_= []\n",
    "FN_rate_= []\n",
    "assigned_positives = 0\n",
    "total_positives = 0\n",
    "accuracy_= []\n",
    "loss_= []\n",
    "statistical_parity_ = []\n",
    "eqop_ = []\n",
    "disc_thresh = 0.005\n",
    "lambda_initial = 0.005\n",
    "disc_tolerance = 0.1\n",
    "epsilon=1\n",
    "#commence global training loop\n",
    "import time\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "destination = \"./\"\n",
    "output_file = open(destination+\"log33333.txt\", \"w+\")\n",
    "start = time.time()\n",
    "end_time = []\n",
    "e=EarlyStopping(patience=5,restore_best_weights=True)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "for comm_round in range(comms_round):\n",
    "            \n",
    "    # get the global model's weights - will serve as the initial weights for all local models\n",
    "    global_weights = global_model.get_weights()\n",
    "    \n",
    "    #initial list to collect local model weights after scalling\n",
    "    scaled_local_weight_list = list()\n",
    "\n",
    "    #randomize client data - using keys\n",
    "    client_names= list(clients_batched.keys())\n",
    "    #random.shuffle(client_names)\n",
    "    \n",
    "    #loop through each client and create new local model\n",
    "    for client in client_names:   ##?? why are w creating local models in each communication round??\n",
    "        smlp_local = SimpleMLP()\n",
    "        local_model = smlp_local.build(Xtr, n)\n",
    "        local_model.compile(loss=loss, \n",
    "                      optimizer=optimizer, \n",
    "                      metrics=metrics)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #set local model weight to the weight of the global model\n",
    "        local_model.set_weights(global_weights)\n",
    "        \n",
    "        #here we should get the traning data from clients and try to do ksmote and see changes\n",
    "       \n",
    "        xxte = [] ##?client data?\n",
    "        yte = []\n",
    "        \n",
    "        for(X_test, Y_test) in clients_batched[client]:\n",
    "            \n",
    "            i = 0\n",
    "            while (i <len(X_test)):       \n",
    "                xxte.append(X_test[i].numpy())  \n",
    "                yte.append(Y_test[i].numpy())\n",
    "                i +=1\n",
    "        print(client)\n",
    "        print(\"length of dataset: %s\" %len(xxte))\n",
    "        ##maryam: separate particular clients test data from clients_test_data_batched\n",
    "        x_test_client = []\n",
    "        y_test_client = []\n",
    "        for(X_test_real, Y_test_real) in clients_test_data_batched[client]:\n",
    "            i = 0\n",
    "            while (i <len(X_test_real)):       \n",
    "                x_test_client.append(X_test_real[i].numpy())  \n",
    "                y_test_client.append(Y_test_real[i].numpy())\n",
    "                i +=1\n",
    "        \n",
    "        x_test_client = np.array(x_test_client)\n",
    "        \n",
    "        y_test_client = np.array(y_test_client)\n",
    "        \n",
    "        #here we change k and r to see how affect our result\n",
    "        #for simple federated learning\n",
    "        #Xtr_1_new,Ytr_1_new = create_synth_data(xxte,yte, 1,8,0.2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #for fair federated learning\n",
    "        minority_label = 1\n",
    "        majority_label = 0\n",
    "        #lambda_score = 0.1 #0.1,0.15,0.2,0.25,0.3\n",
    "        print(\"bismillah\")\n",
    "        print(client)\n",
    "        \n",
    "        ###\n",
    "        \n",
    "        ###\n",
    "        #m = 0\n",
    "        #if m==0:\n",
    "        output_file.write(\"\\n ********************************************************************\")\n",
    "        output_file.write(\"\\n communication round: \" + str(comm_round))\n",
    "        output_file.write(\"\\n client number: \" + str(client))\n",
    "        if comm_round == 0:\n",
    "            Xtr_1_new=np.array(xxte)\n",
    "            Ytr_1_new=np.array(yte)\n",
    "            #Xtr_1_new,Ytr_1_new = create_synth_data(xxte,yte, minority_label,majority_label,5,0.2,'min_p')\n",
    "            #for k in xxte:\n",
    "            #    Xtr_1_new.append(k)\n",
    "                    \n",
    "            #for k in yte:\n",
    "            #    Ytr_1_new.append(k)    \n",
    "                    \n",
    "            Xtr_1_new = np.array(Xtr_1_new)\n",
    "            Ytr_1_new = np.array(Ytr_1_new)\n",
    "            added_points = len(Xtr_1_new) - len(xxte)\n",
    "            \n",
    "            #new batch for new data\n",
    "            #split data for validation set\n",
    "            Xtr_1_new_split,x_val,Ytr_1_new_split,y_val=train_test_split(Xtr_1_new,Ytr_1_new,test_size=0.10,shuffle=True)\n",
    "        \n",
    "            data = list(zip(Xtr_1_new_split, Ytr_1_new_split))\n",
    "            random.shuffle(data)\n",
    "            btach_data = batch_data(data, bs=30)  ##?? bs=4 means?\n",
    "            #fit local model with client's data\n",
    "            #create validation data and early stop\n",
    "            #class_weights = class_weight.compute_class_weight('balanced',\n",
    "            #                                     np.unique(Ytr_1_new),\n",
    "            #                                     Ytr_1_new)\n",
    "            #class_weights = {majority_label:1, minority_label:7}\n",
    "            class_weights=find_class_Weight(Ytr_1_new, majority_label,minority_label)\n",
    "            local_model.fit(btach_data,validation_data=(x_val,y_val),validation_steps=1, callbacks=[e], epochs=3, verbose=0, \n",
    "                            class_weight=class_weights)\n",
    "            ##maryam: find disc score for each client\n",
    "            disc_score, assigned_positives, total_positives, balanced_accuracy_client = test_client_model(x_test_client, y_test_client,  local_model)\n",
    "            print(\"balanced_accuracy_client %s\" % balanced_accuracy_client)\n",
    "            trade_off = (1+epsilon**2)*((balanced_accuracy_client*(1-abs(disc_score)))/(epsilon*balanced_accuracy_client+(1-abs(disc_score))))\n",
    "            lambda_score = lambda_initial*(1+(disc_score/disc_tolerance))\n",
    "            \n",
    "        \n",
    "        if comm_round!=0:\n",
    "            disc_score, assigned_positives, total_positives, balanced_accuracy_client = test_client_model(x_test_client, y_test_client,  local_model)\n",
    "            trade_off = (1+epsilon**2)*((balanced_accuracy_client*(1-abs(disc_score)))/(epsilon*balanced_accuracy_client+(1-abs(disc_score))))\n",
    "        #Xtr_1_new, Ytr_1_new = xxte, yte \n",
    "        if disc_score > disc_thresh:\n",
    "            \n",
    "        #else:\n",
    "            #disc_score, assigned_positives, total_positives = test_client_model(x_test_client, y_test_client,  local_model)\n",
    "            #print(\"assigned: %s\" % assigned_positives)\n",
    "            #print(\"total: %s\" % total_positives)\n",
    "                prev_xtr,prev_ytr=[],[]\n",
    "                prev_disc_score= []\n",
    "                prev_trade_off=[]\n",
    "                output_file.write(\"\\n Disc score is greater than the threshold\")\n",
    "                output_file.write(\"\\n Disc score before decreasing: \" + str(disc_score))\n",
    "                lambda_score = lambda_initial*(1+(disc_score/disc_tolerance))\n",
    "                greater_disc_score = 0\n",
    "                min_disc_score =disc_score\n",
    "                max_trade_off = trade_off\n",
    "                Xtr_1_new=np.array(xxte)\n",
    "                Ytr_1_new=np.array(yte)\n",
    "                prev_xtr,prev_ytr = Xtr_1_new, Ytr_1_new\n",
    "                dmin_p_x, dmin_np_x, dmaj_p_x, dmaj_np_x = get_statistics(Xtr_1_new,Ytr_1_new,minority_label,majority_label)\n",
    "                output_file.write(\"\\n Number of minority_protected_class_instances: \" + str(dmin_p_x))\n",
    "                output_file.write(\"\\n Number of majority_protected_class_instances: \" + str(dmaj_p_x))\n",
    "                output_file.write(\"\\n Number of majority_non-protected_class_instances: \" + str(dmaj_np_x))\n",
    "                output_file.write(\"\\n Number of minority_non-protected_class_instances: \" + str(dmin_np_x))\n",
    "                #class_weights = class_weight.compute_class_weight('balanced',\n",
    "                #                                 np.unique(Ytr_1_new),\n",
    "                #                                 Ytr_1_new)\n",
    "                \n",
    "                #class_weights = {majority_label:1, minority_label:7}\n",
    "                class_weights=find_class_Weight(Ytr_1_new, majority_label,minority_label)\n",
    "                points = len(Xtr_1_new) - len(xxte)\n",
    "                itr = 0\n",
    "                \n",
    "                while disc_score > disc_thresh:\n",
    "                    \n",
    "                    output_file.write(\"\\n iteration: \" + str(itr))\n",
    "                    itr+=1\n",
    "                    closest_to_zero = min(disc_score, min_disc_score, key=abs)\n",
    "                    if trade_off>max_trade_off:\n",
    "                    #if closest_to_zero != min_disc_score:\n",
    "                        min_disc_score = disc_score\n",
    "                        max_trade_off = trade_off\n",
    "                        prev_xtr,prev_ytr = Xtr_1_new, Ytr_1_new\n",
    "                        points = len(Xtr_1_new) - len(xxte)\n",
    "                        local_model.save_weights('./checkpoints/law/10-clients/my_checkpoint')\n",
    "                    if assigned_positives<= total_positives:\n",
    "                        \n",
    "                        dmin_p_x, dmin_np_x, dmaj_p_x, dmaj_np_x = get_statistics(Xtr_1_new,Ytr_1_new,minority_label,majority_label)\n",
    "                        output_file.write(\"\\n Number of minority_protected_class_instances: \" + str(dmin_p_x))\n",
    "                        output_file.write(\"\\n Number of majority_protected_class_instances: \" + str(dmaj_p_x))\n",
    "                        #N(C+,S) =N(C+,S)+ N(C,S)\n",
    "                        Xtr_min_p_new,Ytr_min_p_new = create_synth_data(xxte, yte, minority_label,majority_label,5,lambda_score,'min_p')\n",
    "                    \n",
    "                        #N(C,S) =N(C,S) N(C,S)\n",
    "                        Xtr_maj_p_new,Ytr_maj_p_new = downsample(xxte, yte, minority_label,majority_label,5,lambda_score,'maj_p')    \n",
    "                        \n",
    "                        for k in Xtr_maj_p_new:\n",
    "                            Xtr_min_p_new.append(k)\n",
    "                        for k in Ytr_maj_p_new:\n",
    "                            Ytr_min_p_new.append(k)\n",
    "                        \n",
    "                        Xtr_1_new = np.array(Xtr_min_p_new)\n",
    "                        Ytr_1_new = np.array(Ytr_min_p_new)\n",
    "                        dmin_p_x2, dmin_np_x2, dmaj_p_x2, dmaj_np_x2 = get_statistics(Xtr_1_new,Ytr_1_new,minority_label,majority_label)\n",
    "                        output_file.write(\"\\n Number of minority_protected_class_instances_generated: \" + str(dmin_p_x2-dmin_p_x))\n",
    "                        output_file.write(\"\\n Number of majority_protected_class_instances_removed: \" + str(dmaj_p_x-dmaj_p_x2))\n",
    "                        \n",
    "                    else:\n",
    "                        dmin_p_x, dmin_np_x, dmaj_p_x, dmaj_np_x = get_statistics(Xtr_1_new,Ytr_1_new,minority_label,majority_label)\n",
    "                        output_file.write(\"\\n Number of majority_non-protected_class_instances: \" + str(dmaj_np_x))\n",
    "                        output_file.write(\"\\n Number of minority_non-protected_class_instances: \" + str(dmin_np_x))\n",
    "                        #N(C,S+) =N(C,S+)+ N(C+,S+)\n",
    "                        Xtr_maj_np_new,Ytr_maj_np_new = create_synth_data(xxte, yte, minority_label,majority_label,5,lambda_score,'maj_np')\n",
    "                    \n",
    "                        #N(C+,S+) =N(C+,S+) N(C+,S+)\n",
    "                        Xtr_min_np_new,Ytr_min_np_new = downsample(xxte, yte, minority_label,majority_label,5,lambda_score,'min_np')\n",
    "                        \n",
    "                        for k in Xtr_min_np_new:\n",
    "                            Xtr_maj_np_new.append(k)\n",
    "                        for k in Ytr_min_np_new:\n",
    "                            Ytr_maj_np_new.append(k)\n",
    "                        \n",
    "                        Xtr_1_new = np.array(Xtr_maj_np_new)\n",
    "                        Ytr_1_new = np.array(Ytr_maj_np_new)\n",
    "                        dmin_p_x2, dmin_np_x2, dmaj_p_x2, dmaj_np_x2 = get_statistics(Xtr_1_new,Ytr_1_new,minority_label,majority_label)\n",
    "                        output_file.write(\"\\n Number of majority_non-protected_class_instances_generated: \" + str(dmaj_np_x2-dmaj_np_x))\n",
    "                        output_file.write(\"\\n Number of minority_non-protected_class_instances_removed: \" + str(dmin_np_x-dmin_np_x2))\n",
    "                    \n",
    "                    \n",
    "                    added_points = len(Xtr_1_new) - len(xxte)\n",
    "                    #new batch for new data\n",
    "                    #split data for validation set\n",
    "                    Xtr_1_new_split,x_val,Ytr_1_new_split,y_val=train_test_split(Xtr_1_new,Ytr_1_new,test_size=0.10,shuffle=True)\n",
    "                    \n",
    "                    data = list(zip(Xtr_1_new_split, Ytr_1_new_split))\n",
    "                    random.shuffle(data)\n",
    "                    btach_data = batch_data(data, bs=30)  ##?? bs=4 means?\n",
    "                    #fit local model with client's data\n",
    "                    #create validation data and early stop\n",
    "                    \n",
    "                    local_model.fit(btach_data,validation_data=(x_val,y_val),validation_steps=1, callbacks=[e], epochs=3, verbose=0,\n",
    "                                    class_weight = class_weights)\n",
    "                    ##maryam: find disc score for each client\n",
    "                    \n",
    "                    disc_score, assigned_positives, total_positives, balanced_accuracy_client = test_client_model(x_test_client, y_test_client,  local_model)\n",
    "                    trade_off = (1+epsilon**2)*((balanced_accuracy_client*(1-abs(disc_score)))/(epsilon*balanced_accuracy_client+(1-abs(disc_score))))\n",
    "                    output_file.write(\"\\n Disc score: \" + str(disc_score))\n",
    "                    closest_to_zero = min(disc_score, min_disc_score, key=abs)\n",
    "                    if trade_off>max_trade_off:\n",
    "                    #if closest_to_zero != min_disc_score:\n",
    "                        \n",
    "                        min_disc_score = disc_score\n",
    "                        max_trade_off = trade_off\n",
    "                        prev_xtr,prev_ytr = Xtr_1_new, Ytr_1_new\n",
    "                        points = len(Xtr_1_new) - len(xxte)\n",
    "                        local_model.save_weights('./checkpoints/law/10-clients/my_checkpoint')\n",
    "                    #if len(prev_disc_score)>0:\n",
    "                    if len(prev_trade_off)>0:\n",
    "                        #if disc_score>prev_disc_score[-1]:\n",
    "                        if trade_off<prev_trade_off[-1]:\n",
    "                            greater_disc_score+=1\n",
    "                            print(greater_disc_score)\n",
    "                            if greater_disc_score >2:\n",
    "                                local_model.load_weights('./checkpoints/law/10-clients/my_checkpoint')\n",
    "                                added_points = points\n",
    "                                \n",
    "                                #Xtr_1_new, Ytr_1_new =prev_xtr,prev_ytr\n",
    "                                #xxte, yte = Xtr_1_new, Ytr_1_new\n",
    "                                break\n",
    "                                #output_file.write(\"Balanced accuracy corresponding the selected model: \" + str(min_disc_score))\n",
    "                                \n",
    "                               \n",
    "                     \n",
    "                    #greater_disc_score = 0\n",
    "                    #lambda_score= lambda_score+0.01\n",
    "                    #prev_disc_score.append(disc_score)\n",
    "                    prev_trade_off.append(trade_off)\n",
    "                    xxte, yte = Xtr_1_new, Ytr_1_new\n",
    "                    \n",
    "                \n",
    "                '''\n",
    "                dmin_p_x, dmin_np_x, dmaj_p_x, dmaj_np_x = get_statistics(Xtr_1_new,Ytr_1_new,minority_label,majority_label)\n",
    "                print(\"\\n Number of minority_protected_class_instances: %s \" % dmin_p_x)\n",
    "                print(\"\\n Number of majority_protected_class_instances: %s\"  % dmaj_p_x)\n",
    "                print(\"\\n Number of majority_non-protected_class_instances: %s\"  % dmaj_np_x)\n",
    "                print(\"\\n Number of minority_non-protected_class_instances: %s\"  % dmin_np_x)\n",
    "                print(\"\\n Disc score corresponding the selected model: %s\" %min_disc_score)\n",
    "                '''\n",
    "                \n",
    "                xxtee=[]\n",
    "                ytee=[]\n",
    "                dataset = tf.data.Dataset.from_tensor_slices((list(prev_xtr), list(prev_ytr)))\n",
    "                up_dict = {client:batch_data(dataset)}\n",
    "                clients_batched.update(up_dict)\n",
    "                #clients_batched[client_name] = batch_data(dataset)\n",
    "                for(X_test, Y_test) in clients_batched[client]:\n",
    "            \n",
    "                    i = 0\n",
    "                    while (i <len(X_test)):       \n",
    "                        xxtee.append(X_test[i].numpy())  \n",
    "                        ytee.append(Y_test[i].numpy())\n",
    "                        i +=1\n",
    "                print(client)\n",
    "                print(\"length of dataset after: %s\" %len(xxtee))\n",
    "                print(\"length of dataset after prev_xtr: %s\" %len(prev_xtr))\n",
    "                '''\n",
    "                xxte = [] ##?client data?\n",
    "                yte = []\n",
    "                for(X_test, Y_test) in clients_batched[client]:\n",
    "                    i = 0\n",
    "                    while (i <len(X_test)):       \n",
    "                        xxte.append(X_test[i].numpy())  \n",
    "                        yte.append(Y_test[i].numpy())\n",
    "                        i +=1\n",
    "                \n",
    "                dmin_p_x, dmin_np_x, dmaj_p_x, dmaj_np_x = get_statistics(xxte,yte,minority_label,majority_label)\n",
    "                print(\"\\n Number of minority_protected_class_instances: %s \" % dmin_p_x)\n",
    "                print(\"\\n Number of majority_protected_class_instances: %s\"  % dmaj_p_x)\n",
    "                print(\"\\n Number of majority_non-protected_class_instances: %s\"  % dmaj_np_x)\n",
    "                print(\"\\n Number of minority_non-protected_class_instances: %s\"  % dmin_np_x)\n",
    "                print(\"\\n Disc score corresponding the selected model: %s\" %min_disc_score)\n",
    "                '''\n",
    "                \n",
    "                \n",
    "                \n",
    "        else:\n",
    "            output_file.write(\"\\n Disc score is less than the threshold\")\n",
    "            if comm_round!=0:\n",
    "                Xtr_1_new=np.array(xxte)\n",
    "                Ytr_1_new=np.array(yte)\n",
    "                #class_weights = class_weight.compute_class_weight('balanced',\n",
    "                #                                 np.unique(Ytr_1_new),\n",
    "                #                                 Ytr_1_new)\n",
    "                #class_weights = {majority_label:1, minority_label:7}\n",
    "                class_weights=find_class_Weight(Ytr_1_new, majority_label,minority_label)\n",
    "                Xtr_1_new,x_val,Ytr_1_new,y_val=train_test_split(Xtr_1_new,Ytr_1_new,test_size=0.10,shuffle=True)\n",
    "                data = list(zip(Xtr_1_new,Ytr_1_new))\n",
    "                #random.shuffle(data)\n",
    "                added_points = 0\n",
    "                btach_data = batch_data(data, bs=30)\n",
    "                \n",
    "                local_model.fit(btach_data,validation_data=(x_val,y_val),validation_steps = 1, callbacks=[e], epochs=3, \n",
    "                                verbose=0, class_weight = class_weights)\n",
    "        \n",
    "        #scale the model weights and add to list       \n",
    "        client_names = list(clients_batched.keys())\n",
    "        bs = list(clients_batched[client_name])[0][0].shape[0]\n",
    "        #first calculate the total training data points across clinets\n",
    "        global_count = sum([tf.data.experimental.cardinality(clients_batched[client_name]).numpy() for client_name in client_names])*bs\n",
    "        global_count = global_count + (added_points)\n",
    "        # get the total number of data points held by a client\n",
    "        local_count = len(Xtr_1_new)\n",
    "        scaling_factor = local_count/global_count\n",
    "                \n",
    "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "        scaled_local_weight_list.append(scaled_weights)\n",
    "        \n",
    "        #clear session to free memory after each communication round\n",
    "        K.clear_session()\n",
    "        \n",
    "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "    ##?? after the completion of each communication round we discard the data we synthesized? or we are going to use it in the next as well?\n",
    "    #update global model \n",
    "    \n",
    "    global_model.set_weights(average_weights)\n",
    "    \n",
    "    #test global model and print out metrics after each communications round\n",
    "    for(X_test, Y_test) in test_batched:\n",
    "        global_acc, global_loss,conf,stat_parity,eqop = test_model(X_test, Y_test, global_model, comm_round)\n",
    "        print(\"x_test %s \"% len(X_test))\n",
    "        TN = conf[0][0]\n",
    "        FP = conf[0][1]\n",
    "        FN = conf[1][0]\n",
    "        TP = conf[1][1]\n",
    "        sensitivity = TP/(TP+FN) \n",
    "        specificity = TN/(FP+TN)\n",
    "        \n",
    "        BalanceACC = (sensitivity+specificity)/2\n",
    "        G_mean = math.sqrt(sensitivity*specificity)\n",
    "        FN_rate= FN/(FN+TP) \n",
    "        FP_rate = FP/(FP+TN) \n",
    "        #add the data to arrays\n",
    "        sensitivity_.append(sensitivity)\n",
    "        specificity_.append(specificity)\n",
    "        BalanceACC_.append(BalanceACC)\n",
    "        G_mean_.append(G_mean)\n",
    "        FP_rate_.append(FP_rate)\n",
    "        FN_rate_.append(FN_rate)\n",
    "        accuracy_.append(global_acc)\n",
    "        loss_.append(global_loss)\n",
    "        statistical_parity_.append(stat_parity)\n",
    "        eqop_.append(eqop)\n",
    "        output_file.write(\"\\n Balanced accuracy: \" + str(BalanceACC))\n",
    "        output_file.write(\"\\n Disc. Score: \" + str(statistical_parity_[-1]))\n",
    "    end_time.append(((time.time()) - start))\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "371f6a8f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8310991957104558\n",
      "0.7780748663101604\n",
      "0.804587031010308\n",
      "0.8041501076247486\n",
      "0.22192513368983957\n",
      "0.16890080428954424\n",
      "0.783364535972185\n",
      "0.013247081411193518\n",
      "0.00236994219653186\n",
      "21\n",
      "23\n",
      "23\n",
      "23\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#simple federated learning\n",
    "trade_off=[]\n",
    "epsilon = 1\n",
    "for i in range(len(BalanceACC_)):\n",
    "    \n",
    "    trade_off.append((1+epsilon**2)*((BalanceACC_[i]*(1-abs(eqop_[i])))/(epsilon*BalanceACC_[i]+(1-abs(eqop_[i])))))\n",
    "index = trade_off.index(max(trade_off[0:-1], key=abs))\n",
    "print(sensitivity_[index])\n",
    "print(specificity_[index])\n",
    "print(BalanceACC_[index])\n",
    "print(G_mean_[index])\n",
    "print(FP_rate_[index])\n",
    "print(FN_rate_[index])\n",
    "print(accuracy_[index])\n",
    "print(statistical_parity_[index])\n",
    "print(eqop_[index])\n",
    "print(statistical_parity_.index(min(statistical_parity_[0:-1], key=abs)))\n",
    "print(BalanceACC_.index(max(BalanceACC_[0:-1], key=abs)))\n",
    "print(trade_off.index(max(trade_off[0:-1], key=abs)))\n",
    "destination = \"./results/law/eqop/\"\n",
    "client=\"10-copy-\"\n",
    "output_file = open(destination+client+\"-client-bal_acc.txt\", \"w+\")\n",
    "output_file.write(str(BalanceACC_))\n",
    "output_file.close()\n",
    "\n",
    "output_file = open(destination+client+\"-client-sen.txt\", \"w+\")\n",
    "output_file.write(str(sensitivity_))\n",
    "output_file.close()\n",
    "\n",
    "output_file = open(destination+client+\"-client-spc.txt\", \"w+\")\n",
    "output_file.write(str(specificity_))\n",
    "output_file.close()\n",
    "\n",
    "output_file = open(destination+client+\"-client-gmean.txt\", \"w+\")\n",
    "output_file.write(str(G_mean_))\n",
    "output_file.close()\n",
    "\n",
    "output_file = open(destination+client+\"-client-fp_rate.txt\", \"w+\")\n",
    "output_file.write(str(FP_rate_))\n",
    "output_file.close()\n",
    "\n",
    "output_file = open(destination+client+\"-client-fn_rate.txt\", \"w+\")\n",
    "output_file.write(str(FN_rate_))\n",
    "output_file.close()\n",
    "\n",
    "output_file = open(destination+client+\"-client-sp.txt\", \"w+\")\n",
    "output_file.write(str(statistical_parity_))\n",
    "output_file.close()\n",
    "\n",
    "output_file = open(destination+client+\"-client-eqop.txt\", \"w+\")\n",
    "output_file.write(str(eqop_))\n",
    "output_file.close()\n",
    "print(index)\n",
    "#pyplot.plot(BalanceACC_)\n",
    "plt.plot(BalanceACC_)\n",
    "plt.title('BalanceACC')\n",
    "plt.xlabel('Global Round')\n",
    "plt.ylabel('BalanceACC')\n",
    "plt.show()\n",
    "plt.plot(statistical_parity_)\n",
    "plt.title('statistical_parity')\n",
    "plt.xlabel('Global Round')\n",
    "plt.ylabel('statistical_parity_')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1086c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(abs(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b1d0e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54cd645b",
   "metadata": {},
   "source": [
    "##### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
