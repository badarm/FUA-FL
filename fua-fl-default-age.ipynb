{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b79fd593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder,MinMaxScaler,StandardScaler\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression,LogisticRegressionCV,SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D,Input\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "# Importing necssary modules\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "from collections import Counter\n",
    "# Custom script \n",
    "#%matplotlibe inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e873dfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000, 23)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import urllib2\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import preprocessing\n",
    "from random import seed, shuffle\n",
    "\n",
    "# import utils as ut\n",
    "\n",
    "SEED = 1234\n",
    "seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "def process_loaded_data(dataFrame):\n",
    "    FEATURES_CLASSIFICATION = [\"LIMIT_BAL\",\"SEX\",\"EDUCATION\",\"MARRIAGE\",\"AGE\",\"PAY_0\",\"PAY_2\",\"PAY_3\",\"PAY_4\",\"PAY_5\",\"PAY_6\",\"BILL_AMT1\",\"BILL_AMT2\",\"BILL_AMT3\",\"BILL_AMT4\",\"BILL_AMT5\",\"BILL_AMT6\",\"PAY_AMT1\",\"PAY_AMT2\",\"PAY_AMT3\",\"PAY_AMT4\",\"PAY_AMT5\",\"PAY_AMT6\"]  # features to be used for classification\n",
    "    CONT_VARIABLES = [\"AGE\",\"BILL_AMT1\",\"BILL_AMT2\",\"BILL_AMT3\",\"BILL_AMT4\",\"BILL_AMT5\",\"BILL_AMT6\",\"PAY_AMT1\",\"PAY_AMT2\",\"PAY_AMT3\",\"PAY_AMT4\",\"PAY_AMT5\",\"PAY_AMT6\"]  # continuous features, will need to be handled separately from categorical features, categorical features will be encoded using one-hot\n",
    "    CLASS_FEATURE = \"y\"  # the decision variable\n",
    "    SENSITIVE_ATTRS = [\"SEX\"]\n",
    "    CAT_VARIABLES = [\"LIMIT_BAL\",\"SEX\",\"EDUCATION\",\"MARRIAGE\",\"PAY_0\",\"PAY_2\",\"PAY_3\",\"PAY_4\",\"PAY_5\",\"PAY_6\"]\n",
    "    CAT_VARIABLES_INDICES = [0,2,3,5,6,7,8,9,10]\n",
    "    \n",
    "    INPUT_FILE = \"./datasets/default.csv\"\n",
    "\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    \n",
    "    # convert to np array\n",
    "    data = df.to_dict('list')\n",
    "    \n",
    "    for k in data.keys():\n",
    "        data[k] = np.array(data[k])\n",
    "\n",
    "    \"\"\" Feature normalization and one hot encoding \"\"\"\n",
    "\n",
    "    # convert class label 0 to -1\n",
    "    y = data[CLASS_FEATURE]\n",
    "    #y[y == \"yes\"] = 1\n",
    "    #y[y == 'no'] = -1\n",
    "    y = np.array([int(k) for k in y])\n",
    "\n",
    "    X = np.array([]).reshape(len(y), 0)  # empty array with num rows same as num examples, will hstack the features to it\n",
    "    \n",
    "    x_control = defaultdict(list)\n",
    "    i=0\n",
    "    feature_names = []\n",
    "    \n",
    "    for attr in FEATURES_CLASSIFICATION:\n",
    "        vals = data[attr]\n",
    "        \n",
    "        if attr in CONT_VARIABLES:\n",
    "            vals = [float(v) for v in vals]\n",
    "            vals = preprocessing.scale(vals)  # 0 mean and 1 variance\n",
    "            vals = np.reshape(vals, (len(y), -1))  # convert from 1-d arr to a 2-d arr with one col\n",
    "            \n",
    "\n",
    "        else:  # for binary categorical variables, the label binarizer uses just one var instead of two\n",
    "            lb = preprocessing.LabelEncoder()\n",
    "            lb.fit(vals)\n",
    "            vals = lb.transform(vals)\n",
    "            vals = np.reshape(vals, (len(y), -1))\n",
    "            '''\n",
    "            if attr == 'SEX':\n",
    "                print(lb.classes_)\n",
    "                print(lb.transform(lb.classes_))\n",
    "            '''\n",
    "            #if attr == 'job':\n",
    "            #    print(lb.classes_)\n",
    "            #    print(lb.transform(lb.classes_))\n",
    "            \n",
    "           \n",
    "            \n",
    "            \n",
    "        # add to sensitive features dict\n",
    "        if attr in SENSITIVE_ATTRS:\n",
    "            x_control[attr] = vals\n",
    "            \n",
    "\n",
    "        # add to learnable features\n",
    "        X = np.hstack((X, vals))\n",
    "        \n",
    "        if attr in CONT_VARIABLES:  # continuous feature, just append the name\n",
    "            feature_names.append(attr)\n",
    "        else:  # categorical features\n",
    "            if vals.shape[1] == 1:  # binary features that passed through lib binarizer\n",
    "                feature_names.append(attr)\n",
    "            else:\n",
    "                for k in lb.classes_:  # non-binary categorical features, need to add the names for each cat\n",
    "                    feature_names.append(attr + \"_\" + str(k))\n",
    "\n",
    "    # convert the sensitive feature to 1-d array\n",
    "    \n",
    "    x_control = dict(x_control)\n",
    "    \n",
    "    for k in x_control.keys():\n",
    "        assert (x_control[k].shape[1] == 1)  # make sure that the sensitive feature is binary after one hot encoding\n",
    "        x_control[k] = np.array(x_control[k]).flatten()\n",
    "    \n",
    "    feature_names.append('target')\n",
    "    # '0' is 'female'\n",
    "   \n",
    "    \n",
    "    return X, y, feature_names.index(SENSITIVE_ATTRS[0]), 0, x_control\n",
    "\n",
    "def load_default():\n",
    "\n",
    "    \n",
    "    INPUT_FILE = \"./datasets/default.csv\"\n",
    "\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    ranges = [0,30,40,50,100]\n",
    "    \n",
    "    mask = (df['AGE'] > 0) & (df['AGE'] < 30)\n",
    "    df1 = df[mask]\n",
    "    mask = (df['AGE'] > 29) & (df['AGE'] < 40)\n",
    "    df2 = df[mask]\n",
    "    mask = (df['AGE'] > 39) & (df['AGE'] < 80)\n",
    "    df3 = df[mask]\n",
    "    mask = (df['AGE'] > 49)\n",
    "    df4 = df[mask]\n",
    "    df1 = df1.dropna()\n",
    "    df2 = df2.dropna()\n",
    "    df3 = df3.dropna()\n",
    "    #df4['y'] = pd.Series(np.where(df4.y.values == 'yes', 1, 0),df4.index)\n",
    "    #print(df4.groupby('y').count())\n",
    "    X1,y1, sa_index, p_Group, x_control = process_loaded_data(df1)\n",
    "    X2,y2, sa_index, p_Group, x_control = process_loaded_data(df2)\n",
    "    X3,y3, sa_index, p_Group, x_control = process_loaded_data(df3)\n",
    "    X4,y4, sa_index, p_Group, x_control = process_loaded_data(df4)\n",
    "    np_Group = 1\n",
    "    return X1,X2,X3,X4,y1,y2,y3,y4, sa_index, p_Group, np_Group\n",
    "\n",
    "X1,X2,X3,X4,y1,y2,y3,y4, sa_index, p_Group, np_Group = load_default()\n",
    "\n",
    "# prepare train data and test data of each client\n",
    "clients = {}\n",
    "client_data_testx = []\n",
    "client_data_testy = []\n",
    "client_names = ['{}_{}'.format('client', i+1) for i in range(4)]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X1,y1,test_size=0.2)\n",
    "Xtr1 = x_train\n",
    "Xte1 = x_test\n",
    "Ytr1 = y_train\n",
    "Yte1 = y_test\n",
    "Xtr = x_train\n",
    "client_data_testx.append(Xte1)\n",
    "client_data_testy.append(Yte1)\n",
    "\n",
    "####\n",
    "x_train, x_test, y_train, y_test = train_test_split(X2,y2,test_size=0.2)\n",
    "Xtr2 = x_train\n",
    "Xte2 = x_test\n",
    "Ytr2 = y_train\n",
    "Yte2 = y_test\n",
    "client_data_testx.append(Xte2)\n",
    "client_data_testy.append(Yte2)\n",
    "\n",
    "####\n",
    "x_train, x_test, y_train, y_test = train_test_split(X3,y3,test_size=0.2)\n",
    "Xtr3 = x_train\n",
    "Xte3 = x_test\n",
    "Ytr3 = y_train\n",
    "Yte3 = y_test\n",
    "client_data_testx.append(Xte3)\n",
    "client_data_testy.append(Yte3)\n",
    "x_train.shape\n",
    "####\n",
    "x_train, x_test, y_train, y_test = train_test_split(X4,y4,test_size=0.2)\n",
    "Xtr4 = x_train\n",
    "Xte4 = x_test\n",
    "Ytr4 = y_train\n",
    "Yte4 = y_test\n",
    "#client_data_testx.append(Xte4)\n",
    "#client_data_testy.append(Yte4)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6317f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatnate teset data\n",
    "x_test_new = np.concatenate((client_data_testx[0], client_data_testx[1]), axis=0)\n",
    "x_test_new = np.concatenate((x_test_new, client_data_testx[2]), axis=0)\n",
    "y_test_new = np.concatenate((client_data_testy[0], client_data_testy[1]), axis=0)\n",
    "y_test_new = np.concatenate((y_test_new, client_data_testy[2]), axis=0)\n",
    "test_batched1 = tf.data.Dataset.from_tensor_slices((x_test_new, y_test_new)).batch(len(y_test_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb7ffedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clients(Xtr1, Ytr1,Xtr2,Ytr2,Xtr3,Ytr3, Xtr4,Ytr4,num_clients,initial='clients'):\n",
    "    ''' return: a dictionary with keys clients' names and value as \n",
    "                data shards - tuple of images and label lists.\n",
    "        args: \n",
    "            image_list: a list of numpy arrays of training images\n",
    "            label_list:a list of binarized labels for each image\n",
    "            num_client: number of fedrated members (clients)\n",
    "            initials: the clients'name prefix, e.g, clients_1 \n",
    "            \n",
    "    '''\n",
    "    clients = {}\n",
    "    #create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
    "\n",
    "    data = list(zip(Xtr1, Ytr1))\n",
    "    clients.update({client_names[0] :data})\n",
    "    data = list(zip(Xtr2, Ytr2))\n",
    "    clients.update({client_names[1] :data})\n",
    "    data = list(zip(Xtr3, Ytr3))\n",
    "    clients.update({client_names[2] :data})\n",
    "    #data = list(zip(Xtr4, Ytr4))\n",
    "    #clients.update({client_names[3] :data})\n",
    "\n",
    "    return clients\n",
    "clients = create_clients(Xtr1,Ytr1,Xtr2,Ytr2,Xtr3,Ytr3, Xtr4,Ytr4, num_clients=3, initial='client')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbc221ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clients = create_clients(Xtr, Ytr, num_clients=10, initial='client')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d8b5fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(data_shard, bs=30):\n",
    "    '''Takes in a clients data shard and create a tfds object off it\n",
    "    args:\n",
    "        shard: a data, label constituting a client's data shard\n",
    "        bs:batch size\n",
    "    return:\n",
    "        tfds object'''\n",
    "    #seperate shard into data and labels lists\n",
    "    data, label = zip(*data_shard)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
    "    return dataset.shuffle(len(label)).batch(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e237e7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process and batch the training data for each client\n",
    "clients_batched = dict()\n",
    "for (client_name, data) in clients.items():\n",
    "    clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "#test_batched = tf.data.Dataset.from_tensor_slices((Xte, Yte)).batch(len(Yte))\n",
    "#print('Number of client datasets: {l}'.format(l=len(test_batched)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77a4146d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of client datasets: 3\n",
      "First dataset: <BatchDataset shapes: ((None, 23), (None,)), types: (tf.float64, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "print('Number of client datasets: {l}'.format(l=len(clients_batched)))\n",
    "print('First dataset: {d}'.format(d=clients_batched['client_1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e94cf90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72000\n",
      "24000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client_names = list(clients_batched.keys())\n",
    "bs = list(clients_batched[client_name])[0][0].shape[0] #\n",
    "#first calculate the total training data points across clinets\n",
    "global_count = sum([tf.data.experimental.cardinality(clients_batched[client_name]).numpy() for client_name in client_names])*bs\n",
    "local_count = tf.data.experimental.cardinality(clients_batched['client_1']).numpy()*bs\n",
    "print(global_count) ##\n",
    "print(local_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d56ee0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of client datasets: 3\n",
      "First dataset: <BatchDataset shapes: ((None, 23), (None,)), types: (tf.float64, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "print('Number of client datasets: {l}'.format(l=len(clients_batched)))\n",
    "print('First dataset: {d}'.format(d=clients_batched['client_1']))\n",
    "# check length of number of test datasets created for clients\n",
    "#print('Number of client test datasets: {l}'.format(l=len(clients_test_data_batched)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9fd2115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,BatchNormalization,InputLayer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "class SimpleMLP:\n",
    "    @staticmethod\n",
    "    def build(x_train,n):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(InputLayer(input_shape=(x_train.shape[1],)))\n",
    "        model.add(Dense(x_train.shape[1],activation='relu'))#,input_shape=(x_train.shape[1],)))\n",
    "        #model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(n*x_train.shape[1],activation='relu'))\n",
    "        #model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(n*x_train.shape[1],activation='relu'))\n",
    "\n",
    "        model.add(Dense(1,activation='sigmoid'))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c1a8baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\noptimizer = SGD(lr=lr, \\n                decay=lr / comms_round, \\n                momentum=0.9\\n               )  \\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.0001 \n",
    "comms_round = 50\n",
    "loss='binary_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "optimizer = Adam(lr=lr) \n",
    "'''\n",
    "optimizer = SGD(lr=lr, \n",
    "                decay=lr / comms_round, \n",
    "                momentum=0.9\n",
    "               )  \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e34e6f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# bulid K-SMOTE algorithem\n",
    "# input (dmin, dmaj, K=number of data sample from dmin to upsampling, r=ratio of samples to reate in compare to dmaj)\n",
    "# output(synthatic data for a client)\n",
    "import random\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "\n",
    "def k_nearest_neighbors(data, predict, k):\n",
    "    #k=8\n",
    "    #if len(data) >= k:\n",
    "    #    warnings.warn('K is set to a value less than total voting groups!')\n",
    "\n",
    "    distances = []\n",
    "    count = 0\n",
    "    for sample in data:\n",
    "        euclidean_distance = np.linalg.norm(np.array(sample)-np.array(predict))\n",
    "        distances.append([euclidean_distance,count])\n",
    "        count+=1\n",
    "    \n",
    "    votes = [i[1] for i in sorted(distances)[:k]] ##votes is returning indexes of k random samples\n",
    "\n",
    "    #vote_result = Counter(votes).most_common(9)[0][0]\n",
    "    return votes\n",
    "\n",
    "\n",
    "def fair_k_nearest_neighbors(data, predict, k):\n",
    "    #k=8\n",
    "    #if len(data) >= k:\n",
    "    #    warnings.warn('K is set to a value less than total voting groups!')\n",
    "\n",
    "    distances = []\n",
    "    count = 0\n",
    "    for sample in data:\n",
    "        euclidean_distance = np.linalg.norm(np.array(sample)-np.array(predict))\n",
    "        distances.append([euclidean_distance,count])\n",
    "        count+=1\n",
    "    \n",
    "    votes = [i[1] for i in sorted(distances)[:k]] ##votes is returning indexes of k random samples\n",
    "\n",
    "    #vote_result = Counter(votes).most_common(9)[0][0]\n",
    "    return votes\n",
    "\n",
    "##algo 2:\n",
    "\n",
    "def fair_kSMOTE_algo_2(dmajor,dminor,k,r):\n",
    "    S = []\n",
    "    Ns =  int(r*(len(dmajor)))\n",
    "    \n",
    "    Nks = int(Ns / (k-1))\n",
    "    difference = Ns-Nks*(k-1)\n",
    "    \n",
    "    rb = []\n",
    "    #pick a random k sample from dmin and save them in rb\n",
    "    dmin_rand = random.sample(dminor, k-1)   \n",
    "    #for debugging\n",
    "    sens_attr_vals = []\n",
    "    i = 0\n",
    "    \n",
    "    #do algorithem (choose the nearest neighbor and linear interpolation)\n",
    "    for xb in dmin_rand:\n",
    "        N= k_nearest_neighbors(dminor,xb,k) #from minority-p\n",
    "        \n",
    "        #do linear interpolation\n",
    "        Sxb = []\n",
    "        \n",
    "        if i==0:\n",
    "            Nkss = Nks+difference\n",
    "        else:\n",
    "            Nkss = Nks\n",
    "        \n",
    "        i+=1\n",
    "        for s in range(Nkss):\n",
    "            \n",
    "            \n",
    "            j = 1  \n",
    "            #randome k sample\n",
    "            #j = random.randint(0, len(N))\n",
    "            \n",
    "            ##here nearst neghber insted of dminor\n",
    "            #linear interpolation\n",
    "            x_new = ((dminor[N[j]]-xb) * random.sample(range(0, 1), 1))\n",
    "            j+=1\n",
    "            \n",
    "            while(j < len(N)):\n",
    "                #here on random xb\n",
    "                ind = N[j]\n",
    "                x_new = x_new + ((dminor[ind]-xb) * random.sample(range(0, 1), 1))\n",
    "                j += 1\n",
    "                \n",
    "            x_new = x_new / (len(N)-1) \n",
    "            Synthesized_instance = xb + x_new \n",
    "            \n",
    "            \n",
    "            #for algo 3 when finding nearest neighbors from min_np and assigning the \n",
    "            #'p' sensitive value to all synthesized instances\n",
    "            Synthesized_instance[sa_index] = xb[sa_index] \n",
    "            Sxb.append(Synthesized_instance)\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        S.append(Sxb)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return S\n",
    "\n",
    "\n",
    "\n",
    "###algo 3, algo 3 x_synthesized as a mixture of nearest neighbors from dmin-p and dmin-np\n",
    "\n",
    "def fair_kSMOTE(dmajor,dminor_wg,dminor,k,r):\n",
    "    S = []\n",
    "    #Ns =  int(r*(len(dmajor) - len(dminor)))\n",
    "    Ns =  int(r*(len(dmajor)))\n",
    "    \n",
    "    \n",
    "    Nks = int(Ns / (k-1))\n",
    "    difference = Ns-Nks*(k-1)\n",
    "    #if r==-1:\n",
    "    #    Nks = 1\n",
    "    rb = []\n",
    "    #pick a random k sample from dmin and save them in rb\n",
    "    dmin_rand = random.sample(dminor, k-1)   \n",
    "    #for debugging\n",
    "    sens_attr_vals = []\n",
    "    \n",
    "    \n",
    "    #do algorithem (choose the nearest neighbor and linear interpolation)\n",
    "    i = 0\n",
    "    \n",
    "    for xb in dmin_rand:\n",
    "        N= k_nearest_neighbors(dminor,xb,int(k/2)+1) #from minority-p\n",
    "        N2= k_nearest_neighbors(dminor_wg,xb,int(k/2)) #from minority-np\n",
    "    \n",
    "        N3 = np.hstack((N, N2))\n",
    "        if i==0:\n",
    "            Nkss = Nks+difference\n",
    "        else:\n",
    "            Nkss = Nks\n",
    "        \n",
    "        i+=1\n",
    "        #do linear interpolation\n",
    "        Sxb = []\n",
    "        \n",
    "        for s in range(Nkss):\n",
    "            \n",
    "            j = 1  \n",
    "            #randome k sample\n",
    "            #j = random.randint(0, len(N))\n",
    "            \n",
    "            ##here nearst neghber insted of dminor\n",
    "            #linear interpolation\n",
    "            x_new = ((dminor[N[j]]-xb) * random.sample(range(0, 1), 1))\n",
    "            j+=1\n",
    "            \n",
    "            while(j < len(N)):\n",
    "                #here on random xb\n",
    "                ind = N[j]\n",
    "                \n",
    "                x_new = x_new + ((dminor[ind]-xb) * random.sample(range(0, 1), 1))\n",
    "                j += 1\n",
    "            j = 0\n",
    "            while(j < len(N2)):\n",
    "                #here on random xb\n",
    "                ind = N2[j]\n",
    "                \n",
    "                x_new = x_new + ((dminor_wg[ind]-xb) * random.sample(range(0, 1), 1))\n",
    "                j += 1    \n",
    "            x_new = x_new / (len(N3)-1)\n",
    "            Synthesized_instance = xb + x_new \n",
    "            \n",
    "            \n",
    "            #for algo 3 when finding nearest neighbors from min_np and assigning the \n",
    "            #'p' sensitive value to all synthesized instances\n",
    "            Synthesized_instance[sa_index] = xb[sa_index] \n",
    "            Sxb.append(Synthesized_instance)\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        S.append(Sxb)\n",
    "    \n",
    "    \n",
    "   \n",
    "    return S\n",
    "\n",
    "\n",
    "def kSMOTE(dmajor,dminor,k,r):\n",
    "    S = []\n",
    "    Ns = int(r * (len(dmajor) - len(dminor)))\n",
    "    Nks = int(Ns / k)\n",
    "    rb = []\n",
    "    #pick a random k sample from dmin and save them in rb\n",
    "    dmin_rand = random.sample(dminor, k)   \n",
    "    #for debugging\n",
    "    sens_attr_vals = []\n",
    "    \n",
    "    \n",
    "    #do algorithem (choose the nearest neighbor and linear interpolation)\n",
    "    for xb in dmin_rand:\n",
    "        N= k_nearest_neighbors(dminor,xb,k)\n",
    "        \n",
    "        \n",
    "        #do linear interpolation\n",
    "        Sxb = []\n",
    "        for s in range(Nks):\n",
    "            j = 1  \n",
    "            #randome k sample\n",
    "            #j = random.randint(0, len(N))\n",
    "            \n",
    "            ##here nearst neghber insted of dminor\n",
    "            #linear interpolation\n",
    "            x_new = ((dminor[N[j]]-xb) * random.sample(range(0, 1), 1))\n",
    "            j+=1\n",
    "            \n",
    "            while(j < len(N)):\n",
    "                #here on random xb\n",
    "                ind = N[j]\n",
    "                x_new = x_new + ((dminor[ind]-xb) * random.sample(range(0, 1), 1))\n",
    "                j += 1\n",
    "                \n",
    "            x_new = x_new / len(N) \n",
    "            Synthesized_instance = xb + x_new \n",
    "            \n",
    "            #for debugging\n",
    "            sens_attr_vals.append(Synthesized_instance[10])\n",
    "            \n",
    "            #Synthesized_instance[sa_index] = xb[sa_index] ##Smote is synthesizing the values as if they are numbers but sensitive attribute \n",
    "                                                          ## is binary so we need to keep it binary\n",
    "            Sxb.append(Synthesized_instance)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        S.append(Sxb)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return S\n",
    "\n",
    "def splitYtrain_smotenc(Xtr,Ytr,minority_lable):\n",
    "  \n",
    "    dmaj_x = []\n",
    "    dmin_x = []\n",
    "    label_maj_x = []\n",
    "    label_min_x = []\n",
    "    \n",
    "    for i in range(len(Ytr)):\n",
    "        if((Ytr[i]) == minority_lable):\n",
    "            dmin_x.append(Xtr[i])\n",
    "            label_min_x.append(Ytr[i])\n",
    "        else:\n",
    "            dmaj_x.append(Xtr[i])\n",
    "            label_maj_x.append(Ytr[i])\n",
    "    \n",
    "    return dmaj_x,dmin_x, label_min_x, label_maj_x\n",
    "def splitYtrain(Xtr,Ytr,minority_lable):\n",
    "    #print(Ytr)\n",
    "    dmaj_x = []\n",
    "    dmin_x = []\n",
    "    \n",
    "    \n",
    "    for i in range(len(Ytr)):\n",
    "        if((Ytr[i]) == minority_lable):\n",
    "            dmin_x.append(Xtr[i])\n",
    "            \n",
    "        else:\n",
    "            dmaj_x.append(Xtr[i])\n",
    "            \n",
    "    \n",
    "    return dmaj_x,dmin_x\n",
    "def splitYtrain_sa_value(Xtr,Ytr,minority_lable,majority_label,pp_Group=p_Group,npp_Group=np_Group): #splite Ytrain based on sensitive attribute value\n",
    "    #print(Ytr)\n",
    "    dmaj_p_x = []\n",
    "    dmaj_np_x = []\n",
    "    dmin_p_x = []\n",
    "    dmin_np_x = []\n",
    "    \n",
    "    \n",
    "    for i in range(len(Ytr)):\n",
    "        if((Ytr[i]) == minority_lable and (Xtr[i][sa_index])==pp_Group): #select minority instances with \"protected\" value \n",
    "            dmin_p_x.append(Xtr[i])\n",
    "        elif((Ytr[i]) == minority_lable and (Xtr[i][sa_index])==npp_Group): #select minority instances with \"protected\" value \n",
    "            dmin_np_x.append(Xtr[i])\n",
    "        elif ((Ytr[i]) == majority_label and (Xtr[i][sa_index])==pp_Group): #select minority(positive class) instances with \"non-protected\" value\n",
    "            dmaj_p_x.append(Xtr[i])\n",
    "        elif ((Ytr[i]) == majority_label and (Xtr[i][sa_index])==npp_Group): #select minority(positive class) instances with \"non-protected\" value\n",
    "            dmaj_np_x.append(Xtr[i])\n",
    "    \n",
    "    return dmin_p_x, dmin_np_x, dmaj_p_x, dmaj_np_x\n",
    "def get_statistics(Xtr,Ytr,minority_lable,majority_label): #splite Ytrain based on sensitive attribute value\n",
    "    #print(Ytr)\n",
    "    dmaj_p_x =0\n",
    "    dmaj_np_x = 0\n",
    "    dmin_p_x = 0\n",
    "    dmin_np_x = 0\n",
    "    \n",
    "    \n",
    "    for i in range(len(Ytr)):\n",
    "        if((Ytr[i]) == minority_lable and (Xtr[i][sa_index])==p_Group): #select minority instances with \"protected\" value \n",
    "            dmin_p_x+=1\n",
    "        elif((Ytr[i]) == minority_lable and (Xtr[i][sa_index])==np_Group): #select minority instances with \"protected\" value \n",
    "            dmin_np_x+=1\n",
    "        elif ((Ytr[i]) == majority_label and (Xtr[i][sa_index])==p_Group): #select minority(positive class) instances with \"non-protected\" value\n",
    "            dmaj_p_x+=1\n",
    "        elif ((Ytr[i]) == majority_label and (Xtr[i][sa_index])==np_Group): #select minority(positive class) instances with \"non-protected\" value\n",
    "            dmaj_np_x+=1\n",
    "    \n",
    "    return dmin_p_x, dmin_np_x, dmaj_p_x, dmaj_np_x\n",
    "def create_synth_data(clinet_traning_x, clinet_traning_y, minority_lable,majority_label,k,r,group,pp_group=p_Group,npp_group=np_Group):\n",
    "    \n",
    "    \n",
    "    \n",
    "    #create two data set from traning data (one for maj (ex.0 class) and one for min(ex.1 class)) \n",
    "    #for simple federated learning\n",
    "    dmaj_client,dmin_client = splitYtrain(clinet_traning_x,clinet_traning_y,minority_lable)\n",
    "    \n",
    "    #for fair federated learning\n",
    "    \n",
    "    \n",
    "    dmin_p_x, dmin_np_x, dmaj_p_x, dmaj_np_x = splitYtrain_sa_value(clinet_traning_x,clinet_traning_y,minority_lable,majority_label, pp_group,npp_group)\n",
    "    \n",
    "    group_names = ['dmin_p_x', 'dmin_np_x', 'dmaj_p_x', 'dmaj_np_x']\n",
    "    group_label_dict = {'dmin_p_x':minority_label, 'dmin_np_x': minority_label, 'dmaj_p_x':  majority_label, 'dmaj_np_x': majority_label}\n",
    "    group_dict = {'dmin_p_x':dmin_p_x, 'dmin_np_x': dmin_np_x, 'dmaj_p_x':  dmaj_p_x, 'dmaj_np_x': dmaj_np_x}\n",
    "    \n",
    "    group_lengths = [len(dmin_p_x),len(dmin_np_x), len(dmaj_p_x), len(dmaj_np_x)] \n",
    "    \n",
    "    #group with maximum length\n",
    "    max_length_group = group_lengths.index(max(group_lengths))  \n",
    "    \n",
    "    #group name which has maximum length\n",
    "    max_group_name = group_names[max_length_group]\n",
    "    \n",
    "    #find the insances in the group with maximum length and store them in dmaj_x\n",
    "    for key, value in group_dict.items():\n",
    "        if key== max_group_name:\n",
    "            dmaj_x = value\n",
    "            break\n",
    "    Xtr_new = []\n",
    "    Ytr_new = []  \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ##Algo 3:\n",
    "    \n",
    "     ##Algo 3:\n",
    "    if group =='min_p':\n",
    "        dmaj_x = dmaj_p_x\n",
    "        dmin_x = dmin_p_x\n",
    "        \n",
    "        #dmin_np = dmin_np_x\n",
    "        x_syn = fair_kSMOTE(dmaj_x,dmin_np_x,dmin_x,k,r)\n",
    "        #x_syn = fair_kSMOTE_algo_2(dmaj_x,dmin_x,k,r)\n",
    "        # add the created synthatic data to the traning data\n",
    "        # here merrge old traning data with the new synthatic data\n",
    "        new_label = minority_label\n",
    "        for j in x_syn:\n",
    "            for s in j:\n",
    "                Xtr_new.append(s)\n",
    "                Ytr_new.append(new_label)\n",
    "        \n",
    "    elif group =='maj_np':\n",
    "        dmaj_x = dmin_np_x\n",
    "        dmin_x = dmaj_np_x\n",
    "        #dmin_np = dmin_np_x\n",
    "        x_syn = fair_kSMOTE_algo_2(dmaj_x,dmin_x,k,r)\n",
    "        \n",
    "        \n",
    "        # add the created synthatic data to the traning data\n",
    "        # here merrge old traning data with the new synthatic data\n",
    "        new_label = majority_label\n",
    "        for j in x_syn:\n",
    "            for s in j:\n",
    "                Xtr_new.append(s)\n",
    "                Ytr_new.append(new_label)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return Xtr_new,Ytr_new\n",
    "def k_nearest_neighbors_modified(data, predict, k,data_rand_index):\n",
    "    distances = []\n",
    "    count=0\n",
    "    for sample in data:\n",
    "        if count in data_rand_index:\n",
    "            count+=1\n",
    "        else:\n",
    "            euclidean_distance = np.linalg.norm(np.array(sample)-np.array(predict))\n",
    "            distances.append([euclidean_distance,count])\n",
    "            count+=1\n",
    "    votes = [i[1] for i in sorted(distances)[:k]] ##votes is returning indexes of k random samples\n",
    "    #vote_result = Counter(votes).most_common(9)[0][0]\n",
    "    return votes\n",
    "def downsample_utility_function(clinet_traning_x, clinet_traning_y,data,data_index, k,r,label,reduction_amount):\n",
    "    S = []\n",
    "    #pick a random k sample from data which we want to downsample\n",
    "    d_rand = []\n",
    "    d_rand_index = []\n",
    "    \n",
    "    #reduction_amount = int(r*len(data))  \n",
    "    #print(\"reduction amount: %s\" % reduction_amount)\n",
    "    \n",
    "    for i in range(reduction_amount):\n",
    "        index = random.randint(0, (len(data)-1))\n",
    "        #random.sample(range(0,len(data)),1)\n",
    "        \n",
    "        d_rand.append(data[index])\n",
    "        d_rand_index.append(index)\n",
    "    data_rand_index = list(d_rand_index)\n",
    "    Sxb = []   \n",
    "    sens_attr_vals = []\n",
    "    k = 3 ##number of nearest neighbours\n",
    "    indices_neighbours_removed = list(d_rand_index)\n",
    "    #print(len(d_rand_index))\n",
    "    #do algorithm (choose the nearest neighbor and linear interpolation)\n",
    "    \n",
    "    Nks=1\n",
    "    S=[]\n",
    "    #do algorithem (choose the nearest neighbor and linear interpolation)\n",
    "    for xb in d_rand:\n",
    "        #N= k_nearest_neighbors(data,xb,k)\n",
    "        N= k_nearest_neighbors_modified(data,xb,k,data_rand_index)\n",
    "        \n",
    "        #do linear interpolation\n",
    "        Sxb = []\n",
    "        for s in range(Nks):\n",
    "            j = 0 #j = 1\n",
    "            #linear interpolation\n",
    "            x_new = ((data[N[j]]-xb) * random.sample(range(0, 1), 1))\n",
    "            indices_neighbours_removed.append(N[1])\n",
    "            j+=1\n",
    "            \n",
    "            while(j < len(N)-1):\n",
    "                #here on random xb\n",
    "                ind = N[j]\n",
    "                x_new = x_new + ((data[ind]-xb) * random.sample(range(0, 1), 1))\n",
    "                j += 1\n",
    "                \n",
    "            x_new = x_new / (len(N)-1) \n",
    "            Synthesized_instance = xb + x_new \n",
    "            Synthesized_instance[sa_index] = xb[sa_index] \n",
    "            \n",
    "            Sxb.append(Synthesized_instance)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        S.append(Sxb)\n",
    "    #print(indices_neighbours_removed)\n",
    "    \n",
    "    \n",
    "    for i in range(len(indices_neighbours_removed)):\n",
    "        indx = indices_neighbours_removed[i]\n",
    "        indices_neighbours_removed[i] = data_index[indx]\n",
    "    indices = np.unique(indices_neighbours_removed)\n",
    "    indices = list(indices)\n",
    "    difference = (reduction_amount*2)-len(indices)\n",
    "    \n",
    "        \n",
    "    #for i in range(0,difference-1):\n",
    "    #    indices.pop(0)\n",
    "    #print(\"reduced amount: %s\" % len(indices))\n",
    "    \n",
    "    indices.sort()\n",
    "    \n",
    "    Xtr = []\n",
    "    Ytr = []\n",
    "    for k in clinet_traning_x:\n",
    "        Xtr.append(k)\n",
    "        \n",
    "    for k in clinet_traning_y:\n",
    "        Ytr.append(k)\n",
    "    \n",
    "    for i in range(len(indices)):\n",
    "        indx = indices[i]-i\n",
    "        Xtr.pop(indx)\n",
    "        Ytr.pop(indx)\n",
    "    \n",
    "    for j in S:\n",
    "            for s in j:\n",
    "                Xtr.append(s)\n",
    "                Ytr.append(label)\n",
    "     \n",
    "    Xtr = np.array(Xtr)\n",
    "    Ytr = np.array(Ytr)\n",
    "    return Xtr, Ytr\n",
    "def splitYtrain_sa_value_index(Xtr,Ytr,minority_lable,majority_label,pp_Group=p_Group,npp_Group=np_Group): #split data based on sensitive attribute value\n",
    "    #print(Ytr)\n",
    "    dmaj_p_x = []\n",
    "    dmaj_p_index = [] ##index of maj_p instance in the main dataset\n",
    "    \n",
    "    dmaj_np_x = []\n",
    "    dmaj_np_index = [] ##index of maj_np instance in the main dataset\n",
    "    \n",
    "    dmin_p_x = []\n",
    "    dmin_p_index = [] ##index of min_p instance in the main dataset\n",
    "    \n",
    "    dmin_np_x = []\n",
    "    dmin_np_index = [] ##index of min_np instance in the main dataset\n",
    "    \n",
    "    \n",
    "    for i in range(len(Ytr)):\n",
    "        if((Ytr[i]) == minority_lable and (Xtr[i][sa_index])==pp_Group): #select minority instances with \"protected\" value \n",
    "            dmin_p_x.append(Xtr[i])\n",
    "            dmin_p_index.append(i)\n",
    "        elif((Ytr[i]) == minority_lable and (Xtr[i][sa_index])==npp_Group): #select minority instances with \"protected\" value \n",
    "            dmin_np_x.append(Xtr[i])\n",
    "            dmin_np_index.append(i)\n",
    "        elif ((Ytr[i]) == majority_label and (Xtr[i][sa_index])==pp_Group): #select minority(positive class) instances with \"non-protected\" value\n",
    "            dmaj_p_x.append(Xtr[i])\n",
    "            dmaj_p_index.append(i)\n",
    "        elif ((Ytr[i]) == majority_label and (Xtr[i][sa_index])==npp_Group): #select minority(positive class) instances with \"non-protected\" value\n",
    "            dmaj_np_x.append(Xtr[i])\n",
    "            dmaj_np_index.append(i)\n",
    "    \n",
    "    return dmin_p_x, dmin_np_x, dmaj_p_x, dmaj_np_x, dmin_p_index, dmin_np_index, dmaj_p_index, dmaj_np_index\n",
    "def splitYtrain_sa_value_index(Xtr,Ytr,minority_lable,majority_label,pp_Group=p_Group,npp_Group=np_Group): #split data based on sensitive attribute value\n",
    "    #print(Ytr)\n",
    "    dmaj_p_x = []\n",
    "    dmaj_p_index = [] ##index of maj_p instance in the main dataset\n",
    "    \n",
    "    dmaj_np_x = []\n",
    "    dmaj_np_index = [] ##index of maj_np instance in the main dataset\n",
    "    \n",
    "    dmin_p_x = []\n",
    "    dmin_p_index = [] ##index of min_p instance in the main dataset\n",
    "    \n",
    "    dmin_np_x = []\n",
    "    dmin_np_index = [] ##index of min_np instance in the main dataset\n",
    "    \n",
    "    \n",
    "    for i in range(len(Ytr)):\n",
    "        if((Ytr[i]) == minority_lable and (Xtr[i][sa_index])==pp_Group): #select minority instances with \"protected\" value \n",
    "            dmin_p_x.append(Xtr[i])\n",
    "            dmin_p_index.append(i)\n",
    "        elif((Ytr[i]) == minority_lable and (Xtr[i][sa_index])==npp_Group): #select minority instances with \"protected\" value \n",
    "            dmin_np_x.append(Xtr[i])\n",
    "            dmin_np_index.append(i)\n",
    "        elif ((Ytr[i]) == majority_label and (Xtr[i][sa_index])==pp_Group): #select minority(positive class) instances with \"non-protected\" value\n",
    "            dmaj_p_x.append(Xtr[i])\n",
    "            dmaj_p_index.append(i)\n",
    "        elif ((Ytr[i]) == majority_label and (Xtr[i][sa_index])==npp_Group): #select minority(positive class) instances with \"non-protected\" value\n",
    "            dmaj_np_x.append(Xtr[i])\n",
    "            dmaj_np_index.append(i)\n",
    "    \n",
    "    return dmin_p_x, dmin_np_x, dmaj_p_x, dmaj_np_x, dmin_p_index, dmin_np_index, dmaj_p_index, dmaj_np_index\n",
    "\n",
    "def downsample(clinet_traning_x, clinet_traning_y, minority_lable,majority_label,k,r,group,pp_group=p_Group,npp_group=np_Group):\n",
    "    \n",
    "    #group: maj_p, maj_np, min_p, min_np\n",
    "    \n",
    "    dmaj_client,dmin_client = splitYtrain(clinet_traning_x,clinet_traning_y,minority_lable)\n",
    "   \n",
    "    #for fair federated learning\n",
    "    \n",
    "    dmin_p_x, dmin_np_x, dmaj_p_x, dmaj_np_x, dmin_p_index, dmin_np_index, dmaj_p_index, dmaj_np_index = splitYtrain_sa_value_index(clinet_traning_x,clinet_traning_y,minority_lable,majority_label,pp_group,npp_group)\n",
    "    \n",
    "    ##Algo 5:\n",
    "    if group =='min_np':\n",
    "        \n",
    "        label = minority_label\n",
    "        data = dmin_np_x\n",
    "        reduction_amount = int(r*len(data))\n",
    "        #print(\"reduction amount: %s\" % reduction_amount)\n",
    "        x_small,y_small = downsample_utility_function(clinet_traning_x, clinet_traning_y,data,dmin_np_index, k,r,label,reduction_amount)\n",
    "        \n",
    "        #difference = len(clinet_traning_x) - len(x_small)\n",
    "        #if difference+2<reduction_amount:\n",
    "        #    reduction_amount = reduction_amount-difference\n",
    "        #    x_small,y_small = downsample_utility_function(x_small,y_small,data,dmin_np_index, k,r,label,reduction_amount)\n",
    "        \n",
    "        \n",
    "        # add the created synthatic data to the traning data\n",
    "        # here merrge old traning data with the new synthatic data\n",
    "        #new_label = minority_label\n",
    "        #for j in x_small:\n",
    "        #    for s in j:\n",
    "        #        Xtr_new.append(s)\n",
    "        #        Ytr_new.append(new_label)\n",
    "        \n",
    "    elif group =='maj_p':\n",
    "        label = majority_label\n",
    "        data = dmaj_p_x\n",
    "        reduction_amount = int(r*len(data))\n",
    "        #print(\"reduction amount: %s\" % reduction_amount)\n",
    "        x_small,y_small = downsample_utility_function(clinet_traning_x, clinet_traning_y,data,dmaj_p_index, k,r,label,reduction_amount)\n",
    "        \n",
    "    return x_small,y_small\n",
    "def create_synth_data_from_smote_nc(clinet_traning_x, clinet_traning_y, minority_lable,majority_label,k,r):\n",
    "    #create two data set from traning data (one for maj (ex.0 class) and one for min(ex.1 class)) \n",
    "    \n",
    "    #for simple federated learning\n",
    "    #dmaj_x,dmin_x = splitYtrain(clinet_traning_x,clinet_traning_y,minority_lable)\n",
    "    \n",
    "    #for fair federated learning\n",
    "    CAT_VARIABLES_INDICES = [1,2,3,4,6,7,8,10,14,15]\n",
    "    dmaj_x,dmin_x,label_min_x,label_maj_x = splitYtrain_smotenc(clinet_traning_x,clinet_traning_y,minority_lable)\n",
    "    smote_nc = SMOTENC(categorical_features=CAT_VARIABLES_INDICES, sampling_strategy = r, random_state=0)\n",
    "    X_resampled, y_resampled = smote_nc.fit_resample(clinet_traning_x, clinet_traning_y)\n",
    "    \n",
    "    return X_resampled,y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d648153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(sa_index)\n",
    "def weight_scalling_factor(clients_trn_data, client_name):\n",
    "    client_names = list(clients_trn_data.keys())\n",
    "    #get the bs\n",
    "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
    "    #first calculate the total training data points across clinets\n",
    "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n",
    "    # get the total number of data points held by a client\n",
    "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n",
    "    return local_count/global_count\n",
    "\n",
    "\n",
    "def scale_model_weights(weight, scalar):\n",
    "    '''function for scaling a models weights'''\n",
    "    weight_final = []\n",
    "    steps = len(weight)\n",
    "    for i in range(steps):\n",
    "        weight_final.append(scalar * weight[i])\n",
    "    return weight_final\n",
    "\n",
    "\n",
    "\n",
    "def sum_scaled_weights(scaled_weight_list):\n",
    "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
    "    avg_grad = list()\n",
    "    #get the average grad accross all client gradients\n",
    "    for grad_list_tuple in zip(*scaled_weight_list):\n",
    "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "        \n",
    "    return avg_grad\n",
    "\n",
    "def find_statistical_parity_score(data,labels,predictions,pp_Group=p_Group,npp_Group=np_Group):\n",
    "    protected_pos = 0.\n",
    "    protected_neg = 0.\n",
    "    non_protected_pos = 0.\n",
    "    non_protected_neg = 0.\n",
    "\n",
    "    tp_protected = 0.\n",
    "    tn_protected = 0.\n",
    "    fp_protected = 0.\n",
    "    fn_protected = 0.\n",
    "\n",
    "    tp_non_protected = 0.\n",
    "    tn_non_protected = 0.\n",
    "    fp_non_protected = 0.\n",
    "    fn_non_protected = 0.\n",
    "    \n",
    "    saIndex = sa_index\n",
    "    saValue = pp_Group\n",
    "    \n",
    "    for idx, val in enumerate(data):\n",
    "        # protected population\n",
    "        if val[saIndex] == saValue:\n",
    "            if predictions[idx] == 1:\n",
    "                protected_pos += 1.\n",
    "            else:\n",
    "                protected_neg += 1.\n",
    "            # correctly classified\n",
    "            '''\n",
    "            if labels[idx] == predictions[idx]:\n",
    "                if labels[idx] == 1:\n",
    "                    tp_protected += 1.\n",
    "                else:\n",
    "                    tn_protected += 1.\n",
    "            # misclassified\n",
    "            else:\n",
    "                if labels[idx] == 1:\n",
    "                    fn_protected += 1.\n",
    "                else:\n",
    "                    fp_protected += 1.\n",
    "            '''\n",
    "        else:\n",
    "            if predictions[idx] == 1:\n",
    "                non_protected_pos += 1.\n",
    "            else:\n",
    "                non_protected_neg += 1.\n",
    "            '''\n",
    "            # correctly classified\n",
    "            if labels[idx] == predictions[idx]:\n",
    "                if labels[idx] == 1:\n",
    "                    tp_non_protected += 1.\n",
    "                else:\n",
    "                    tn_non_protected += 1.\n",
    "            # misclassified\n",
    "            else:\n",
    "                if labels[idx] == 1:\n",
    "                    fn_non_protected += 1.\n",
    "                else:\n",
    "                    fp_non_protected += 1.\n",
    "            '''\n",
    "    '''\n",
    "    tpr_protected = tp_protected / (tp_protected + fn_protected)\n",
    "    tnr_protected = tn_protected / (tn_protected + fp_protected)\n",
    "\n",
    "    tpr_non_protected = tp_non_protected / (tp_non_protected + fn_non_protected)\n",
    "    tnr_non_protected = tn_non_protected / (tn_non_protected + fp_non_protected)\n",
    "    '''\n",
    "    C_prot = (protected_pos) / (protected_pos + protected_neg)\n",
    "    C_non_prot = (non_protected_pos) / (non_protected_pos + non_protected_neg)\n",
    "\n",
    "    stat_par = C_non_prot - C_prot\n",
    "    '''\n",
    "    print(\"protected_pos: %s\" %protected_pos)\n",
    "    print(\"protected_neg: %s\" %protected_neg)\n",
    "    print(\"non-protected_pos: %s\" %non_protected_pos)\n",
    "    print(\"non-protected_neg: %s\" %non_protected_neg)\n",
    "    '''\n",
    "    return stat_par\n",
    "    \n",
    "def find_eqop_score(data,labels,predictions, pp_Group=p_Group,npp_Group=np_Group):\n",
    "    print(\"p_Group %s\" % pp_Group)\n",
    "    print(\"np_Group %s\" % npp_Group)\n",
    "    protected_pos = 0.\n",
    "    protected_neg = 0.\n",
    "    non_protected_pos = 0.\n",
    "    non_protected_neg = 0.\n",
    "\n",
    "    tp_protected = 0.\n",
    "    tn_protected = 0.\n",
    "    fp_protected = 0.\n",
    "    fn_protected = 0.\n",
    "\n",
    "    tp_non_protected = 0.\n",
    "    tn_non_protected = 0.\n",
    "    fp_non_protected = 0.\n",
    "    fn_non_protected = 0.\n",
    "    saIndex = sa_index\n",
    "    saValue = pp_Group\n",
    "    for idx, val in enumerate(data):\n",
    "        # protrcted population\n",
    "        if val[saIndex] == saValue:\n",
    "            '''\n",
    "            if predictions[idx] == 1:\n",
    "                protected_pos += 1.\n",
    "            else:\n",
    "                protected_neg += 1.\n",
    "            '''\n",
    "            # correctly classified\n",
    "            if labels[idx] == predictions[idx]:\n",
    "                if labels[idx] == 1:\n",
    "                    tp_protected += 1.\n",
    "                #else:\n",
    "                #    tn_protected += 1.\n",
    "            # misclassified\n",
    "            else:\n",
    "                if labels[idx] == 1:\n",
    "                    fn_protected += 1.\n",
    "                #else:\n",
    "                #    fp_protected += 1.\n",
    "\n",
    "        else:\n",
    "            '''\n",
    "            if predictions[idx] == 1:\n",
    "                non_protected_pos += 1.\n",
    "            else:\n",
    "                non_protected_neg += 1.\n",
    "            '''\n",
    "            # correctly classified\n",
    "            if labels[idx] == predictions[idx]:\n",
    "                if labels[idx] == 1:\n",
    "                    tp_non_protected += 1.\n",
    "                #else:\n",
    "                #    tn_non_protected += 1.\n",
    "            # misclassified\n",
    "            else:\n",
    "                if labels[idx] == 1:\n",
    "                    fn_non_protected += 1.\n",
    "                #else:\n",
    "                #    fp_non_protected += 1.\n",
    "\n",
    "    tpr_protected = tp_protected / (tp_protected + fn_protected)\n",
    "    #tnr_protected = tn_protected / (tn_protected + fp_protected)\n",
    "\n",
    "    tpr_non_protected = tp_non_protected / (tp_non_protected + fn_non_protected)\n",
    "    #tnr_non_protected = tn_non_protected / (tn_non_protected + fp_non_protected)\n",
    "\n",
    "    \n",
    "    eqop = tpr_non_protected - tpr_protected\n",
    "    return eqop\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "def test_client_model(X_test, Y_test,  model,pp_Group=p_Group,npp_Group=np_Group):\n",
    "    \n",
    "    cce = tf.keras.losses.BinaryCrossentropy()\n",
    "    logits = model.predict(X_test)\n",
    "    preidect = np.around(logits)\n",
    "    preidect = np.nan_to_num(preidect)\n",
    "    Y_test = np.nan_to_num(Y_test)\n",
    "    conf = (confusion_matrix(Y_test,preidect)) \n",
    "    TN = conf[0][0]\n",
    "    FP = conf[0][1]\n",
    "    FN = conf[1][0]\n",
    "    TP = conf[1][1]\n",
    "    sensitivity = TP/(TP+FN) \n",
    "    specificity = TN/(FP+TN)\n",
    "        \n",
    "    BalanceACC = (sensitivity+specificity)/2\n",
    "    stat_parity = find_statistical_parity_score(X_test,Y_test,preidect, pp_Group, npp_Group)\n",
    "    assigned_positive_labels = 0\n",
    "    total_positive_labels = 0\n",
    "    \n",
    "    \n",
    "    unique, counts = np.unique(preidect, return_counts=True)\n",
    "    count_ap_dict = dict(zip(unique, counts))\n",
    "    assigned_positive_labels = count_ap_dict.get(1,0)\n",
    "    \n",
    "    unique, counts = np.unique(Y_test, return_counts=True)\n",
    "    count_tp_dict = dict(zip(unique, counts))\n",
    "    total_positive_labels = count_tp_dict.get(1,0)\n",
    "    \n",
    "     \n",
    "    #eqop = find_eqop_score(X_test,Y_test,preidect, pp_Group,npp_Group) \n",
    "    print('stat_parity: {}'.format(stat_parity))\n",
    "    return stat_parity, assigned_positive_labels, total_positive_labels, BalanceACC\n",
    "    #print('stat_parity: {}'.format(stat_parity))\n",
    "    \n",
    "    #return stat_parity, assigned_positive_labels, total_positive_labels, BalanceACC\n",
    "\n",
    "\n",
    "def find_class_Weight(labels,majority_label,minority_label):\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    count_ap_dict = dict(zip(unique, counts))\n",
    "    \n",
    "    majority_class_weight = 1\n",
    "    minority_class_weight = count_ap_dict.get(majority_label,0)/count_ap_dict.get(minority_label,1)\n",
    "    class_weights={majority_label:1,minority_label:minority_class_weight}\n",
    "    \n",
    "    return class_weights\n",
    "\n",
    "def test_model(X_test, Y_test,  model, comm_round):\n",
    "    cce = tf.keras.losses.BinaryCrossentropy()\n",
    "    logits = model.predict(X_test)\n",
    "    preidect = np.around(logits)\n",
    "    preidect = np.nan_to_num(preidect)\n",
    "    Y_test = np.nan_to_num(Y_test)\n",
    "    stat_parity = find_statistical_parity_score(X_test,Y_test,preidect)\n",
    "    eqop = find_eqop_score(X_test,Y_test,preidect)    \n",
    "        \n",
    "    conf = (confusion_matrix(Y_test,preidect))   \n",
    "    loss = cce(Y_test, preidect)\n",
    "    acc = accuracy_score(preidect,Y_test)\n",
    "    print('comm_round: {} | global_acc: {} | global_loss: {}'.format(comm_round, acc, loss))\n",
    "    return acc, loss,conf,stat_parity, eqop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b06b366",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bismillah\n",
      "client_1\n",
      "stat_parity: 0.05932344967188391\n",
      "balanced_accuracy_client 0.6027768263754106\n",
      "stat_parity: 0.05247592175274485\n",
      "stat_parity: 0.04322544318813165\n",
      "stat_parity: 0.025819899065098906\n",
      "stat_parity: 0.006592908312775592\n",
      "stat_parity: -0.00935893414201422\n",
      "stat_parity: 0.00935893414201422\n",
      "stat_parity: -0.009432249645359603\n",
      "stat_parity: 0.009432249645359603\n",
      "stat_parity: -0.0010538519883736552\n",
      "stat_parity: 0.0010538519883736552\n",
      "bismillah\n",
      "client_2\n",
      "stat_parity: 0.09253526981551885\n",
      "balanced_accuracy_client 0.6036817177909506\n",
      "stat_parity: 0.08245377165746437\n",
      "stat_parity: 0.07292051765620072\n",
      "stat_parity: 0.038008667203893565\n",
      "stat_parity: 0.019547608429323265\n",
      "stat_parity: 0.013519465152230764\n",
      "stat_parity: 0.017094493185604798\n",
      "stat_parity: 0.021752257202320713\n",
      "stat_parity: 0.03455012949725755\n",
      "1\n",
      "stat_parity: 0.038536225082424436\n",
      "2\n",
      "stat_parity: 0.04200319909248487\n",
      "stat_parity: 0.0371139567776973\n",
      "stat_parity: 0.037411654279361134\n",
      "stat_parity: 0.027529853824286132\n",
      "stat_parity: 0.025113835202475315\n",
      "3\n",
      "bismillah\n",
      "client_3\n",
      "stat_parity: 0.07235382451495959\n",
      "balanced_accuracy_client 0.6275096198740964\n",
      "stat_parity: 0.05151517028826397\n",
      "stat_parity: 0.035455063061133885\n",
      "stat_parity: -0.003092537314828503\n",
      "stat_parity: 0.003092537314828503\n",
      "p_Group 0\n",
      "np_Group 1\n",
      "comm_round: 0 | global_acc: 0.7841666666666667 | global_loss: 5.126479625701904\n",
      "x_test 6000 \n",
      "bismillah\n",
      "client_1\n",
      "stat_parity: -0.018418885793475115\n",
      "stat_parity: 0.018418885793475115\n",
      "stat_parity: 0.0016813533107962186\n",
      "bismillah\n",
      "client_2\n",
      "stat_parity: 0.01743668352039776\n",
      "stat_parity: 0.019074019779549006\n",
      "stat_parity: 0.018367912739500003\n",
      "stat_parity: 0.0174403816260085\n",
      "1\n",
      "stat_parity: 0.015752196414709707\n",
      "stat_parity: 0.012848836812884762\n",
      "stat_parity: 0.0048257966904035055\n",
      "bismillah\n",
      "client_3\n",
      "stat_parity: -0.02227896283286096\n",
      "stat_parity: 0.02227896283286096\n",
      "stat_parity: 0.0017559094206960058\n",
      "p_Group 0\n",
      "np_Group 1\n",
      "comm_round: 1 | global_acc: 0.8036666666666666 | global_loss: 4.9575958251953125\n",
      "x_test 6000 \n",
      "bismillah\n",
      "client_1\n",
      "stat_parity: -0.001341043291287064\n",
      "stat_parity: 0.001341043291287064\n",
      "bismillah\n",
      "client_2\n",
      "stat_parity: 0.031308277666250356\n",
      "stat_parity: 0.01398877786739261\n",
      "stat_parity: 0.003537006885063698\n",
      "bismillah\n",
      "client_3\n",
      "stat_parity: -0.00595807582157612\n",
      "stat_parity: 0.00595807582157612\n",
      "stat_parity: 0.009899929995063783\n",
      "stat_parity: 0.013992571224382688\n",
      "stat_parity: 0.023774209358592785\n",
      "1\n",
      "stat_parity: 0.021280027810276214\n",
      "2\n",
      "stat_parity: 0.023735051479752883\n",
      "3\n",
      "p_Group 0\n",
      "np_Group 1\n",
      "comm_round: 2 | global_acc: 0.8075 | global_loss: 4.913600921630859\n",
      "x_test 6000 \n",
      "bismillah\n",
      "client_1\n",
      "stat_parity: -0.0023331374750911182\n",
      "stat_parity: 0.0023331374750911182\n",
      "bismillah\n",
      "client_2\n",
      "stat_parity: 0.027854015894226786\n",
      "stat_parity: 0.015166855636011167\n"
     ]
    }
   ],
   "source": [
    "smlp_global = SimpleMLP()\n",
    "num_layers_mult=1\n",
    "n=num_layers_mult\n",
    "comms_round = 50\n",
    "global_model = smlp_global.build(Xtr,n)\n",
    "sensitivity_= []\n",
    "specificity_= []\n",
    "BalanceACC_= []\n",
    "G_mean_= []\n",
    "FP_rate_= []\n",
    "FN_rate_= []\n",
    "assigned_positives = 0\n",
    "total_positives = 0\n",
    "accuracy_= []\n",
    "loss_= []\n",
    "statistical_parity_ = []\n",
    "eqop_ = []\n",
    "disc_thresh = 0.005\n",
    "lambda_initial = 0.005\n",
    "disc_tolerance = 0.1\n",
    "epsilon=1\n",
    "pp_group = 0\n",
    "npp_group=1\n",
    "#commence global training loop\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "end_time = []\n",
    "e=EarlyStopping(patience=5,restore_best_weights=True)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "for comm_round in range(comms_round):\n",
    "            \n",
    "    # get the global model's weights - will serve as the initial weights for all local models\n",
    "    global_weights = global_model.get_weights()\n",
    "    \n",
    "    #initial list to collect local model weights after scalling\n",
    "    scaled_local_weight_list = list()\n",
    "\n",
    "    #randomize client data - using keys\n",
    "    client_names= list(clients_batched.keys())\n",
    "    #random.shuffle(client_names)\n",
    "    client_number=0\n",
    "    #loop through each client and create new local model\n",
    "    for client in client_names:   \n",
    "        smlp_local = SimpleMLP()\n",
    "        local_model = smlp_local.build(Xtr, n)\n",
    "        local_model.compile(loss=loss, \n",
    "                      optimizer=optimizer, \n",
    "                      metrics=metrics)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #set local model weight to the weight of the global model\n",
    "        local_model.set_weights(global_weights)\n",
    "        \n",
    "        #here we should get the traning data from clients and try to do ksmote and see changes\n",
    "        count=0\n",
    "        xxte = []\n",
    "        yte = []\n",
    "        \n",
    "        \n",
    "        for(X_test, Y_test) in clients_batched[client]:\n",
    "            \n",
    "            i = 0\n",
    "            while (i <len(X_test)):       \n",
    "                xxte.append(X_test[i].numpy())  \n",
    "                yte.append(Y_test[i].numpy())\n",
    "                i +=1\n",
    "        \n",
    "        \n",
    "        y_test_client = client_data_testy[client_number]\n",
    "        x_test_client = client_data_testx[client_number]\n",
    "        test_batched = tf.data.Dataset.from_tensor_slices((x_test_client, y_test_client)).batch(len(y_test_client))\n",
    "        #print(x_test_client[0])\n",
    "        client_number+=1    \n",
    "        #here we change k and r to see how affect our result\n",
    "        #for simple federated learning\n",
    "        #Xtr_1_new,Ytr_1_new = create_synth_data(xxte,yte, 1,8,0.2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #for fair federated learning\n",
    "        minority_label = 1\n",
    "        majority_label = 0\n",
    "        #lambda_score = 0.2 #0.1,0.15,0.2,0.25,0.3\n",
    "        print(\"bismillah\")\n",
    "        print(client)\n",
    "        \n",
    "        ###\n",
    "        prev_xtr,prev_ytr=[],[]\n",
    "        prev_disc_score= []\n",
    "        ###\n",
    "        \n",
    "        if comm_round == 0:\n",
    "            Xtr_1_new=np.array(xxte)\n",
    "            Ytr_1_new=np.array(yte)\n",
    "            #Xtr_1_new,Ytr_1_new = create_synth_data(xxte,yte, minority_label,majority_label,5,0.2,'min_p')\n",
    "            #for k in xxte:\n",
    "            #    Xtr_1_new.append(k)\n",
    "                    \n",
    "            #for k in yte:\n",
    "            #    Ytr_1_new.append(k)    \n",
    "                    \n",
    "            Xtr_1_new = np.array(Xtr_1_new)\n",
    "            Ytr_1_new = np.array(Ytr_1_new)\n",
    "            added_points = len(Xtr_1_new) - len(xxte)\n",
    "            \n",
    "            #new batch for new data\n",
    "            #split data for validation set\n",
    "            Xtr_1_new_split,x_val,Ytr_1_new_split,y_val=train_test_split(Xtr_1_new,Ytr_1_new,test_size=0.10,shuffle=True)\n",
    "        \n",
    "            data = list(zip(Xtr_1_new_split, Ytr_1_new_split))\n",
    "            random.shuffle(data)\n",
    "            btach_data = batch_data(data, bs=30)  \n",
    "            \n",
    "            class_weights=find_class_Weight(Ytr_1_new, majority_label,minority_label)\n",
    "            local_model.fit(btach_data,validation_data=(x_val,y_val),validation_steps=1, callbacks=[e], epochs=3, verbose=0, \n",
    "                            class_weight=class_weights)\n",
    "            ##find disc score for each client\n",
    "            disc_score, assigned_positives, total_positives, balanced_accuracy_client = test_client_model(x_test_client, y_test_client,  local_model)\n",
    "            print(\"balanced_accuracy_client %s\" % balanced_accuracy_client)\n",
    "            if disc_score<0:\n",
    "                pp_group = 1\n",
    "                npp_group=0\n",
    "                disc_score, assigned_positives, total_positives, balanced_accuracy_client = test_client_model(x_test_client, y_test_client,  local_model,1,0)\n",
    "            else:\n",
    "                pp_group = 0\n",
    "                npp_group=1\n",
    "            trade_off = (1+epsilon**2)*((balanced_accuracy_client*(1-abs(disc_score)))/(epsilon*balanced_accuracy_client+(1-abs(disc_score))))\n",
    "            lambda_score = lambda_initial*(1+(disc_score/disc_tolerance))\n",
    "            \n",
    "        \n",
    "        if comm_round!=0:\n",
    "            disc_score, assigned_positives, total_positives, balanced_accuracy_client = test_client_model(x_test_client, y_test_client,  local_model)\n",
    "            if disc_score<0:\n",
    "                pp_group = 1\n",
    "                npp_group=0\n",
    "                disc_score, assigned_positives, total_positives, balanced_accuracy_client = test_client_model(x_test_client, y_test_client,  local_model,1,0)\n",
    "            else:\n",
    "                pp_group = 0\n",
    "                npp_group=1\n",
    "            trade_off = (1+epsilon**2)*((balanced_accuracy_client*(1-abs(disc_score)))/(epsilon*balanced_accuracy_client+(1-abs(disc_score))))\n",
    "        #Xtr_1_new, Ytr_1_new = xxte, yte \n",
    "        if disc_score > disc_thresh:\n",
    "            \n",
    "        \n",
    "                prev_xtr,prev_ytr=[],[]\n",
    "                prev_disc_score= []\n",
    "                prev_trade_off=[]\n",
    "                \n",
    "                lambda_score = lambda_initial*(1+(disc_score/disc_tolerance))\n",
    "                greater_disc_score = 0\n",
    "                min_disc_score =disc_score\n",
    "                max_trade_off = trade_off\n",
    "                Xtr_1_new=np.array(xxte)\n",
    "                Ytr_1_new=np.array(yte)\n",
    "                prev_xtr,prev_ytr = Xtr_1_new, Ytr_1_new\n",
    "                class_weights=find_class_Weight(Ytr_1_new, majority_label,minority_label)\n",
    "                points = len(Xtr_1_new) - len(xxte)\n",
    "                itr = 0\n",
    "                \n",
    "                while disc_score > disc_thresh:\n",
    "                    \n",
    "                    \n",
    "                    itr+=1\n",
    "                    closest_to_zero = min(disc_score, min_disc_score, key=abs)\n",
    "                    if trade_off>max_trade_off:\n",
    "                    #if closest_to_zero != min_disc_score:\n",
    "                        min_disc_score = disc_score\n",
    "                        max_trade_off = trade_off\n",
    "                        prev_xtr,prev_ytr = Xtr_1_new, Ytr_1_new\n",
    "                        points = len(Xtr_1_new) - len(xxte)\n",
    "                        local_model.save_weights('./checkpoints/default/age-sp/my_checkpoint')\n",
    "                    if assigned_positives<= total_positives:\n",
    "                        \n",
    "                        \n",
    "                        #N(C+,S−) =N(C+,S−)+λ× N(C−,S−)\n",
    "                        Xtr_min_p_new,Ytr_min_p_new = create_synth_data(xxte, yte, minority_label,majority_label,5,lambda_score,'min_p',pp_group,npp_group)\n",
    "                    \n",
    "                        #N(C−,S−) =N(C−,S−)−λ× N(C−,S−)\n",
    "                        Xtr_maj_p_new,Ytr_maj_p_new = downsample(xxte, yte, minority_label,majority_label,5,lambda_score,'maj_p',pp_group,npp_group)    \n",
    "                        \n",
    "                        for k in Xtr_maj_p_new:\n",
    "                            Xtr_min_p_new.append(k)\n",
    "                        for k in Ytr_maj_p_new:\n",
    "                            Ytr_min_p_new.append(k)\n",
    "                        \n",
    "                        Xtr_1_new = np.array(Xtr_min_p_new)\n",
    "                        Ytr_1_new = np.array(Ytr_min_p_new)\n",
    "                        \n",
    "                        \n",
    "                    else:\n",
    "                        \n",
    "                        #N(C−,S+) =N(C−,S+)+λ× N(C+,S+)\n",
    "                        Xtr_maj_np_new,Ytr_maj_np_new = create_synth_data(xxte, yte, minority_label,majority_label,5,lambda_score,'maj_np',pp_group,npp_group)\n",
    "                    \n",
    "                        #N(C+,S+) =N(C+,S+)−λ× N(C+,S+)\n",
    "                        Xtr_min_np_new,Ytr_min_np_new = downsample(xxte, yte, minority_label,majority_label,5,lambda_score,'min_np',pp_group,npp_group)\n",
    "                        \n",
    "                        for k in Xtr_min_np_new:\n",
    "                            Xtr_maj_np_new.append(k)\n",
    "                        for k in Ytr_min_np_new:\n",
    "                            Ytr_maj_np_new.append(k)\n",
    "                        \n",
    "                        Xtr_1_new = np.array(Xtr_maj_np_new)\n",
    "                        Ytr_1_new = np.array(Ytr_maj_np_new)\n",
    "                        \n",
    "                    \n",
    "                    added_points = len(Xtr_1_new) - len(xxte)\n",
    "                    #new batch for new data\n",
    "                    #split data for validation set\n",
    "                    Xtr_1_new_split,x_val,Ytr_1_new_split,y_val=train_test_split(Xtr_1_new,Ytr_1_new,test_size=0.10,shuffle=True)\n",
    "                    \n",
    "                    data = list(zip(Xtr_1_new_split, Ytr_1_new_split))\n",
    "                    random.shuffle(data)\n",
    "                    btach_data = batch_data(data, bs=30)  \n",
    "                    #fit local model with client's data\n",
    "                    #create validation data and early stop\n",
    "                    \n",
    "                    local_model.fit(btach_data,validation_data=(x_val,y_val),validation_steps=1, callbacks=[e], epochs=3, verbose=0,\n",
    "                                    class_weight = class_weights)\n",
    "                    ##find disc score for each client\n",
    "                    \n",
    "                    disc_score, assigned_positives, total_positives, balanced_accuracy_client = test_client_model(x_test_client, y_test_client,  local_model)\n",
    "                    if disc_score<0:\n",
    "                        pp_group = 1\n",
    "                        npp_group=0\n",
    "                        disc_score, assigned_positives, total_positives, balanced_accuracy_client = test_client_model(x_test_client, y_test_client,  local_model,1,0)\n",
    "                    else:\n",
    "                        pp_group = 0\n",
    "                        npp_group=1\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    trade_off = (1+epsilon**2)*((balanced_accuracy_client*(1-abs(disc_score)))/(epsilon*balanced_accuracy_client+(1-abs(disc_score))))\n",
    "                    \n",
    "                    closest_to_zero = min(disc_score, min_disc_score, key=abs)\n",
    "                    if trade_off>max_trade_off:\n",
    "                    #if closest_to_zero != min_disc_score:\n",
    "                        \n",
    "                        min_disc_score = disc_score\n",
    "                        max_trade_off = trade_off\n",
    "                        prev_xtr,prev_ytr = Xtr_1_new, Ytr_1_new\n",
    "                        points = len(Xtr_1_new) - len(xxte)\n",
    "                        local_model.save_weights('./checkpoints/default/age-sp/my_checkpoint')\n",
    "                    #if len(prev_disc_score)>0:\n",
    "                    if len(prev_trade_off)>0:\n",
    "                        #if disc_score>prev_disc_score[-1]:\n",
    "                        if trade_off<prev_trade_off[-1]:\n",
    "                            greater_disc_score+=1\n",
    "                            print(greater_disc_score)\n",
    "                            if greater_disc_score >2:\n",
    "                                local_model.load_weights('./checkpoints/default/age-sp/my_checkpoint')\n",
    "                                added_points = points\n",
    "                                \n",
    "                                #Xtr_1_new, Ytr_1_new =prev_xtr,prev_ytr\n",
    "                                #xxte, yte = Xtr_1_new, Ytr_1_new\n",
    "                                break\n",
    "                                \n",
    "                                \n",
    "                               \n",
    "                     \n",
    "                    #greater_disc_score = 0\n",
    "                    #lambda_score= lambda_score+0.01\n",
    "                    #prev_disc_score.append(disc_score)\n",
    "                    prev_trade_off.append(trade_off)\n",
    "                    xxte, yte = Xtr_1_new, Ytr_1_new\n",
    "                    \n",
    "                \n",
    "                \n",
    "                \n",
    "                xxtee=[]\n",
    "                ytee=[]\n",
    "                dataset = tf.data.Dataset.from_tensor_slices((list(prev_xtr), list(prev_ytr)))\n",
    "                up_dict = {client:batch_data(dataset)}\n",
    "                clients_batched.update(up_dict)\n",
    "                #clients_batched[client_name] = batch_data(dataset)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "        else:\n",
    "            \n",
    "            if comm_round!=0:\n",
    "                Xtr_1_new=np.array(xxte)\n",
    "                Ytr_1_new=np.array(yte)\n",
    "                #class_weights = class_weight.compute_class_weight('balanced',\n",
    "                #                                 np.unique(Ytr_1_new),\n",
    "                #                                 Ytr_1_new)\n",
    "                #class_weights = {majority_label:1, minority_label:7}\n",
    "                class_weights=find_class_Weight(Ytr_1_new, majority_label,minority_label)\n",
    "                Xtr_1_new,x_val,Ytr_1_new,y_val=train_test_split(Xtr_1_new,Ytr_1_new,test_size=0.10,shuffle=True)\n",
    "                data = list(zip(Xtr_1_new,Ytr_1_new))\n",
    "                #random.shuffle(data)\n",
    "                added_points = 0\n",
    "                btach_data = batch_data(data, bs=30)\n",
    "                \n",
    "                local_model.fit(btach_data,validation_data=(x_val,y_val),validation_steps = 1, callbacks=[e], epochs=3, \n",
    "                                verbose=0, class_weight = class_weights)\n",
    "        \n",
    "        #scale the model weights and add to list       \n",
    "        client_names = list(clients_batched.keys())\n",
    "        bs = list(clients_batched[client_name])[0][0].shape[0]\n",
    "        #first calculate the total training data points across clinets\n",
    "        global_count = sum([tf.data.experimental.cardinality(clients_batched[client_name]).numpy() for client_name in client_names])*bs\n",
    "        global_count = global_count + (added_points)\n",
    "        # get the total number of data points held by a client\n",
    "        local_count = len(Xtr_1_new)\n",
    "        scaling_factor = local_count/global_count\n",
    "                \n",
    "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "        scaled_local_weight_list.append(scaled_weights)\n",
    "        \n",
    "        #clear session to free memory after each communication round\n",
    "        K.clear_session()\n",
    "        \n",
    "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "    #update global model \n",
    "    \n",
    "    global_model.set_weights(average_weights)\n",
    "    \n",
    "    #test global model and print out metrics after each communications round\n",
    "    for(X_test, Y_test) in test_batched:\n",
    "        global_acc, global_loss,conf,stat_parity,eqop = test_model(X_test, Y_test, global_model, comm_round)\n",
    "        print(\"x_test %s \"% len(X_test))\n",
    "        \n",
    "        TN = conf[0][0]\n",
    "        FP = conf[0][1]\n",
    "        FN = conf[1][0]\n",
    "        TP = conf[1][1]\n",
    "        sensitivity = TP/(TP+FN) \n",
    "        specificity = TN/(FP+TN)\n",
    "        \n",
    "        BalanceACC = (sensitivity+specificity)/2\n",
    "        G_mean = math.sqrt(sensitivity*specificity)\n",
    "        FN_rate= FN/(FN+TP) \n",
    "        FP_rate = FP/(FP+TN) \n",
    "        #add the data to arrays\n",
    "        sensitivity_.append(sensitivity)\n",
    "        specificity_.append(specificity)\n",
    "        BalanceACC_.append(BalanceACC)\n",
    "        G_mean_.append(G_mean)\n",
    "        FP_rate_.append(FP_rate)\n",
    "        FN_rate_.append(FN_rate)\n",
    "        accuracy_.append(global_acc)\n",
    "        loss_.append(global_loss)\n",
    "        statistical_parity_.append(stat_parity)\n",
    "        eqop_.append(eqop)\n",
    "    end_time.append(((time.time()) - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371f6a8f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(sensitivity_[index])\n",
    "print(specificity_[index])\n",
    "print(BalanceACC_[index])\n",
    "print(G_mean_[index])\n",
    "print(FP_rate_[index])\n",
    "print(FN_rate_[index])\n",
    "print(accuracy_[index])\n",
    "print(statistical_parity_[index])\n",
    "print(eqop_[index])\n",
    "print(statistical_parity_.index(min(statistical_parity_[0:-1], key=abs)))\n",
    "print(BalanceACC_.index(max(BalanceACC_[0:-1], key=abs)))\n",
    "print(trade_off.index(max(trade_off[0:-1], key=abs)))\n",
    "destination = \"./results/default/sp/\"\n",
    "client=\"age\"\n",
    "output_file = open(destination+client+\"-client-bal_acc.txt\", \"w+\")\n",
    "output_file.write(str(BalanceACC_))\n",
    "output_file.close()\n",
    "\n",
    "output_file = open(destination+client+\"-client-sen.txt\", \"w+\")\n",
    "output_file.write(str(sensitivity_))\n",
    "output_file.close()\n",
    "\n",
    "output_file = open(destination+client+\"-client-spc.txt\", \"w+\")\n",
    "output_file.write(str(specificity_))\n",
    "output_file.close()\n",
    "\n",
    "output_file = open(destination+client+\"-client-gmean.txt\", \"w+\")\n",
    "output_file.write(str(G_mean_))\n",
    "output_file.close()\n",
    "\n",
    "output_file = open(destination+client+\"-client-fp_rate.txt\", \"w+\")\n",
    "output_file.write(str(FP_rate_))\n",
    "output_file.close()\n",
    "\n",
    "output_file = open(destination+client+\"-client-fn_rate.txt\", \"w+\")\n",
    "output_file.write(str(FN_rate_))\n",
    "output_file.close()\n",
    "\n",
    "output_file = open(destination+client+\"-client-sp.txt\", \"w+\")\n",
    "output_file.write(str(statistical_parity_))\n",
    "output_file.close()\n",
    "\n",
    "output_file = open(destination+client+\"-client-eqop.txt\", \"w+\")\n",
    "output_file.write(str(eqop_))\n",
    "output_file.close()\n",
    "print(index)\n",
    "#pyplot.plot(BalanceACC_)\n",
    "plt.plot(BalanceACC_)\n",
    "plt.title('BalanceACC')\n",
    "plt.xlabel('Global Round')\n",
    "plt.ylabel('BalanceACC')\n",
    "plt.show()\n",
    "plt.plot(eqop_)\n",
    "plt.title('statistical_parity')\n",
    "plt.xlabel('Global Round')\n",
    "plt.ylabel('statistical_parity_')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1086c59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b1d0e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
