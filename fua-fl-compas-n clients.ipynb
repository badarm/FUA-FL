{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b79fd593",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b48a022dddf8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_style\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'darkgrid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-keras\\lib\\site-packages\\seaborn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Import seaborn objects\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mrcmod\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m  \u001b[1;31m# noqa: F401,F403\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m  \u001b[1;31m# noqa: F401,F403\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpalettes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m  \u001b[1;31m# noqa: F401,F403\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mrelational\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m  \u001b[1;31m# noqa: F401,F403\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-keras\\lib\\site-packages\\seaborn\\rcmod.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcycler\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcycler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpalettes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-keras\\lib\\site-packages\\seaborn\\palettes.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mexternal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhusl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdesaturate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_color_cycle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcolors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mxkcd_rgb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcrayons\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-keras\\lib\\site-packages\\seaborn\\utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-keras\\lib\\site-packages\\scipy\\stats\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m \"\"\"\n\u001b[1;32m--> 388\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmorestats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-keras\\lib\\site-packages\\scipy\\stats\\stats.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mspecial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmstats_basic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m from ._stats_mstats_common import (_find_repeats, linregress, theilslopes,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-keras\\lib\\site-packages\\scipy\\stats\\distributions.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#       instead of `git blame -Lxxx,+x`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m from ._distn_infrastructure import (entropy, rv_discrete, rv_continuous,\n\u001b[0m\u001b[0;32m      9\u001b[0m                                     rv_frozen)\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-keras\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m# for functions of continuous distributions (e.g. moments, entropy, cdf)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mintegrate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# to approximate the pdf of a continuous distribution given its cdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-keras\\lib\\site-packages\\scipy\\integrate\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_quadrature\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0modepack\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mquadpack\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_ode\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_bvp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msolve_bvp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-keras\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-keras\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-keras\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-keras\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-keras\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-keras\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder,MinMaxScaler,StandardScaler\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression,LogisticRegressionCV,SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D,Input\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "# Importing necssary modules\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "from collections import Counter\n",
    "# Custom script \n",
    "#%matplotlibe inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e873dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib2\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import preprocessing\n",
    "from random import seed, shuffle\n",
    "\n",
    "# import utils as ut\n",
    "\n",
    "SEED = 1234\n",
    "seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "def load_compas():\n",
    "    FEATURES_CLASSIFICATION = [\"age_cat\", \"race\", \"sex\", \"priors_count\", \"c_charge_degree\"] #features to be used for classification\n",
    "    CONT_VARIABLES = [\"priors_count\"] # continuous features, will need to be handled separately from categorical features, categorical features will be encoded using one-hot\n",
    "    CLASS_FEATURE = \"two_year_recid\" # the decision variable\n",
    "    SENSITIVE_ATTRS = [\"race\"]\n",
    "    sa = \"race\"\n",
    "\n",
    "    # COMPAS_INPUT_FILE = \"compas-scores-two-years.csv\"\n",
    "    COMPAS_INPUT_FILE = \"./datasets/compas-scores-two-years.csv\"\n",
    "    # check_data_file(COMPAS_INPUT_FILE)\n",
    "\n",
    "    # load the data and get some stats\n",
    "    df = pd.read_csv(COMPAS_INPUT_FILE)\n",
    "    df = df.dropna(subset=[\"days_b_screening_arrest\"]) # dropping missing vals\n",
    "    \n",
    "    # convert to np array\n",
    "    data = df.to_dict('list')\n",
    "    for k in data.keys():\n",
    "        data[k] = np.array(data[k])\n",
    "        \n",
    "    \"\"\" Filtering the data \"\"\"\n",
    "\n",
    "\n",
    "    # These filters are the same as propublica (refer to https://github.com/propublica/compas-analysis)\n",
    "\n",
    "    # If the charge date of a defendants Compas scored crime was not within 30 days from when the person was arrested, we assume that because of data quality reasons, that we do not have the right offense. \n",
    "\n",
    "    idx = np.logical_and(data[\"days_b_screening_arrest\"]<=30, data[\"days_b_screening_arrest\"]>=-30)\n",
    "\n",
    "\n",
    "\n",
    "    # We coded the recidivist flag -- is_recid -- to be -1 if we could not find a compas case at all.\n",
    "\n",
    "    idx = np.logical_and(idx, data[\"is_recid\"] != -1)\n",
    "\n",
    "\n",
    "    # In a similar vein, ordinary traffic offenses -- those with a c_charge_degree of 'O' -- will not result in Jail time are removed (only two of them).\n",
    "\n",
    "    idx = np.logical_and(idx, data[\"c_charge_degree\"] != \"O\") # F: felony, M: misconduct\n",
    "\n",
    "\n",
    "    # We filtered the underlying data from Broward county to include only those rows representing people who had either recidivated in two years, or had at least two years outside of a correctional facility.\n",
    "\n",
    "    idx = np.logical_and(idx, data[\"score_text\"] != \"NA\")\n",
    "\n",
    "\n",
    "    # we will only consider blacks and whites for this analysis\n",
    "\n",
    "    idx = np.logical_and(idx, np.logical_or(data[\"race\"] == \"African-American\", data[\"race\"] == \"Caucasian\"))\n",
    "\n",
    "\n",
    "    # select the examples that satisfy this criteria\n",
    "    for k in data.keys():\n",
    "        data[k] = data[k][idx]\n",
    "        \n",
    "    print (collections.Counter(data[sa]))\n",
    "\n",
    "    test = pd.DataFrame.from_dict(data)\n",
    "\n",
    "    # print test\n",
    "\n",
    "    \"\"\" Feature normalization and one hot encoding \"\"\"\n",
    "\n",
    "\n",
    "    # convert class label 0 to -1\n",
    "\n",
    "    y = data[CLASS_FEATURE]\n",
    "\n",
    "    y[y==0] = -1\n",
    "\n",
    "\n",
    "    X = np.array([]).reshape(len(y), 0) # empty array with num rows same as num examples, will hstack the features to it\n",
    "\n",
    "    x_control = defaultdict(list)\n",
    "\n",
    "\n",
    "    feature_names = []\n",
    "\n",
    "    index = -1\n",
    "\n",
    "    saIndex = 0\n",
    "\n",
    "    for attr in FEATURES_CLASSIFICATION:\n",
    "        index +=1\n",
    "\n",
    "        vals = data[attr]\n",
    "        if attr in CONT_VARIABLES:\n",
    "            vals = [float(v) for v in vals]\n",
    "            vals = preprocessing.scale(vals) # 0 mean and 1 variance  \n",
    "            vals = np.reshape(vals, (len(y), -1)) # convert from 1-d arr to a 2-d arr with one col\n",
    "\n",
    "        else: # for binary categorical variables, the label binarizer uses just one var instead of two\n",
    "            lb = preprocessing.LabelBinarizer()\n",
    "            lb.fit(vals)\n",
    "            vals = lb.transform(vals)\n",
    "            if attr == 'race':\n",
    "                print(lb.classes_)\n",
    "                print(lb.transform(lb.classes_))\n",
    "        # add to sensitive features dict\n",
    "        if attr in SENSITIVE_ATTRS:\n",
    "            x_control[attr] = vals\n",
    "\n",
    "\n",
    "        # add to learnable features\n",
    "        X = np.hstack((X, vals))\n",
    "\n",
    "        if attr in CONT_VARIABLES: # continuous feature, just append the name\n",
    "            feature_names.append(attr)\n",
    "        else: # categorical features\n",
    "            if vals.shape[1] == 1: # binary features that passed through lib binarizer\n",
    "                feature_names.append(attr)\n",
    "            else:\n",
    "                for k in lb.classes_: # non-binary categorical features, need to add the names for each cat\n",
    "                    feature_names.append(attr + \"_\" + str(k))\n",
    "\n",
    "\n",
    "    # convert the sensitive feature to 1-d array\n",
    "    x_control = dict(x_control)\n",
    "    for k in x_control.keys():\n",
    "        assert(x_control[k].shape[1] == 1) # make sure that the sensitive feature is binary after one hot encoding\n",
    "        x_control[k] = np.array(x_control[k]).flatten()\n",
    "\n",
    "    # sys.exit(1)\n",
    "\n",
    "    \"\"\"permute the date randomly\"\"\"\n",
    "    # perm = range(0,X.shape[0])\n",
    "    # shuffle(perm)\n",
    "    # X = X[perm]\n",
    "    # y = y[perm]\n",
    "    # for k in x_control.keys():\n",
    "    # \tx_control[k] = x_control[k][perm]\n",
    "    feature_names.append('target')\n",
    "    print (\"Features we will be using for classification are:\", feature_names, \"\\n\")\n",
    "\n",
    "    # pd.DataFrame(np.c_[X, y]).to_csv(\"test_compas_X.csv\", header=feature_names)\n",
    "    # print np.sum(X[:,feature_names.index(SENSITIVE_ATTRS[0])])\n",
    "\n",
    "    return X, y, feature_names.index(SENSITIVE_ATTRS[0]), 1, x_control\n",
    "\n",
    "X,y, sa_index, p_Group, x_control= load_compas()\n",
    "print(X)\n",
    "#print(X[0])\n",
    "#print(X[0][1])\n",
    "#print(sa_index)\n",
    "np_Group = 0 #non-protected group's sa_value\n",
    "Y = []\n",
    "for i in y:\n",
    "    if (i == -1):\n",
    "        Y.append(0)\n",
    "    else:\n",
    "        Y.append(1)\n",
    "Y = np.array(Y)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.2)\n",
    "Xtr = x_train\n",
    "Xte = x_test\n",
    "Ytr = y_train\n",
    "Yte = y_test\n",
    "x_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7ffedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_clients(image_list, label_list, num_clients, initial='clients'):\n",
    "    ''' return: a dictionary with keys clients' names and value as \n",
    "                data shards - tuple of images and label lists.\n",
    "        args: \n",
    "            image_list: a list of numpy arrays of training images\n",
    "            label_list:a list of binarized labels for each image\n",
    "            num_client: number of fedrated members (clients)\n",
    "            initials: the clients'name prefix, e.g, clients_1 \n",
    "            \n",
    "    '''\n",
    "\n",
    "    #create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
    "\n",
    "    #randomize the data\n",
    "    data = list(zip(image_list, label_list))\n",
    "    random.shuffle(data)\n",
    "\n",
    "    #shard data and place at each client\n",
    "    size = len(data)//num_clients\n",
    "    shards = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
    "\n",
    "    #number of clients must equal number of shards\n",
    "    assert(len(shards) == len(client_names))\n",
    "\n",
    "    return {client_names[i] : shards[i] for i in range(len(client_names))} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc221ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clients = create_clients(Xtr, Ytr, num_clients=10, initial='client')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8b5fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test data for each client\n",
    "clients_test_data = create_clients(Xte, Yte, num_clients=3, initial='client')\n",
    "clients = create_clients(Xtr, Ytr, num_clients=3, initial='client')\n",
    "def batch_data(data_shard, bs=30):\n",
    "    '''Takes in a clients data shard and create a tfds object off it\n",
    "    args:\n",
    "        shard: a data, label constituting a client's data shard\n",
    "        bs:batch size\n",
    "    return:\n",
    "        tfds object'''\n",
    "    #seperate shard into data and labels lists\n",
    "    data, label = zip(*data_shard)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
    "    return dataset.shuffle(len(label)).batch(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e237e7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch clients test data\n",
    "clients_test_data_batched = dict()\n",
    "for (client_name, data) in clients_test_data.items():\n",
    "    clients_test_data_batched[client_name] = batch_data(data)\n",
    "    \n",
    "clients_batched = dict()\n",
    "for (client_name, data) in clients.items():\n",
    "    clients_batched[client_name] = batch_data(data)\n",
    "#process and batch the test set  \n",
    "test_batched = tf.data.Dataset.from_tensor_slices((Xte, Yte)).batch(len(Yte))\n",
    "#print('Number of client datasets: {l}'.format(l=len(test_batched)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94cf90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client_names = list(clients_batched.keys())\n",
    "bs = list(clients_batched[client_name])[0][0].shape[0] \n",
    "#first calculate the total training data points across clinets\n",
    "global_count = sum([tf.data.experimental.cardinality(clients_batched[client_name]).numpy() for client_name in client_names])*bs\n",
    "local_count = tf.data.experimental.cardinality(clients_batched['client_1']).numpy()*bs\n",
    "print(global_count) \n",
    "print(local_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56ee0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of client datasets: {l}'.format(l=len(clients_batched)))\n",
    "print('First dataset: {d}'.format(d=clients_batched['client_1']))\n",
    "# check length of number of test datasets created for clients\n",
    "print('Number of client test datasets: {l}'.format(l=len(clients_test_data_batched)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fd2115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,BatchNormalization,InputLayer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "class SimpleMLP:\n",
    "    @staticmethod\n",
    "    def build(x_train,n):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(InputLayer(input_shape=(x_train.shape[1],)))\n",
    "        model.add(Dense(x_train.shape[1],activation='relu'))#,input_shape=(x_train.shape[1],)))\n",
    "        #model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(n*x_train.shape[1],activation='relu'))\n",
    "        #model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(n*x_train.shape[1],activation='relu'))\n",
    "\n",
    "        model.add(Dense(1,activation='sigmoid'))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1a8baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001 \n",
    "comms_round = 50\n",
    "loss='binary_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "optimizer = Adam(learning_rate=lr)\n",
    "'''\n",
    "optimizer = SGD(lr=lr, \n",
    "                decay=lr / comms_round, \n",
    "                momentum=0.9\n",
    "               )  \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34e6f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sa_index)\n",
    "def weight_scalling_factor(clients_trn_data, client_name):\n",
    "    client_names = list(clients_trn_data.keys())\n",
    "    #get the bs\n",
    "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
    "    #first calculate the total training data points across clinets\n",
    "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n",
    "    # get the total number of data points held by a client\n",
    "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n",
    "    return local_count/global_count\n",
    "\n",
    "\n",
    "def scale_model_weights(weight, scalar):\n",
    "    '''function for scaling a models weights'''\n",
    "    weight_final = []\n",
    "    steps = len(weight)\n",
    "    for i in range(steps):\n",
    "        weight_final.append(scalar * weight[i])\n",
    "    return weight_final\n",
    "\n",
    "\n",
    "\n",
    "def sum_scaled_weights(scaled_weight_list):\n",
    "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
    "    avg_grad = list()\n",
    "    #get the average grad accross all client gradients\n",
    "    for grad_list_tuple in zip(*scaled_weight_list):\n",
    "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "        \n",
    "    return avg_grad\n",
    "\n",
    "def find_statistical_parity_score(data,labels,predictions,pp_Group=p_Group,npp_Group=np_Group):\n",
    "    protected_pos = 0.\n",
    "    protected_neg = 0.\n",
    "    non_protected_pos = 0.\n",
    "    non_protected_neg = 0.\n",
    "\n",
    "    tp_protected = 0.\n",
    "    tn_protected = 0.\n",
    "    fp_protected = 0.\n",
    "    fn_protected = 0.\n",
    "\n",
    "    tp_non_protected = 0.\n",
    "    tn_non_protected = 0.\n",
    "    fp_non_protected = 0.\n",
    "    fn_non_protected = 0.\n",
    "    \n",
    "    saIndex = sa_index\n",
    "    saValue = pp_Group\n",
    "    \n",
    "    for idx, val in enumerate(data):\n",
    "        # protected population\n",
    "        if val[saIndex] == saValue:\n",
    "            if predictions[idx] == 1:\n",
    "                protected_pos += 1.\n",
    "            else:\n",
    "                protected_neg += 1.\n",
    "            # correctly classified\n",
    "            '''\n",
    "            if labels[idx] == predictions[idx]:\n",
    "                if labels[idx] == 1:\n",
    "                    tp_protected += 1.\n",
    "                else:\n",
    "                    tn_protected += 1.\n",
    "            # misclassified\n",
    "            else:\n",
    "                if labels[idx] == 1:\n",
    "                    fn_protected += 1.\n",
    "                else:\n",
    "                    fp_protected += 1.\n",
    "            '''\n",
    "        else:\n",
    "            if predictions[idx] == 1:\n",
    "                non_protected_pos += 1.\n",
    "            else:\n",
    "                non_protected_neg += 1.\n",
    "            '''\n",
    "            # correctly classified\n",
    "            if labels[idx] == predictions[idx]:\n",
    "                if labels[idx] == 1:\n",
    "                    tp_non_protected += 1.\n",
    "                else:\n",
    "                    tn_non_protected += 1.\n",
    "            # misclassified\n",
    "            else:\n",
    "                if labels[idx] == 1:\n",
    "                    fn_non_protected += 1.\n",
    "                else:\n",
    "                    fp_non_protected += 1.\n",
    "            '''\n",
    "    '''\n",
    "    tpr_protected = tp_protected / (tp_protected + fn_protected)\n",
    "    tnr_protected = tn_protected / (tn_protected + fp_protected)\n",
    "\n",
    "    tpr_non_protected = tp_non_protected / (tp_non_protected + fn_non_protected)\n",
    "    tnr_non_protected = tn_non_protected / (tn_non_protected + fp_non_protected)\n",
    "    '''\n",
    "    C_prot = (protected_pos) / (protected_pos + protected_neg)\n",
    "    C_non_prot = (non_protected_pos) / (non_protected_pos + non_protected_neg)\n",
    "\n",
    "    stat_par = C_non_prot - C_prot\n",
    "    '''\n",
    "    print(\"protected_pos: %s\" %protected_pos)\n",
    "    print(\"protected_neg: %s\" %protected_neg)\n",
    "    print(\"non-protected_pos: %s\" %non_protected_pos)\n",
    "    print(\"non-protected_neg: %s\" %non_protected_neg)\n",
    "    '''\n",
    "    return stat_par\n",
    "    \n",
    "def find_eqop_score(data,labels,predictions, pp_Group=p_Group,npp_Group=np_Group):\n",
    "    protected_pos = 0.\n",
    "    protected_neg = 0.\n",
    "    non_protected_pos = 0.\n",
    "    non_protected_neg = 0.\n",
    "\n",
    "    tp_protected = 0.\n",
    "    tn_protected = 0.\n",
    "    fp_protected = 0.\n",
    "    fn_protected = 0.\n",
    "\n",
    "    tp_non_protected = 0.\n",
    "    tn_non_protected = 0.\n",
    "    fp_non_protected = 0.\n",
    "    fn_non_protected = 0.\n",
    "    saIndex = sa_index\n",
    "    saValue = pp_Group\n",
    "    for idx, val in enumerate(data):\n",
    "        # protrcted population\n",
    "        if val[saIndex] == saValue:\n",
    "            '''\n",
    "            if predictions[idx] == 1:\n",
    "                protected_pos += 1.\n",
    "            else:\n",
    "                protected_neg += 1.\n",
    "            '''\n",
    "            # correctly classified\n",
    "            if labels[idx] == predictions[idx]:\n",
    "                if labels[idx] == 1:\n",
    "                    tp_protected += 1.\n",
    "                #else:\n",
    "                #    tn_protected += 1.\n",
    "            # misclassified\n",
    "            else:\n",
    "                if labels[idx] == 1:\n",
    "                    fn_protected += 1.\n",
    "                #else:\n",
    "                #    fp_protected += 1.\n",
    "\n",
    "        else:\n",
    "            '''\n",
    "            if predictions[idx] == 1:\n",
    "                non_protected_pos += 1.\n",
    "            else:\n",
    "                non_protected_neg += 1.\n",
    "            '''\n",
    "            # correctly classified\n",
    "            if labels[idx] == predictions[idx]:\n",
    "                if labels[idx] == 1:\n",
    "                    tp_non_protected += 1.\n",
    "                #else:\n",
    "                #    tn_non_protected += 1.\n",
    "            # misclassified\n",
    "            else:\n",
    "                if labels[idx] == 1:\n",
    "                    fn_non_protected += 1.\n",
    "                #else:\n",
    "                #    fp_non_protected += 1.\n",
    "\n",
    "    tpr_protected = tp_protected / (tp_protected + fn_protected)\n",
    "    #tnr_protected = tn_protected / (tn_protected + fp_protected)\n",
    "\n",
    "    tpr_non_protected = tp_non_protected / (tp_non_protected + fn_non_protected)\n",
    "    #tnr_non_protected = tn_non_protected / (tn_non_protected + fp_non_protected)\n",
    "\n",
    "    \n",
    "    eqop = tpr_non_protected - tpr_protected\n",
    "    return eqop\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "def test_client_model(X_test, Y_test,  model,pp_Group=p_Group,npp_Group=np_Group):\n",
    "    \n",
    "    cce = tf.keras.losses.BinaryCrossentropy()\n",
    "    logits = model.predict(X_test)\n",
    "    preidect = np.around(logits)\n",
    "    preidect = np.nan_to_num(preidect)\n",
    "    Y_test = np.nan_to_num(Y_test)\n",
    "    conf = (confusion_matrix(Y_test,preidect)) \n",
    "    TN = conf[0][0]\n",
    "    FP = conf[0][1]\n",
    "    FN = conf[1][0]\n",
    "    TP = conf[1][1]\n",
    "    sensitivity = TP/(TP+FN) \n",
    "    specificity = TN/(FP+TN)\n",
    "        \n",
    "    BalanceACC = (sensitivity+specificity)/2\n",
    "    stat_parity = find_statistical_parity_score(X_test,Y_test,preidect, pp_Group, npp_Group)\n",
    "    assigned_positive_labels = 0\n",
    "    total_positive_labels = 0\n",
    "    \n",
    "    \n",
    "    unique, counts = np.unique(preidect, return_counts=True)\n",
    "    count_ap_dict = dict(zip(unique, counts))\n",
    "    assigned_positive_labels = count_ap_dict.get(1,0)\n",
    "    \n",
    "    unique, counts = np.unique(Y_test, return_counts=True)\n",
    "    count_tp_dict = dict(zip(unique, counts))\n",
    "    total_positive_labels = count_tp_dict.get(1,0)\n",
    "    \n",
    "    '''\n",
    "    for i in range(len(preidect)):\n",
    "        if preidect[i]==1:\n",
    "            assigned_positive_labels+=1\n",
    "        if Y_test[i]==1:\n",
    "            total_positive_labels+=1\n",
    "    '''    \n",
    "    eqop = find_eqop_score(X_test,Y_test,preidect, pp_Group,npp_Group) \n",
    "    print('eqop: {}'.format(eqop))\n",
    "    return eqop, assigned_positive_labels, total_positive_labels, BalanceACC\n",
    "    #print('stat_parity: {}'.format(stat_parity))\n",
    "    \n",
    "    #return stat_parity, assigned_positive_labels, total_positive_labels, BalanceACC\n",
    "\n",
    "\n",
    "def find_class_Weight(labels,majority_label,minority_label):\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    count_ap_dict = dict(zip(unique, counts))\n",
    "    \n",
    "    majority_class_weight = 1\n",
    "    minority_class_weight = count_ap_dict.get(majority_label,0)/count_ap_dict.get(minority_label,1)\n",
    "    class_weights={majority_label:1,minority_label:minority_class_weight}\n",
    "    \n",
    "    return class_weights\n",
    "\n",
    "def test_model(X_test, Y_test,  model, comm_round):\n",
    "    cce = tf.keras.losses.BinaryCrossentropy()\n",
    "    logits = model.predict(X_test)\n",
    "    preidect = np.around(logits)\n",
    "    preidect = np.nan_to_num(preidect)\n",
    "    Y_test = np.nan_to_num(Y_test)\n",
    "    stat_parity = find_statistical_parity_score(X_test,Y_test,preidect)\n",
    "    eqop = find_eqop_score(X_test,Y_test,preidect)    \n",
    "        \n",
    "    conf = (confusion_matrix(Y_test,preidect))   \n",
    "    loss = cce(Y_test, preidect)\n",
    "    acc = accuracy_score(preidect,Y_test)\n",
    "    print('comm_round: {} | global_acc: {} | global_loss: {}'.format(comm_round, acc, loss))\n",
    "    return acc, loss,conf,stat_parity, eqop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d648153",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# bulid K-SMOTE algorithem\n",
    "# input (dmin, dmaj, K=number of data sample from dmin to upsampling, r=ratio of samples to reate in compare to dmaj)\n",
    "# output(synthatic data for a client)\n",
    "import random\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "\n",
    "def k_nearest_neighbors(data, predict, k):\n",
    "    #k=8\n",
    "    #if len(data) >= k:\n",
    "    #    warnings.warn('K is set to a value less than total voting groups!')\n",
    "\n",
    "    distances = []\n",
    "    count = 0\n",
    "    for sample in data:\n",
    "        euclidean_distance = np.linalg.norm(np.array(sample)-np.array(predict))\n",
    "        distances.append([euclidean_distance,count])\n",
    "        count+=1\n",
    "    \n",
    "    votes = [i[1] for i in sorted(distances)[:k]] ##votes is returning indexes of k random samples\n",
    "\n",
    "    #vote_result = Counter(votes).most_common(9)[0][0]\n",
    "    return votes\n",
    "\n",
    "\n",
    "def fair_k_nearest_neighbors(data, predict, k):\n",
    "    #k=8\n",
    "    #if len(data) >= k:\n",
    "    #    warnings.warn('K is set to a value less than total voting groups!')\n",
    "\n",
    "    distances = []\n",
    "    count = 0\n",
    "    for sample in data:\n",
    "        euclidean_distance = np.linalg.norm(np.array(sample)-np.array(predict))\n",
    "        distances.append([euclidean_distance,count])\n",
    "        count+=1\n",
    "    \n",
    "    votes = [i[1] for i in sorted(distances)[:k]] ##votes is returning indexes of k random samples\n",
    "\n",
    "    #vote_result = Counter(votes).most_common(9)[0][0]\n",
    "    return votes\n",
    "\n",
    "##algo 2:\n",
    "\n",
    "def fair_kSMOTE_algo_2(dmajor,dminor,k,r):\n",
    "    S = []\n",
    "    Ns =  int(r*(len(dmajor)))\n",
    "    \n",
    "    Nks = int(Ns / (k-1))\n",
    "    difference = Ns-Nks*(k-1)\n",
    "    \n",
    "    rb = []\n",
    "    #pick a random k sample from dmin and save them in rb\n",
    "    dmin_rand = random.sample(dminor, k-1)   \n",
    "    #for debugging\n",
    "    sens_attr_vals = []\n",
    "    i = 0\n",
    "    \n",
    "    #do algorithem (choose the nearest neighbor and linear interpolation)\n",
    "    for xb in dmin_rand:\n",
    "        N= k_nearest_neighbors(dminor,xb,k) #from minority-p\n",
    "        \n",
    "        #do linear interpolation\n",
    "        Sxb = []\n",
    "        \n",
    "        if i==0:\n",
    "            Nkss = Nks+difference\n",
    "        else:\n",
    "            Nkss = Nks\n",
    "        \n",
    "        i+=1\n",
    "        for s in range(Nkss):\n",
    "            \n",
    "            \n",
    "            j = 1 \n",
    "            #randome k sample\n",
    "            #j = random.randint(0, len(N))\n",
    "            \n",
    "            ##here nearst neghber insted of dminor\n",
    "            #linear interpolation\n",
    "            x_new = ((dminor[N[j]]-xb) * random.sample(range(0, 1), 1))\n",
    "            j+=1\n",
    "            \n",
    "            while(j < len(N)):\n",
    "                #here on random xb\n",
    "                ind = N[j]\n",
    "                x_new = x_new + ((dminor[ind]-xb) * random.sample(range(0, 1), 1))\n",
    "                j += 1\n",
    "                \n",
    "            x_new = x_new / (len(N)-1) \n",
    "            Synthesized_instance = xb + x_new \n",
    "            \n",
    "            \n",
    "            #for algo 3 when finding nearest neighbors from min_np and assigning the \n",
    "            #'p' sensitive value to all synthesized instances\n",
    "            Synthesized_instance[sa_index] = xb[sa_index] \n",
    "            Sxb.append(Synthesized_instance)\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        S.append(Sxb)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return S\n",
    "\n",
    "\n",
    "\n",
    "###algo 3, algo 3 x_synthesized as a mixture of nearest neighbors from dmin-p and dmin-np\n",
    "\n",
    "def fair_kSMOTE(dmajor,dminor_wg,dminor,k,r):\n",
    "    S = []\n",
    "    #Ns =  int(r*(len(dmajor) - len(dminor)))\n",
    "    Ns =  int(r*(len(dmajor)))\n",
    "    \n",
    "    \n",
    "    Nks = int(Ns / (k-1))\n",
    "    difference = Ns-Nks*(k-1)\n",
    "    #if r==-1:\n",
    "    #    Nks = 1\n",
    "    rb = []\n",
    "    #pick a random k sample from dmin and save them in rb\n",
    "    dmin_rand = random.sample(dminor, k-1)   \n",
    "    #for debugging\n",
    "    sens_attr_vals = []\n",
    "    \n",
    "    \n",
    "    #do algorithem (choose the nearest neighbor and linear interpolation)\n",
    "    i = 0\n",
    "    \n",
    "    for xb in dmin_rand:\n",
    "        N= k_nearest_neighbors(dminor,xb,int(k/2)+1) #from minority-p\n",
    "        N2= k_nearest_neighbors(dminor_wg,xb,int(k/2)) #from minority-np\n",
    "    \n",
    "        N3 = np.hstack((N, N2))\n",
    "        if i==0:\n",
    "            Nkss = Nks+difference\n",
    "        else:\n",
    "            Nkss = Nks\n",
    "        \n",
    "        i+=1\n",
    "        #do linear interpolation\n",
    "        Sxb = []\n",
    "        \n",
    "        for s in range(Nkss):\n",
    "            \n",
    "            j = 1  \n",
    "            #randome k sample\n",
    "            #j = random.randint(0, len(N))\n",
    "            \n",
    "            ##here nearst neghber insted of dminor\n",
    "            #linear interpolation\n",
    "            x_new = ((dminor[N[j]]-xb) * random.sample(range(0, 1), 1))\n",
    "            j+=1\n",
    "            \n",
    "            while(j < len(N)):\n",
    "                #here on random xb\n",
    "                ind = N[j]\n",
    "                \n",
    "                x_new = x_new + ((dminor[ind]-xb) * random.sample(range(0, 1), 1))\n",
    "                j += 1\n",
    "            j = 0\n",
    "            while(j < len(N2)):\n",
    "                #here on random xb\n",
    "                ind = N2[j]\n",
    "                \n",
    "                x_new = x_new + ((dminor_wg[ind]-xb) * random.sample(range(0, 1), 1))\n",
    "                j += 1    \n",
    "            x_new = x_new / (len(N3)-1) \n",
    "            Synthesized_instance = xb + x_new \n",
    "            \n",
    "            \n",
    "            #for algo 3 when finding nearest neighbors from min_np and assigning the \n",
    "            #'p' sensitive value to all synthesized instances\n",
    "            Synthesized_instance[sa_index] = xb[sa_index] \n",
    "            Sxb.append(Synthesized_instance)\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        S.append(Sxb)\n",
    "    \n",
    "    \n",
    "   \n",
    "    return S\n",
    "\n",
    "\n",
    "def kSMOTE(dmajor,dminor,k,r):\n",
    "    S = []\n",
    "    Ns = int(r * (len(dmajor) - len(dminor)))\n",
    "    Nks = int(Ns / k)\n",
    "    rb = []\n",
    "    #pick a random k sample from dmin and save them in rb\n",
    "    dmin_rand = random.sample(dminor, k)   \n",
    "    #for debugging\n",
    "    sens_attr_vals = []\n",
    "    \n",
    "    \n",
    "    #do algorithem (choose the nearest neighbor and linear interpolation)\n",
    "    for xb in dmin_rand:\n",
    "        N= k_nearest_neighbors(dminor,xb,k)\n",
    "        \n",
    "        \n",
    "        #do linear interpolation\n",
    "        Sxb = []\n",
    "        for s in range(Nks):\n",
    "            j = 1  \n",
    "            #randome k sample\n",
    "            #j = random.randint(0, len(N))\n",
    "            \n",
    "            ##here nearst neghber insted of dminor\n",
    "            #linear interpolation\n",
    "            x_new = ((dminor[N[j]]-xb) * random.sample(range(0, 1), 1))\n",
    "            j+=1\n",
    "            \n",
    "            while(j < len(N)):\n",
    "                #here on random xb\n",
    "                ind = N[j]\n",
    "                x_new = x_new + ((dminor[ind]-xb) * random.sample(range(0, 1), 1))\n",
    "                j += 1\n",
    "                \n",
    "            x_new = x_new / len(N) #\n",
    "            Synthesized_instance = xb + x_new \n",
    "            \n",
    "            #for debugging\n",
    "            sens_attr_vals.append(Synthesized_instance[10])\n",
    "            \n",
    "            #Synthesized_instance[sa_index] = xb[sa_index] ##Smote is synthesizing the values as if they are numbers but sensitive attribute \n",
    "                                                          ## is binary so we need to keep it binary\n",
    "            Sxb.append(Synthesized_instance)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        S.append(Sxb)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return S\n",
    "\n",
    "def splitYtrain_smotenc(Xtr,Ytr,minority_lable):\n",
    "  \n",
    "    dmaj_x = []\n",
    "    dmin_x = []\n",
    "    label_maj_x = []\n",
    "    label_min_x = []\n",
    "    \n",
    "    for i in range(len(Ytr)):\n",
    "        if((Ytr[i]) == minority_lable):\n",
    "            dmin_x.append(Xtr[i])\n",
    "            label_min_x.append(Ytr[i])\n",
    "        else:\n",
    "            dmaj_x.append(Xtr[i])\n",
    "            label_maj_x.append(Ytr[i])\n",
    "    \n",
    "    return dmaj_x,dmin_x, label_min_x, label_maj_x\n",
    "def splitYtrain(Xtr,Ytr,minority_lable):\n",
    "    #print(Ytr)\n",
    "    dmaj_x = []\n",
    "    dmin_x = []\n",
    "    \n",
    "    \n",
    "    for i in range(len(Ytr)):\n",
    "        if((Ytr[i]) == minority_lable):\n",
    "            dmin_x.append(Xtr[i])\n",
    "            \n",
    "        else:\n",
    "            dmaj_x.append(Xtr[i])\n",
    "            \n",
    "    \n",
    "    return dmaj_x,dmin_x\n",
    "def splitYtrain_sa_value(Xtr,Ytr,minority_lable,majority_label,pp_Group=p_Group,npp_Group=np_Group): #splite Ytrain based on sensitive attribute value\n",
    "    #print(Ytr)\n",
    "    dmaj_p_x = []\n",
    "    dmaj_np_x = []\n",
    "    dmin_p_x = []\n",
    "    dmin_np_x = []\n",
    "    \n",
    "    \n",
    "    for i in range(len(Ytr)):\n",
    "        if((Ytr[i]) == minority_lable and (Xtr[i][sa_index])==pp_Group): #select minority instances with \"protected\" value \n",
    "            dmin_p_x.append(Xtr[i])\n",
    "        elif((Ytr[i]) == minority_lable and (Xtr[i][sa_index])==npp_Group): #select minority instances with \"protected\" value \n",
    "            dmin_np_x.append(Xtr[i])\n",
    "        elif ((Ytr[i]) == majority_label and (Xtr[i][sa_index])==pp_Group): #select minority(positive class) instances with \"non-protected\" value\n",
    "            dmaj_p_x.append(Xtr[i])\n",
    "        elif ((Ytr[i]) == majority_label and (Xtr[i][sa_index])==npp_Group): #select minority(positive class) instances with \"non-protected\" value\n",
    "            dmaj_np_x.append(Xtr[i])\n",
    "    \n",
    "    return dmin_p_x, dmin_np_x, dmaj_p_x, dmaj_np_x\n",
    "def get_statistics(Xtr,Ytr,minority_lable,majority_label): #splite Ytrain based on sensitive attribute value\n",
    "    #print(Ytr)\n",
    "    dmaj_p_x =0\n",
    "    dmaj_np_x = 0\n",
    "    dmin_p_x = 0\n",
    "    dmin_np_x = 0\n",
    "    \n",
    "    \n",
    "    for i in range(len(Ytr)):\n",
    "        if((Ytr[i]) == minority_lable and (Xtr[i][sa_index])==p_Group): #select minority instances with \"protected\" value \n",
    "            dmin_p_x+=1\n",
    "        elif((Ytr[i]) == minority_lable and (Xtr[i][sa_index])==np_Group): #select minority instances with \"protected\" value \n",
    "            dmin_np_x+=1\n",
    "        elif ((Ytr[i]) == majority_label and (Xtr[i][sa_index])==p_Group): #select minority(positive class) instances with \"non-protected\" value\n",
    "            dmaj_p_x+=1\n",
    "        elif ((Ytr[i]) == majority_label and (Xtr[i][sa_index])==np_Group): #select minority(positive class) instances with \"non-protected\" value\n",
    "            dmaj_np_x+=1\n",
    "    \n",
    "    return dmin_p_x, dmin_np_x, dmaj_p_x, dmaj_np_x\n",
    "def create_synth_data(clinet_traning_x, clinet_traning_y, minority_lable,majority_label,k,r,group,pp_group=p_Group,npp_group=np_Group):\n",
    "    \n",
    "    \n",
    "    \n",
    "    #create two data set from traning data (one for maj (ex.0 class) and one for min(ex.1 class)) \n",
    "    #for simple federated learning\n",
    "    dmaj_client,dmin_client = splitYtrain(clinet_traning_x,clinet_traning_y,minority_lable)\n",
    "    \n",
    "    #for fair federated learning\n",
    "    \n",
    "    \n",
    "    dmin_p_x, dmin_np_x, dmaj_p_x, dmaj_np_x = splitYtrain_sa_value(clinet_traning_x,clinet_traning_y,minority_lable,majority_label, pp_group,npp_group)\n",
    "    \n",
    "    group_names = ['dmin_p_x', 'dmin_np_x', 'dmaj_p_x', 'dmaj_np_x']\n",
    "    group_label_dict = {'dmin_p_x':minority_label, 'dmin_np_x': minority_label, 'dmaj_p_x':  majority_label, 'dmaj_np_x': majority_label}\n",
    "    group_dict = {'dmin_p_x':dmin_p_x, 'dmin_np_x': dmin_np_x, 'dmaj_p_x':  dmaj_p_x, 'dmaj_np_x': dmaj_np_x}\n",
    "    \n",
    "    group_lengths = [len(dmin_p_x),len(dmin_np_x), len(dmaj_p_x), len(dmaj_np_x)] \n",
    "    \n",
    "    #group with maximum length\n",
    "    max_length_group = group_lengths.index(max(group_lengths))  \n",
    "    \n",
    "    #group name which has maximum length\n",
    "    max_group_name = group_names[max_length_group]\n",
    "    \n",
    "    #find the insances in the group with maximum length and store them in dmaj_x\n",
    "    for key, value in group_dict.items():\n",
    "        if key== max_group_name:\n",
    "            dmaj_x = value\n",
    "            break\n",
    "    Xtr_new = []\n",
    "    Ytr_new = []  \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ##Algo 3:\n",
    "    \n",
    "     ##Algo 3:\n",
    "    if group =='min_p':\n",
    "        dmaj_x = dmaj_p_x\n",
    "        dmin_x = dmin_p_x\n",
    "        \n",
    "        #dmin_np = dmin_np_x\n",
    "        x_syn = fair_kSMOTE(dmaj_x,dmin_np_x,dmin_x,k,r)\n",
    "        #x_syn = fair_kSMOTE_algo_2(dmaj_x,dmin_x,k,r)\n",
    "        # add the created synthatic data to the traning data\n",
    "        # here merrge old traning data with the new synthatic data\n",
    "        new_label = minority_label\n",
    "        for j in x_syn:\n",
    "            for s in j:\n",
    "                Xtr_new.append(s)\n",
    "                Ytr_new.append(new_label)\n",
    "        \n",
    "    elif group =='maj_np':\n",
    "        dmaj_x = dmin_np_x\n",
    "        dmin_x = dmaj_np_x\n",
    "        #dmin_np = dmin_np_x\n",
    "        x_syn = fair_kSMOTE_algo_2(dmaj_x,dmin_x,k,r)\n",
    "        \n",
    "        \n",
    "        # add the created synthatic data to the traning data\n",
    "        # here merrge old traning data with the new synthatic data\n",
    "        new_label = majority_label\n",
    "        for j in x_syn:\n",
    "            for s in j:\n",
    "                Xtr_new.append(s)\n",
    "                Ytr_new.append(new_label)\n",
    "    \n",
    "    \n",
    "      \n",
    "    \n",
    "    \n",
    "    return Xtr_new,Ytr_new\n",
    "def k_nearest_neighbors_modified(data, predict, k,data_rand_index):\n",
    "    distances = []\n",
    "    count=0\n",
    "    for sample in data:\n",
    "        if count in data_rand_index:\n",
    "            count+=1\n",
    "        else:\n",
    "            euclidean_distance = np.linalg.norm(np.array(sample)-np.array(predict))\n",
    "            distances.append([euclidean_distance,count])\n",
    "            count+=1\n",
    "    votes = [i[1] for i in sorted(distances)[:k]] ##votes is returning indexes of k random samples\n",
    "    #vote_result = Counter(votes).most_common(9)[0][0]\n",
    "    return votes\n",
    "def downsample_utility_function(clinet_traning_x, clinet_traning_y,data,data_index, k,r,label,reduction_amount):\n",
    "    S = []\n",
    "    #pick a random k sample from data which we want to downsample\n",
    "    d_rand = []\n",
    "    d_rand_index = []\n",
    "    \n",
    "    #reduction_amount = int(r*len(data))  \n",
    "    #print(\"reduction amount: %s\" % reduction_amount)\n",
    "    \n",
    "    for i in range(reduction_amount):\n",
    "        index = random.randint(0, (len(data)-1))\n",
    "        #random.sample(range(0,len(data)),1)\n",
    "        \n",
    "        d_rand.append(data[index])\n",
    "        d_rand_index.append(index)\n",
    "    data_rand_index = list(d_rand_index)\n",
    "    Sxb = []   \n",
    "    sens_attr_vals = []\n",
    "    k = 3 ##number of nearest neighbours\n",
    "    indices_neighbours_removed = list(d_rand_index)\n",
    "    #print(len(d_rand_index))\n",
    "    #do algorithm (choose the nearest neighbor and linear interpolation)\n",
    "    \n",
    "    Nks=1\n",
    "    S=[]\n",
    "    #do algorithem (choose the nearest neighbor and linear interpolation)\n",
    "    for xb in d_rand:\n",
    "        #N= k_nearest_neighbors(data,xb,k)\n",
    "        N= k_nearest_neighbors_modified(data,xb,k,data_rand_index)\n",
    "        \n",
    "        #do linear interpolation\n",
    "        Sxb = []\n",
    "        for s in range(Nks):\n",
    "            j = 0 #j = 1\n",
    "            #linear interpolation\n",
    "            x_new = ((data[N[j]]-xb) * random.sample(range(0, 1), 1))\n",
    "            indices_neighbours_removed.append(N[1])\n",
    "            j+=1\n",
    "            \n",
    "            while(j < len(N)-1):\n",
    "                #here on random xb\n",
    "                ind = N[j]\n",
    "                x_new = x_new + ((data[ind]-xb) * random.sample(range(0, 1), 1))\n",
    "                j += 1\n",
    "                \n",
    "            x_new = x_new / (len(N)-1) \n",
    "            Synthesized_instance = xb + x_new \n",
    "            Synthesized_instance[sa_index] = xb[sa_index] \n",
    "            \n",
    "            Sxb.append(Synthesized_instance)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        S.append(Sxb)\n",
    "    #print(indices_neighbours_removed)\n",
    "    \n",
    "    \n",
    "    for i in range(len(indices_neighbours_removed)):\n",
    "        indx = indices_neighbours_removed[i]\n",
    "        indices_neighbours_removed[i] = data_index[indx]\n",
    "    indices = np.unique(indices_neighbours_removed)\n",
    "    indices = list(indices)\n",
    "    difference = (reduction_amount*2)-len(indices)\n",
    "    \n",
    "        \n",
    "    #for i in range(0,difference-1):\n",
    "    #    indices.pop(0)\n",
    "    #print(\"reduced amount: %s\" % len(indices))\n",
    "    \n",
    "    indices.sort()\n",
    "    \n",
    "    Xtr = []\n",
    "    Ytr = []\n",
    "    for k in clinet_traning_x:\n",
    "        Xtr.append(k)\n",
    "        \n",
    "    for k in clinet_traning_y:\n",
    "        Ytr.append(k)\n",
    "    \n",
    "    for i in range(len(indices)):\n",
    "        indx = indices[i]-i\n",
    "        Xtr.pop(indx)\n",
    "        Ytr.pop(indx)\n",
    "    \n",
    "    for j in S:\n",
    "            for s in j:\n",
    "                Xtr.append(s)\n",
    "                Ytr.append(label)\n",
    "    '''\n",
    "    indices=[]\n",
    "    for i in range(difference):\n",
    "        index = random.randint(0, (len(Xtr)-1))\n",
    "        indices.append(index)\n",
    "    indices.sort()\n",
    "    \n",
    "    for i in range(len(indices)):\n",
    "        indx = indices[i]-i\n",
    "        Xtr.pop(indx)\n",
    "        Ytr.pop(indx)\n",
    "    '''    \n",
    "    Xtr = np.array(Xtr)\n",
    "    Ytr = np.array(Ytr)\n",
    "    return Xtr, Ytr\n",
    "def splitYtrain_sa_value_index(Xtr,Ytr,minority_lable,majority_label,pp_Group=p_Group,npp_Group=np_Group): #split data based on sensitive attribute value\n",
    "    #print(Ytr)\n",
    "    dmaj_p_x = []\n",
    "    dmaj_p_index = [] ##index of maj_p instance in the main dataset\n",
    "    \n",
    "    dmaj_np_x = []\n",
    "    dmaj_np_index = [] ##index of maj_np instance in the main dataset\n",
    "    \n",
    "    dmin_p_x = []\n",
    "    dmin_p_index = [] ##index of min_p instance in the main dataset\n",
    "    \n",
    "    dmin_np_x = []\n",
    "    dmin_np_index = [] ##index of min_np instance in the main dataset\n",
    "    \n",
    "    \n",
    "    for i in range(len(Ytr)):\n",
    "        if((Ytr[i]) == minority_lable and (Xtr[i][sa_index])==pp_Group): #select minority instances with \"protected\" value \n",
    "            dmin_p_x.append(Xtr[i])\n",
    "            dmin_p_index.append(i)\n",
    "        elif((Ytr[i]) == minority_lable and (Xtr[i][sa_index])==npp_Group): #select minority instances with \"protected\" value \n",
    "            dmin_np_x.append(Xtr[i])\n",
    "            dmin_np_index.append(i)\n",
    "        elif ((Ytr[i]) == majority_label and (Xtr[i][sa_index])==pp_Group): #select minority(positive class) instances with \"non-protected\" value\n",
    "            dmaj_p_x.append(Xtr[i])\n",
    "            dmaj_p_index.append(i)\n",
    "        elif ((Ytr[i]) == majority_label and (Xtr[i][sa_index])==npp_Group): #select minority(positive class) instances with \"non-protected\" value\n",
    "            dmaj_np_x.append(Xtr[i])\n",
    "            dmaj_np_index.append(i)\n",
    "    \n",
    "    return dmin_p_x, dmin_np_x, dmaj_p_x, dmaj_np_x, dmin_p_index, dmin_np_index, dmaj_p_index, dmaj_np_index\n",
    "def splitYtrain_sa_value_index(Xtr,Ytr,minority_lable,majority_label,pp_Group=p_Group,npp_Group=np_Group): #split data based on sensitive attribute value\n",
    "    #print(Ytr)\n",
    "    dmaj_p_x = []\n",
    "    dmaj_p_index = [] ##index of maj_p instance in the main dataset\n",
    "    \n",
    "    dmaj_np_x = []\n",
    "    dmaj_np_index = [] ##index of maj_np instance in the main dataset\n",
    "    \n",
    "    dmin_p_x = []\n",
    "    dmin_p_index = [] ##index of min_p instance in the main dataset\n",
    "    \n",
    "    dmin_np_x = []\n",
    "    dmin_np_index = [] ##index of min_np instance in the main dataset\n",
    "    \n",
    "    \n",
    "    for i in range(len(Ytr)):\n",
    "        if((Ytr[i]) == minority_lable and (Xtr[i][sa_index])==pp_Group): #select minority instances with \"protected\" value \n",
    "            dmin_p_x.append(Xtr[i])\n",
    "            dmin_p_index.append(i)\n",
    "        elif((Ytr[i]) == minority_lable and (Xtr[i][sa_index])==npp_Group): #select minority instances with \"protected\" value \n",
    "            dmin_np_x.append(Xtr[i])\n",
    "            dmin_np_index.append(i)\n",
    "        elif ((Ytr[i]) == majority_label and (Xtr[i][sa_index])==pp_Group): #select minority(positive class) instances with \"non-protected\" value\n",
    "            dmaj_p_x.append(Xtr[i])\n",
    "            dmaj_p_index.append(i)\n",
    "        elif ((Ytr[i]) == majority_label and (Xtr[i][sa_index])==npp_Group): #select minority(positive class) instances with \"non-protected\" value\n",
    "            dmaj_np_x.append(Xtr[i])\n",
    "            dmaj_np_index.append(i)\n",
    "    \n",
    "    return dmin_p_x, dmin_np_x, dmaj_p_x, dmaj_np_x, dmin_p_index, dmin_np_index, dmaj_p_index, dmaj_np_index\n",
    "\n",
    "def downsample(clinet_traning_x, clinet_traning_y, minority_lable,majority_label,k,r,group,pp_group=p_Group,npp_group=np_Group):\n",
    "    \n",
    "    #group: maj_p, maj_np, min_p, min_np\n",
    "    \n",
    "    dmaj_client,dmin_client = splitYtrain(clinet_traning_x,clinet_traning_y,minority_lable)\n",
    "   \n",
    "    #for fair federated learning\n",
    "    \n",
    "    dmin_p_x, dmin_np_x, dmaj_p_x, dmaj_np_x, dmin_p_index, dmin_np_index, dmaj_p_index, dmaj_np_index = splitYtrain_sa_value_index(clinet_traning_x,clinet_traning_y,minority_lable,majority_label,pp_group,npp_group)\n",
    "    \n",
    "    ##Algo 5:\n",
    "    if group =='min_np':\n",
    "        \n",
    "        label = minority_label\n",
    "        data = dmin_np_x\n",
    "        reduction_amount = int(r*len(data))\n",
    "        #print(\"reduction amount: %s\" % reduction_amount)\n",
    "        x_small,y_small = downsample_utility_function(clinet_traning_x, clinet_traning_y,data,dmin_np_index, k,r,label,reduction_amount)\n",
    "        \n",
    "        \n",
    "        \n",
    "    elif group =='maj_p':\n",
    "        label = majority_label\n",
    "        data = dmaj_p_x\n",
    "        reduction_amount = int(r*len(data))\n",
    "        #print(\"reduction amount: %s\" % reduction_amount)\n",
    "        x_small,y_small = downsample_utility_function(clinet_traning_x, clinet_traning_y,data,dmaj_p_index, k,r,label,reduction_amount)\n",
    "    return x_small,y_small\n",
    "def create_synth_data_from_smote_nc(clinet_traning_x, clinet_traning_y, minority_lable,majority_label,k,r):\n",
    "    #create two data set from traning data (one for maj (ex.0 class) and one for min(ex.1 class)) \n",
    "    \n",
    "    #for simple federated learning\n",
    "    #dmaj_x,dmin_x = splitYtrain(clinet_traning_x,clinet_traning_y,minority_lable)\n",
    "    \n",
    "    #for fair federated learning\n",
    "    CAT_VARIABLES_INDICES = [1,2,3,4,6,7,8,10,14,15]\n",
    "    dmaj_x,dmin_x,label_min_x,label_maj_x = splitYtrain_smotenc(clinet_traning_x,clinet_traning_y,minority_lable)\n",
    "    smote_nc = SMOTENC(categorical_features=CAT_VARIABLES_INDICES, sampling_strategy = r, random_state=0)\n",
    "    X_resampled, y_resampled = smote_nc.fit_resample(clinet_traning_x, clinet_traning_y)\n",
    "    \n",
    "    return X_resampled,y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b06b366",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "smlp_global = SimpleMLP()\n",
    "num_layers_mult=1\n",
    "n=num_layers_mult\n",
    "comms_round = 50\n",
    "global_model = smlp_global.build(Xtr,n)\n",
    "sensitivity_= []\n",
    "specificity_= []\n",
    "BalanceACC_= []\n",
    "G_mean_= []\n",
    "FP_rate_= []\n",
    "FN_rate_= []\n",
    "assigned_positives = 0\n",
    "total_positives = 0\n",
    "accuracy_= []\n",
    "loss_= []\n",
    "statistical_parity_ = []\n",
    "eqop_ = []\n",
    "disc_thresh = 0.005\n",
    "lambda_initial = 0.005\n",
    "disc_tolerance = 0.1\n",
    "epsilon=1\n",
    "pp_group = p_Group\n",
    "npp_group=np_Group\n",
    "#commence global training loop\n",
    "import time\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "destination = \"./\"\n",
    "output_file = open(destination+\"log33333.txt\", \"w+\")\n",
    "start = time.time()\n",
    "end_time = []\n",
    "e=EarlyStopping(patience=5,restore_best_weights=True)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "for comm_round in range(comms_round):\n",
    "            \n",
    "    # get the global model's weights - will serve as the initial weights for all local models\n",
    "    global_weights = global_model.get_weights()\n",
    "    \n",
    "    #initial list to collect local model weights after scalling\n",
    "    scaled_local_weight_list = list()\n",
    "\n",
    "    #randomize client data - using keys\n",
    "    client_names= list(clients_batched.keys())\n",
    "    #random.shuffle(client_names)\n",
    "    \n",
    "    #loop through each client and create new local model\n",
    "    for client in client_names:   \n",
    "        smlp_local = SimpleMLP()\n",
    "        local_model = smlp_local.build(Xtr, n)\n",
    "        local_model.compile(loss=loss, \n",
    "                      optimizer=optimizer, \n",
    "                      metrics=metrics)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #set local model weight to the weight of the global model\n",
    "        local_model.set_weights(global_weights)\n",
    "        \n",
    "        #here we should get the traning data from clients and try to do ksmote and see changes\n",
    "       \n",
    "        xxte = [] \n",
    "        yte = []\n",
    "        \n",
    "        for(X_test, Y_test) in clients_batched[client]:\n",
    "            \n",
    "            i = 0\n",
    "            while (i <len(X_test)):       \n",
    "                xxte.append(X_test[i].numpy())  \n",
    "                yte.append(Y_test[i].numpy())\n",
    "                i +=1\n",
    "        print(client)\n",
    "        print(\"length of dataset: %s\" %len(xxte))\n",
    "        ##separate particular clients test data from clients_test_data_batched\n",
    "        x_test_client = []\n",
    "        y_test_client = []\n",
    "        for(X_test_real, Y_test_real) in clients_test_data_batched[client]:\n",
    "            i = 0\n",
    "            while (i <len(X_test_real)):       \n",
    "                x_test_client.append(X_test_real[i].numpy())  \n",
    "                y_test_client.append(Y_test_real[i].numpy())\n",
    "                i +=1\n",
    "        \n",
    "        x_test_client = np.array(x_test_client)\n",
    "        \n",
    "        y_test_client = np.array(y_test_client)\n",
    "        \n",
    "        #here we change k and r to see how affect our result\n",
    "        #for simple federated learning\n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "        #for fair federated learning\n",
    "        minority_label = 1\n",
    "        majority_label = 0\n",
    "        #lambda_score = 0.1 #0.1,0.15,0.2,0.25,0.3\n",
    "        print(\"bismillah\")\n",
    "        print(client)\n",
    "        \n",
    "        \n",
    "        if comm_round == 0:\n",
    "            Xtr_1_new=np.array(xxte)\n",
    "            Ytr_1_new=np.array(yte)\n",
    "              \n",
    "                    \n",
    "            Xtr_1_new = np.array(Xtr_1_new)\n",
    "            Ytr_1_new = np.array(Ytr_1_new)\n",
    "            added_points = len(Xtr_1_new) - len(xxte)\n",
    "            \n",
    "            #new batch for new data\n",
    "            #split data for validation set\n",
    "            Xtr_1_new_split,x_val,Ytr_1_new_split,y_val=train_test_split(Xtr_1_new,Ytr_1_new,test_size=0.10,shuffle=True)\n",
    "        \n",
    "            data = list(zip(Xtr_1_new_split, Ytr_1_new_split))\n",
    "            random.shuffle(data)\n",
    "            btach_data = batch_data(data, bs=30)  \n",
    "           \n",
    "            class_weights=find_class_Weight(Ytr_1_new, majority_label,minority_label)\n",
    "            local_model.fit(btach_data,validation_data=(x_val,y_val),validation_steps=1, callbacks=[e], epochs=3, verbose=0, \n",
    "                            class_weight=class_weights)\n",
    "            ## find disc score for each client\n",
    "            pp_group = p_Group\n",
    "            npp_group=np_Group\n",
    "            disc_score, assigned_positives, total_positives, balanced_accuracy_client = test_client_model(x_test_client, y_test_client,  local_model,pp_group,npp_group)\n",
    "            print(\"balanced_accuracy_client %s\" % balanced_accuracy_client)\n",
    "            if disc_score<0:\n",
    "                pp_group = np_Group\n",
    "                npp_group=p_Group\n",
    "                disc_score, assigned_positives, total_positives, balanced_accuracy_client = test_client_model(x_test_client, y_test_client,  local_model,pp_group,npp_group)\n",
    "            else:\n",
    "                pp_group = p_Group\n",
    "                npp_group=np_Group\n",
    "            trade_off = (1+epsilon**2)*((balanced_accuracy_client*(1-abs(disc_score)))/(epsilon*balanced_accuracy_client+(1-abs(disc_score))))\n",
    "            lambda_score = lambda_initial*(1+(disc_score/disc_tolerance))\n",
    "            \n",
    "        \n",
    "        if comm_round!=0:\n",
    "            pp_group = p_Group\n",
    "            npp_group=np_Group\n",
    "            disc_score, assigned_positives, total_positives, balanced_accuracy_client = test_client_model(x_test_client, y_test_client,  local_model,pp_group,npp_group)\n",
    "            if disc_score<0:\n",
    "                pp_group = np_Group\n",
    "                npp_group=p_Group\n",
    "                disc_score, assigned_positives, total_positives, balanced_accuracy_client = test_client_model(x_test_client, y_test_client,  local_model,pp_group,npp_group)\n",
    "            else:\n",
    "                pp_group = p_Group\n",
    "                npp_group=np_Group\n",
    "            trade_off = (1+epsilon**2)*((balanced_accuracy_client*(1-abs(disc_score)))/(epsilon*balanced_accuracy_client+(1-abs(disc_score))))\n",
    "        #Xtr_1_new, Ytr_1_new = xxte, yte \n",
    "        if disc_score > disc_thresh:\n",
    "            \n",
    "       \n",
    "                prev_xtr,prev_ytr=[],[]\n",
    "                prev_disc_score= []\n",
    "                prev_trade_off=[]\n",
    "               \n",
    "                lambda_score = lambda_initial*(1+(disc_score/disc_tolerance))\n",
    "                greater_disc_score = 0\n",
    "                min_disc_score =disc_score\n",
    "                max_trade_off = trade_off\n",
    "                Xtr_1_new=np.array(xxte)\n",
    "                Ytr_1_new=np.array(yte)\n",
    "                prev_xtr,prev_ytr = Xtr_1_new, Ytr_1_new\n",
    "                \n",
    "                class_weights=find_class_Weight(Ytr_1_new, majority_label,minority_label)\n",
    "                points = len(Xtr_1_new) - len(xxte)\n",
    "                itr = 0\n",
    "                \n",
    "                while disc_score > disc_thresh:\n",
    "                    \n",
    "                    output_file.write(\"\\n iteration: \" + str(itr))\n",
    "                    itr+=1\n",
    "                    closest_to_zero = min(disc_score, min_disc_score, key=abs)\n",
    "                    if trade_off>max_trade_off:\n",
    "                    #if closest_to_zero != min_disc_score:\n",
    "                        min_disc_score = disc_score\n",
    "                        max_trade_off = trade_off\n",
    "                        prev_xtr,prev_ytr = Xtr_1_new, Ytr_1_new\n",
    "                        points = len(Xtr_1_new) - len(xxte)\n",
    "                        local_model.save_weights('./checkpoints/compas/3-clients/my_checkpoint')\n",
    "                    if assigned_positives<= total_positives:\n",
    "                        \n",
    "                        #N(C+,S−) =N(C+,S−)+λ× N(C−,S−)\n",
    "                        Xtr_min_p_new,Ytr_min_p_new = create_synth_data(xxte, yte, minority_label,majority_label,5,lambda_score,'min_p',pp_group,npp_group)\n",
    "                    \n",
    "                        #N(C−,S−) =N(C−,S−)−λ× N(C−,S−)\n",
    "                        Xtr_maj_p_new,Ytr_maj_p_new = downsample(xxte, yte, minority_label,majority_label,5,lambda_score,'maj_p',pp_group,npp_group)    \n",
    "                        \n",
    "                        for k in Xtr_maj_p_new:\n",
    "                            Xtr_min_p_new.append(k)\n",
    "                        for k in Ytr_maj_p_new:\n",
    "                            Ytr_min_p_new.append(k)\n",
    "                        \n",
    "                        Xtr_1_new = np.array(Xtr_min_p_new)\n",
    "                        Ytr_1_new = np.array(Ytr_min_p_new)\n",
    "                        \n",
    "                    else:\n",
    "                        #N(C−,S+) =N(C−,S+)+λ× N(C+,S+)\n",
    "                        Xtr_maj_np_new,Ytr_maj_np_new = create_synth_data(xxte, yte, minority_label,majority_label,5,lambda_score,'maj_np',pp_group,npp_group)\n",
    "                    \n",
    "                        #N(C+,S+) =N(C+,S+)−λ× N(C+,S+)\n",
    "                        Xtr_min_np_new,Ytr_min_np_new = downsample(xxte, yte, minority_label,majority_label,5,lambda_score,'min_np',pp_group,npp_group)\n",
    "                        \n",
    "                        for k in Xtr_min_np_new:\n",
    "                            Xtr_maj_np_new.append(k)\n",
    "                        for k in Ytr_min_np_new:\n",
    "                            Ytr_maj_np_new.append(k)\n",
    "                        \n",
    "                        Xtr_1_new = np.array(Xtr_maj_np_new)\n",
    "                        Ytr_1_new = np.array(Ytr_maj_np_new)\n",
    "                        \n",
    "                    \n",
    "                    added_points = len(Xtr_1_new) - len(xxte)\n",
    "                    #new batch for new data\n",
    "                    #split data for validation set\n",
    "                    Xtr_1_new_split,x_val,Ytr_1_new_split,y_val=train_test_split(Xtr_1_new,Ytr_1_new,test_size=0.10,shuffle=True)\n",
    "                    \n",
    "                    data = list(zip(Xtr_1_new_split, Ytr_1_new_split))\n",
    "                    random.shuffle(data)\n",
    "                    btach_data = batch_data(data, bs=30)  \n",
    "                    #fit local model with client's data\n",
    "                    #create validation data and early stop\n",
    "                    \n",
    "                    local_model.fit(btach_data,validation_data=(x_val,y_val),validation_steps=1, callbacks=[e], epochs=3, verbose=0,\n",
    "                                    class_weight = class_weights)\n",
    "                    ##find disc score for each client\n",
    "                    \n",
    "                    disc_score, assigned_positives, total_positives, balanced_accuracy_client = test_client_model(x_test_client, y_test_client,  local_model,pp_group,npp_group)\n",
    "                    \n",
    "                    trade_off = (1+epsilon**2)*((balanced_accuracy_client*(1-abs(disc_score)))/(epsilon*balanced_accuracy_client+(1-abs(disc_score))))\n",
    "                    output_file.write(\"\\n Disc score: \" + str(disc_score))\n",
    "                    closest_to_zero = min(disc_score, min_disc_score, key=abs)\n",
    "                    if trade_off>max_trade_off:\n",
    "                    #if closest_to_zero != min_disc_score:\n",
    "                        \n",
    "                        min_disc_score = disc_score\n",
    "                        max_trade_off = trade_off\n",
    "                        prev_xtr,prev_ytr = Xtr_1_new, Ytr_1_new\n",
    "                        points = len(Xtr_1_new) - len(xxte)\n",
    "                        local_model.save_weights('./checkpoints/compas/3-clients/my_checkpoint')\n",
    "                    #if len(prev_disc_score)>0:\n",
    "                    if len(prev_trade_off)>0:\n",
    "                        #if disc_score>prev_disc_score[-1]:\n",
    "                        if trade_off<prev_trade_off[-1]:\n",
    "                            greater_disc_score+=1\n",
    "                            print(greater_disc_score)\n",
    "                            if greater_disc_score >2:\n",
    "                                local_model.load_weights('./checkpoints/compas/3-clients/my_checkpoint')\n",
    "                                added_points = points\n",
    "                                break\n",
    "                                #Xtr_1_new, Ytr_1_new =prev_xtr,prev_ytr\n",
    "                                #xxte, yte = Xtr_1_new, Ytr_1_new\n",
    "                                \n",
    "                                #output_file.write(\"Balanced accuracy corresponding the selected model: \" + str(min_disc_score))\n",
    "                                \n",
    "                               \n",
    "                     \n",
    "                    \n",
    "                    prev_trade_off.append(trade_off)\n",
    "                    xxte, yte = Xtr_1_new, Ytr_1_new\n",
    "                \n",
    "                xxtee=[]\n",
    "                ytee=[]\n",
    "                dataset = tf.data.Dataset.from_tensor_slices((list(prev_xtr), list(prev_ytr)))\n",
    "                up_dict = {client:batch_data(dataset)}\n",
    "                clients_batched.update(up_dict)\n",
    "                #clients_batched[client_name] = batch_data(dataset)\n",
    "                for(X_test, Y_test) in clients_batched[client]:\n",
    "            \n",
    "                    i = 0\n",
    "                    while (i <len(X_test)):       \n",
    "                        xxtee.append(X_test[i].numpy())  \n",
    "                        ytee.append(Y_test[i].numpy())\n",
    "                        i +=1\n",
    "                print(client)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "        else:\n",
    "            output_file.write(\"\\n Disc score is less than the threshold\")\n",
    "            if comm_round!=0:\n",
    "                Xtr_1_new=np.array(xxte)\n",
    "                Ytr_1_new=np.array(yte)\n",
    "                #class_weights = class_weight.compute_class_weight('balanced',\n",
    "                #                                 np.unique(Ytr_1_new),\n",
    "                #                                 Ytr_1_new)\n",
    "                #class_weights = {majority_label:1, minority_label:7}\n",
    "                class_weights=find_class_Weight(Ytr_1_new, majority_label,minority_label)\n",
    "                Xtr_1_new,x_val,Ytr_1_new,y_val=train_test_split(Xtr_1_new,Ytr_1_new,test_size=0.10,shuffle=True)\n",
    "                data = list(zip(Xtr_1_new,Ytr_1_new))\n",
    "                #random.shuffle(data)\n",
    "                added_points = 0\n",
    "                btach_data = batch_data(data, bs=30)\n",
    "                \n",
    "                local_model.fit(btach_data,validation_data=(x_val,y_val),validation_steps = 1, callbacks=[e], epochs=3, \n",
    "                                verbose=0, class_weight = class_weights)\n",
    "        \n",
    "        #scale the model weights and add to list       \n",
    "        client_names = list(clients_batched.keys())\n",
    "        bs = list(clients_batched[client_name])[0][0].shape[0]\n",
    "        #first calculate the total training data points across clinets\n",
    "        global_count = sum([tf.data.experimental.cardinality(clients_batched[client_name]).numpy() for client_name in client_names])*bs\n",
    "        global_count = global_count + (added_points)\n",
    "        # get the total number of data points held by a client\n",
    "        local_count = len(Xtr_1_new)\n",
    "        scaling_factor = local_count/global_count\n",
    "                \n",
    "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "        scaled_local_weight_list.append(scaled_weights)\n",
    "        \n",
    "        #clear session to free memory after each communication round\n",
    "        K.clear_session()\n",
    "        \n",
    "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "    #update global model \n",
    "    \n",
    "    global_model.set_weights(average_weights)\n",
    "    \n",
    "    #test global model and print out metrics after each communications round\n",
    "    for(X_test, Y_test) in test_batched:\n",
    "        global_acc, global_loss,conf,stat_parity,eqop = test_model(X_test, Y_test, global_model, comm_round)\n",
    "        print(\"x_test %s \"% len(X_test))\n",
    "        TN = conf[0][0]\n",
    "        FP = conf[0][1]\n",
    "        FN = conf[1][0]\n",
    "        TP = conf[1][1]\n",
    "        sensitivity = TP/(TP+FN) \n",
    "        specificity = TN/(FP+TN)\n",
    "        \n",
    "        BalanceACC = (sensitivity+specificity)/2\n",
    "        G_mean = math.sqrt(sensitivity*specificity)\n",
    "        FN_rate= FN/(FN+TP) \n",
    "        FP_rate = FP/(FP+TN) \n",
    "        #add the data to arrays\n",
    "        sensitivity_.append(sensitivity)\n",
    "        specificity_.append(specificity)\n",
    "        BalanceACC_.append(BalanceACC)\n",
    "        G_mean_.append(G_mean)\n",
    "        FP_rate_.append(FP_rate)\n",
    "        FN_rate_.append(FN_rate)\n",
    "        accuracy_.append(global_acc)\n",
    "        loss_.append(global_loss)\n",
    "        statistical_parity_.append(stat_parity)\n",
    "        eqop_.append(eqop)\n",
    "        output_file.write(\"\\n Balanced accuracy: \" + str(BalanceACC))\n",
    "        output_file.write(\"\\n Disc. Score: \" + str(statistical_parity_[-1]))\n",
    "    end_time.append(((time.time()) - start))\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371f6a8f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(sensitivity_[index])\n",
    "print(specificity_[index])\n",
    "print(BalanceACC_[index])\n",
    "print(G_mean_[index])\n",
    "print(FP_rate_[index])\n",
    "print(FN_rate_[index])\n",
    "print(accuracy_[index])\n",
    "print(statistical_parity_[index])\n",
    "print(eqop_[index])\n",
    "print(statistical_parity_.index(min(statistical_parity_[0:-1], key=abs)))\n",
    "print(BalanceACC_.index(max(BalanceACC_[0:-1], key=abs)))\n",
    "print(trade_off.index(max(trade_off[0:-1], key=abs)))\n",
    "destination = \"./results/compas/eqopf/\"\n",
    "client=\"3-modified\"\n",
    "output_file = open(destination+client+\"-client-bal_acc.txt\", \"w+\")\n",
    "output_file.write(str(BalanceACC_))\n",
    "output_file.close()\n",
    "\n",
    "output_file = open(destination+client+\"-client-sen.txt\", \"w+\")\n",
    "output_file.write(str(sensitivity_))\n",
    "output_file.close()\n",
    "\n",
    "output_file = open(destination+client+\"-client-spc.txt\", \"w+\")\n",
    "output_file.write(str(specificity_))\n",
    "output_file.close()\n",
    "\n",
    "output_file = open(destination+client+\"-client-gmean.txt\", \"w+\")\n",
    "output_file.write(str(G_mean_))\n",
    "output_file.close()\n",
    "\n",
    "output_file = open(destination+client+\"-client-fp_rate.txt\", \"w+\")\n",
    "output_file.write(str(FP_rate_))\n",
    "output_file.close()\n",
    "\n",
    "output_file = open(destination+client+\"-client-fn_rate.txt\", \"w+\")\n",
    "output_file.write(str(FN_rate_))\n",
    "output_file.close()\n",
    "\n",
    "output_file = open(destination+client+\"-client-sp.txt\", \"w+\")\n",
    "output_file.write(str(statistical_parity_))\n",
    "output_file.close()\n",
    "\n",
    "output_file = open(destination+client+\"-client-eqop.txt\", \"w+\")\n",
    "output_file.write(str(eqop_))\n",
    "output_file.close()\n",
    "print(index)\n",
    "#pyplot.plot(BalanceACC_)\n",
    "plt.plot(BalanceACC_)\n",
    "plt.title('BalanceACC')\n",
    "plt.xlabel('Global Round')\n",
    "plt.ylabel('BalanceACC')\n",
    "plt.show()\n",
    "plt.plot(eqop_)\n",
    "plt.title('statistical_parity')\n",
    "plt.xlabel('Global Round')\n",
    "plt.ylabel('statistical_parity_')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cd645b",
   "metadata": {},
   "source": [
    "##### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
